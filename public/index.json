[{"authors":null,"categories":null,"content":"Hi there, welcome to my first trial in creating my personal website, I\u0026rsquo;m a biomedical engineer and I\u0026rsquo;m studying now to get my master degree in Engineering from University of New Brunswick. I am using this website to share some information about me and to post my latest work using R.\n  Download my resumé.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/nouran-mostafa/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nouran-mostafa/","section":"authors","summary":"Hi there, welcome to my first trial in creating my personal website, I\u0026rsquo;m a biomedical engineer and I\u0026rsquo;m studying now to get my master degree in Engineering from University of New Brunswick.","tags":null,"title":"Nouran Mostafa","type":"authors"},{"authors":null,"categories":null,"content":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bb560906b6a99893cc21387348c0b074","permalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/%E5%90%B3%E6%81%A9%E9%81%94/","section":"authors","summary":"吳恩達 is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"吳恩達","type":"authors"},{"authors":[],"categories":["ggplot2","Tideverse","tidymodels","yardstick"],"content":"\r\r","date":1628121600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628165313,"objectID":"951124be499879e41fd79d2eaf5997e7","permalink":"/post/predicting-delinquent-customer/","publishdate":"2021-08-05T00:00:00Z","relpermalink":"/post/predicting-delinquent-customer/","section":"post","summary":"\r\r","tags":[],"title":"Predicting Delinquent Customer","type":"post"},{"authors":[],"categories":["tidymodels","yardstick"],"content":"\r\rThe Goal\rThe Goal of this study is to help the credit card provider company CredX to identify the right customer based on predictive model where the past data of the bank’s applicants was used to built the predictive model and determine the factors affecting credit risk.\nThree models were created which are the Logistic regression, random forest and support vector machine among which the best model is defined as the model with the best accuracy taking inconsideration the sensitivity and specificity.\n\rThe Data\rThe data provided are two data frames one provide the Demographic data that is obtained from the applicants at the time of the credit card application and the other is Credit Bureau which is taken from the credit bureau.The Demographic data composed of 71295 observations for 11 independent variables and one dependent variable that is the performance_tag while the size of the Credit Bureau is also 71295 observations for 18 independent variables and one dependent variable that is the performance_tag.\nDuring the study we will provide two models the first one is based on the Demographic data only and the second one is based on both the Demographic data and the Credit Bureau data to check the predictive power of the application data.\n\rUnivariate Analysis\rIn the univariate analysis, it is easy notice the frequency of data in the dataset through the graphs. The most frequent values of the dominant predictors are listed as shown below,\nAge - 38 years\rGender - Male\rMarital Status - Married\rNumber of dependents - 3\rIncome - $\rEducation - Professional Education\rType of Residence - Rented\rEmployement in current company - 60\rProfession - SAL\rAverage CC utilisation in last 12 months - 10\rTrades opened in last 6 months - 1\rTrades opened in last 12 months - 2\r\r\rBivariate Analysis\rIn the Bivariate Analysis, the correlation plot is easily identified for the demogs data and combined data in the above cluster of plots.\nThen the dataset is briefly analysed by the scatter plots between Performance Tag and\nNo of dependents\rIncome\rNo of months in current residence\rNo of months in current residence\rNo of times 90 dpd or worse in last 6 months\rNo of times 60 dpd or worse in last 6 months\rNo of times 30 dpd or worse in last 6 months\rNo of times 90 dpd or worse in last 12 months\rNo of times 60 dpd or worse in last 12 months\rNo of times 30 dpd or worse in last 12 months\rAverage CC utilisation in last 12 months\rAverage CC utilisation in last 6 months\rNo of Trades opened in last 12 months\rNo of Trades opened in last 6 months\rNo of PL Trades opened in last 12 months\rNo of PL Trades opened in last 6 months\rPresence of Open Home loan\rOutstanding Balance\rTotal No of trades\rPresence of Open Auto loan\r\r## tibble [69,870 x 12] (S3: tbl_df/tbl/data.frame)\r## $ application_id : num [1:69870] 9.54e+08 4.33e+08 9.41e+08 3.92e+08 1.82e+08 ...\r## $ age : num [1:69870] 48 31 32 43 35 20 42 34 30 22 ...\r## $ gender : chr [1:69870] \u0026quot;F\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; \u0026quot;M\u0026quot; ...\r## $ marital_status_at_the_time_of_application: chr [1:69870] \u0026quot;Married\u0026quot; \u0026quot;Married\u0026quot; \u0026quot;Single\u0026quot; \u0026quot;Married\u0026quot; ...\r## $ no_of_dependents : num [1:69870] 2 4 2 1 5 1 2 2 3 1 ...\r## $ income : num [1:69870] 40 55 46 53 44 39 55 49 48 38 ...\r## $ education : chr [1:69870] \u0026quot;Bachelor\u0026quot; \u0026quot;Professional\u0026quot; \u0026quot;Bachelor\u0026quot; \u0026quot;Bachelor\u0026quot; ...\r## $ profession : chr [1:69870] \u0026quot;SAL\u0026quot; \u0026quot;SE_PROF\u0026quot; \u0026quot;SE_PROF\u0026quot; \u0026quot;SE\u0026quot; ...\r## $ type_of_residence : chr [1:69870] \u0026quot;Rented\u0026quot; \u0026quot;Rented\u0026quot; \u0026quot;Rented\u0026quot; \u0026quot;Rented\u0026quot; ...\r## $ no_of_months_in_current_residence : num [1:69870] 113 112 104 94 112 116 104 108 115 111 ...\r## $ no_of_months_in_current_company : num [1:69870] 56 46 49 53 43 52 41 40 58 57 ...\r## $ performance_tag : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## - attr(*, \u0026quot;spec\u0026quot;)=\r## .. cols(\r## .. `Application ID` = col_double(),\r## .. Age = col_double(),\r## .. Gender = col_character(),\r## .. `Marital Status (at the time of application)` = col_character(),\r## .. `No of dependents` = col_double(),\r## .. Income = col_double(),\r## .. Education = col_character(),\r## .. Profession = col_character(),\r## .. `Type of residence` = col_character(),\r## .. `No of months in current residence` = col_double(),\r## .. `No of months in current company` = col_double(),\r## .. `Performance Tag` = col_double()\r## .. )\r## demogs_data ## ## 12 Variables 69870 Observations\r## --------------------------------------------------------------------------------\r## application_id ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 69867 1 499226413 333192630 50010162 99493659 ## .25 .50 .75 .90 .95 ## 248578041 498048707 749864490 899535686 949412069 ## ## lowest : 100450 128993 142768 176721 197956\r## highest: 1000026076 1000026258 1000057167 1000072652 1000084142\r## --------------------------------------------------------------------------------\r## age ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 53 0.999 45 11.38 29 31 ## .25 .50 .75 .90 .95 ## 37 45 53 58 62 ## ## lowest : -3 0 15 16 17, highest: 61 62 63 64 65\r## --------------------------------------------------------------------------------\r## gender ## n missing distinct ## 69868 2 2 ## ## Value F M\r## Frequency 16506 53362\r## Proportion 0.236 0.764\r## --------------------------------------------------------------------------------\r## marital_status_at_the_time_of_application ## n missing distinct ## 69864 6 2 ## ## Value Married Single\r## Frequency 59547 10317\r## Proportion 0.852 0.148\r## --------------------------------------------------------------------------------\r## no_of_dependents ## n missing distinct Info Mean Gmd ## 69867 3 5 0.958 2.86 1.564 ## ## lowest : 1 2 3 4 5, highest: 1 2 3 4 5\r## ## Value 1 2 3 4 5\r## Frequency 15218 15128 15645 12000 11876\r## Proportion 0.218 0.217 0.224 0.172 0.170\r## --------------------------------------------------------------------------------\r## income ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 63 0.999 27.41 17.81 4.5 6.0 ## .25 .50 .75 .90 .95 ## 14.0 27.0 40.0 49.0 54.0 ## ## lowest : -0.5 0.0 1.0 2.0 3.0, highest: 56.0 57.0 58.0 59.0 60.0\r## --------------------------------------------------------------------------------\r## education ## n missing distinct ## 69752 118 5 ## ## lowest : Bachelor Masters Others Phd Professional\r## highest: Bachelor Masters Others Phd Professional\r## ## Value Bachelor Masters Others Phd Professional\r## Frequency 17302 23481 119 4464 24386\r## Proportion 0.248 0.337 0.002 0.064 0.350\r## --------------------------------------------------------------------------------\r## profession ## n missing distinct ## 69857 13 3 ## ## Value SAL SE SE_PROF\r## Frequency 39674 13927 16256\r## Proportion 0.568 0.199 0.233\r## --------------------------------------------------------------------------------\r## type_of_residence ## n missing distinct ## 69862 8 5 ## ## lowest : Company provided Living with Parents Others Owned Rented ## highest: Company provided Living with Parents Others Owned Rented ## ## Value Company provided Living with Parents Others\r## Frequency 1603 1778 198\r## Proportion 0.023 0.025 0.003\r## ## Value Owned Rented\r## Frequency 14003 52280\r## Proportion 0.200 0.748\r## --------------------------------------------------------------------------------\r## no_of_months_in_current_residence ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 121 0.889 34.61 38.07 6 6 ## .25 .50 .75 .90 .95 ## 6 10 61 98 110 ## ## lowest : 6 7 8 9 10, highest: 122 123 124 125 126\r## --------------------------------------------------------------------------------\r## no_of_months_in_current_company ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 83 1 34.2 23.45 3 6 ## .25 .50 .75 .90 .95 ## 17 34 51 62 68 ## ## lowest : 3 4 5 6 7, highest: 121 123 126 128 133\r## --------------------------------------------------------------------------------\r## performance_tag ## n missing distinct Info Sum Mean Gmd ## 69870 0 2 0.121 2948 0.04219 0.08083 ## ## --------------------------------------------------------------------------------\r## application_id age gender\r## nbr.val 6.987000e+04 6.987000e+04 NA\r## nbr.null 0.000000e+00 1.900000e+01 NA\r## nbr.na 0.000000e+00 0.000000e+00 NA\r## min 1.004500e+05 -3.000000e+00 NA\r## max 1.000084e+09 6.500000e+01 NA\r## range 9.999837e+08 6.800000e+01 NA\r## sum 3.488095e+13 3.143841e+06 NA\r## median 4.980487e+08 4.500000e+01 NA\r## mean 4.992264e+08 4.499558e+01 NA\r## SE.mean 1.091638e+06 3.756691e-02 NA\r## CI.mean.0.95 2.139608e+06 7.363106e-02 NA\r## var 8.326222e+16 9.860561e+01 NA\r## std.dev 2.885519e+08 9.930036e+00 NA\r## coef.var 5.779981e-01 2.206892e-01 NA\r## marital_status_at_the_time_of_application no_of_dependents\r## nbr.val NA 6.986700e+04\r## nbr.null NA 0.000000e+00\r## nbr.na NA 3.000000e+00\r## min NA 1.000000e+00\r## max NA 5.000000e+00\r## range NA 4.000000e+00\r## sum NA 1.997890e+05\r## median NA 3.000000e+00\r## mean NA 2.859562e+00\r## SE.mean NA 5.241886e-03\r## CI.mean.0.95 NA 1.027408e-02\r## var NA 1.919761e+00\r## std.dev NA 1.385554e+00\r## coef.var NA 4.845338e-01\r## income education profession type_of_residence\r## nbr.val 6.987000e+04 NA NA NA\r## nbr.null 2.600000e+01 NA NA NA\r## nbr.na 0.000000e+00 NA NA NA\r## min -5.000000e-01 NA NA NA\r## max 6.000000e+01 NA NA NA\r## range 6.050000e+01 NA NA NA\r## sum 1.915029e+06 NA NA NA\r## median 2.700000e+01 NA NA NA\r## mean 2.740845e+01 NA NA NA\r## SE.mean 5.854818e-02 NA NA NA\r## CI.mean.0.95 1.147543e-01 NA NA NA\r## var 2.395067e+02 NA NA NA\r## std.dev 1.547600e+01 NA NA NA\r## coef.var 5.646435e-01 NA NA NA\r## no_of_months_in_current_residence no_of_months_in_current_company\r## nbr.val 6.987000e+04 6.987000e+04\r## nbr.null 0.000000e+00 0.000000e+00\r## nbr.na 0.000000e+00 0.000000e+00\r## min 6.000000e+00 3.000000e+00\r## max 1.260000e+02 1.330000e+02\r## range 1.200000e+02 1.300000e+02\r## sum 2.418071e+06 2.389497e+06\r## median 1.000000e+01 3.400000e+01\r## mean 3.460814e+01 3.419918e+01\r## SE.mean 1.393930e-01 7.698814e-02\r## CI.mean.0.95 2.732100e-01 1.508966e-01\r## var 1.357602e+03 4.141317e+02\r## std.dev 3.684566e+01 2.035023e+01\r## coef.var 1.064653e+00 5.950500e-01\r## performance_tag\r## nbr.val 6.987000e+04\r## nbr.null 6.692200e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 1.000000e+00\r## range 1.000000e+00\r## sum 2.948000e+03\r## median 0.000000e+00\r## mean 4.219264e-02\r## SE.mean 7.605280e-04\r## CI.mean.0.95 1.490633e-03\r## var 4.041300e-02\r## std.dev 2.010299e-01\r## coef.var 4.764571e+00\r## The Correlation between Number of dependents and performance Tag is -0.000307697\r## The Correlation between Number of dependents and performance Tag is -0.03872444\r## The Correlation between Number of dependents and performance Tag is 0.01625466\r## The Correlation between Number of dependents and performance Tag is -0.01903269\r## application_id age no_of_dependents income\r## application_id 1.00 0.00 -0.01 0.00\r## age 0.00 1.00 0.17 0.06\r## no_of_dependents -0.01 0.17 1.00 0.03\r## income 0.00 0.06 0.03 1.00\r## no_of_months_in_current_residence 0.00 -0.07 -0.01 -0.09\r## no_of_months_in_current_company 0.00 -0.02 -0.01 -0.10\r## performance_tag 0.00 0.00 0.00 -0.04\r## no_of_months_in_current_residence\r## application_id 0.00\r## age -0.07\r## no_of_dependents -0.01\r## income -0.09\r## no_of_months_in_current_residence 1.00\r## no_of_months_in_current_company -0.08\r## performance_tag 0.02\r## no_of_months_in_current_company\r## application_id 0.00\r## age -0.02\r## no_of_dependents -0.01\r## income -0.10\r## no_of_months_in_current_residence -0.08\r## no_of_months_in_current_company 1.00\r## performance_tag -0.02\r## performance_tag\r## application_id 0.00\r## age 0.00\r## no_of_dependents 0.00\r## income -0.04\r## no_of_months_in_current_residence 0.02\r## no_of_months_in_current_company -0.02\r## performance_tag 1.00\r## tibble [69,870 x 19] (S3: tbl_df/tbl/data.frame)\r## $ application_id : num [1:69870] 9.54e+08 4.33e+08 9.41e+08 3.92e+08 1.82e+08 ...\r## $ no_of_times_90_dpd_or_worse_in_last_6_months : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## $ no_of_times_60_dpd_or_worse_in_last_6_months : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## $ no_of_times_30_dpd_or_worse_in_last_6_months : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## $ no_of_times_90_dpd_or_worse_in_last_12_months : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## $ no_of_times_60_dpd_or_worse_in_last_12_months : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## $ no_of_times_30_dpd_or_worse_in_last_12_months : num [1:69870] 0 0 0 0 0 0 0 0 1 0 ...\r## $ avgas_cc_utilization_in_last_12_months : num [1:69870] 4 3 7 11 12 10 11 13 9 6 ...\r## $ no_of_trades_opened_in_last_6_months : num [1:69870] 1 1 0 1 0 0 0 1 0 1 ...\r## $ no_of_trades_opened_in_last_12_months : num [1:69870] 2 2 0 1 1 0 1 1 0 1 ...\r## $ no_of_pl_trades_opened_in_last_6_months : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## $ no_of_pl_trades_opened_in_last_12_months : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## $ no_of_inquiries_in_last_6_months_excluding_home_auto_loans : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## $ no_of_inquiries_in_last_12_months_excluding_home_auto_loans: num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## $ presence_of_open_home_loan : num [1:69870] 1 0 1 1 1 0 1 1 1 0 ...\r## $ outstanding_balance : num [1:69870] 2999395 3078 3004972 3355373 3014283 ...\r## $ total_no_of_trades : num [1:69870] 4 5 2 4 4 1 4 3 2 1 ...\r## $ presence_of_open_auto_loan : num [1:69870] 0 0 0 1 0 0 0 0 0 1 ...\r## $ performance_tag : num [1:69870] 0 0 0 0 0 0 0 0 0 0 ...\r## - attr(*, \u0026quot;spec\u0026quot;)=\r## .. cols(\r## .. `Application ID` = col_double(),\r## .. `No of times 90 DPD or worse in last 6 months` = col_double(),\r## .. `No of times 60 DPD or worse in last 6 months` = col_double(),\r## .. `No of times 30 DPD or worse in last 6 months` = col_double(),\r## .. `No of times 90 DPD or worse in last 12 months` = col_double(),\r## .. `No of times 60 DPD or worse in last 12 months` = col_double(),\r## .. `No of times 30 DPD or worse in last 12 months` = col_double(),\r## .. `Avgas CC Utilization in last 12 months` = col_double(),\r## .. `No of trades opened in last 6 months` = col_double(),\r## .. `No of trades opened in last 12 months` = col_double(),\r## .. `No of PL trades opened in last 6 months` = col_double(),\r## .. `No of PL trades opened in last 12 months` = col_double(),\r## .. `No of Inquiries in last 6 months (excluding home \u0026amp; auto loans)` = col_double(),\r## .. `No of Inquiries in last 12 months (excluding home \u0026amp; auto loans)` = col_double(),\r## .. `Presence of open home loan` = col_double(),\r## .. `Outstanding Balance` = col_double(),\r## .. `Total No of Trades` = col_double(),\r## .. `Presence of open auto loan` = col_double(),\r## .. `Performance Tag` = col_double()\r## .. )\r## credit_bureau_data ## ## 19 Variables 69870 Observations\r## --------------------------------------------------------------------------------\r## application_id ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 69867 1 499226413 333192630 50010162 99493659 ## .25 .50 .75 .90 .95 ## 248578041 498048707 749864490 899535686 949412069 ## ## lowest : 100450 128993 142768 176721 197956\r## highest: 1000026076 1000026258 1000057167 1000072652 1000084142\r## --------------------------------------------------------------------------------\r## no_of_times_90_dpd_or_worse_in_last_6_months ## n missing distinct Info Mean Gmd ## 69870 0 4 0.514 0.249 0.4016 ## ## Value 0 1 2 3\r## Frequency 54666 13220 1776 208\r## Proportion 0.782 0.189 0.025 0.003\r## --------------------------------------------------------------------------------\r## no_of_times_60_dpd_or_worse_in_last_6_months ## n missing distinct Info Mean Gmd ## 69870 0 6 0.586 0.3917 0.6297 ## ## lowest : 0 1 2 3 4, highest: 1 2 3 4 5\r## ## Value 0 1 2 3 4 5\r## Frequency 51871 11132 4917 1469 411 70\r## Proportion 0.742 0.159 0.070 0.021 0.006 0.001\r## --------------------------------------------------------------------------------\r## no_of_times_30_dpd_or_worse_in_last_6_months ## n missing distinct Info Mean Gmd ## 69870 0 8 0.628 0.5235 0.8348 ## ## lowest : 0 1 2 3 4, highest: 3 4 5 6 7\r## ## Value 0 1 2 3 4 5 6 7\r## Frequency 50099 9501 5898 2830 1045 386 96 15\r## Proportion 0.717 0.136 0.084 0.041 0.015 0.006 0.001 0.000\r## --------------------------------------------------------------------------------\r## no_of_times_90_dpd_or_worse_in_last_12_months ## n missing distinct Info Mean Gmd ## 69870 0 6 0.617 0.4148 0.6505 ## ## lowest : 0 1 2 3 4, highest: 1 2 3 4 5\r## ## Value 0 1 2 3 4 5\r## Frequency 50494 11663 6161 1244 272 36\r## Proportion 0.723 0.167 0.088 0.018 0.004 0.001\r## --------------------------------------------------------------------------------\r## no_of_times_60_dpd_or_worse_in_last_12_months ## n missing distinct Info Mean Gmd ## 69870 0 8 0.71 0.6034 0.9091 ## ## lowest : 0 1 2 3 4, highest: 3 4 5 6 7\r## ## Value 0 1 2 3 4 5 6 7\r## Frequency 45869 12816 6416 3205 1048 398 111 7\r## Proportion 0.656 0.183 0.092 0.046 0.015 0.006 0.002 0.000\r## --------------------------------------------------------------------------------\r## no_of_times_30_dpd_or_worse_in_last_12_months ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 10 0.73 0.7339 1.109 0 0 ## .25 .50 .75 .90 .95 ## 0 0 1 3 3 ## ## lowest : 0 1 2 3 4, highest: 5 6 7 8 9\r## ## Value 0 1 2 3 4 5 6 7 8 9\r## Frequency 44858 11474 6117 4137 1924 853 376 107 23 1\r## Proportion 0.642 0.164 0.088 0.059 0.028 0.012 0.005 0.002 0.000 0.000\r## --------------------------------------------------------------------------------\r## avgas_cc_utilization_in_last_12_months ## n missing distinct Info Mean Gmd .05 .10 ## 68847 1023 114 0.999 29.27 30.06 3 5 ## .25 .50 .75 .90 .95 ## 8 15 45 72 104 ## ## lowest : 0 1 2 3 4, highest: 109 110 111 112 113\r## --------------------------------------------------------------------------------\r## no_of_trades_opened_in_last_6_months ## n missing distinct Info Mean Gmd .05 .10 ## 69869 1 13 0.962 2.285 2.197 0 0 ## .25 .50 .75 .90 .95 ## 1 2 3 5 7 ## ## lowest : 0 1 2 3 4, highest: 8 9 10 11 12\r## ## Value 0 1 2 3 4 5 6 7 8 9 10\r## Frequency 12194 20122 12117 9403 6297 3665 2336 1649 1154 618 238\r## Proportion 0.175 0.288 0.173 0.135 0.090 0.052 0.033 0.024 0.017 0.009 0.003\r## ## Value 11 12\r## Frequency 65 11\r## Proportion 0.001 0.000\r## --------------------------------------------------------------------------------\r## no_of_trades_opened_in_last_12_months ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 29 0.991 5.785 5.501 0 1 ## .25 .50 .75 .90 .95 ## 2 4 9 13 16 ## ## lowest : 0 1 2 3 4, highest: 24 25 26 27 28\r## --------------------------------------------------------------------------------\r## no_of_pl_trades_opened_in_last_6_months ## n missing distinct Info Mean Gmd ## 69870 0 7 0.897 1.19 1.426 ## ## lowest : 0 1 2 3 4, highest: 2 3 4 5 6\r## ## Value 0 1 2 3 4 5 6\r## Frequency 31081 13548 12565 7949 3341 1090 296\r## Proportion 0.445 0.194 0.180 0.114 0.048 0.016 0.004\r## --------------------------------------------------------------------------------\r## no_of_pl_trades_opened_in_last_12_months ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 13 0.944 2.363 2.645 0 0 ## .25 .50 .75 .90 .95 ## 0 2 4 6 7 ## ## lowest : 0 1 2 3 4, highest: 8 9 10 11 12\r## ## Value 0 1 2 3 4 5 6 7 8 9 10\r## Frequency 25825 6641 6830 8131 7904 6189 4023 2223 1172 601 255\r## Proportion 0.370 0.095 0.098 0.116 0.113 0.089 0.058 0.032 0.017 0.009 0.004\r## ## Value 11 12\r## Frequency 66 10\r## Proportion 0.001 0.000\r## --------------------------------------------------------------------------------\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 11 0.939 1.758 2.052 0 0 ## .25 .50 .75 .90 .95 ## 0 1 3 5 6 ## ## lowest : 0 1 2 3 4, highest: 6 7 8 9 10\r## ## Value 0 1 2 3 4 5 6 7 8 9 10\r## Frequency 25069 13177 12832 7258 4248 3019 1750 1149 835 425 108\r## Proportion 0.359 0.189 0.184 0.104 0.061 0.043 0.025 0.016 0.012 0.006 0.002\r## --------------------------------------------------------------------------------\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 21 0.969 3.525 3.846 0 0 ## .25 .50 .75 .90 .95 ## 0 3 5 9 11 ## ## lowest : 0 1 2 3 4, highest: 16 17 18 19 20\r## --------------------------------------------------------------------------------\r## presence_of_open_home_loan ## n missing distinct Info Sum Mean Gmd ## 69598 272 2 0.577 18071 0.2596 0.3845 ## ## --------------------------------------------------------------------------------\r## outstanding_balance ## n missing distinct Info Mean Gmd .05 .10 ## 69598 272 63947 1 1253338 1371190 2649 6848 ## .25 .50 .75 .90 .95 ## 208391 774235 2926224 3282343 3650667 ## ## lowest : 0 1 2 4 5\r## highest: 5046778 5047711 5052166 5151848 5218801\r## --------------------------------------------------------------------------------\r## total_no_of_trades ## n missing distinct Info Mean Gmd .05 .10 ## 69870 0 45 0.994 8.175 7.164 1 2 ## .25 .50 .75 .90 .95 ## 3 6 10 20 25 ## ## lowest : 0 1 2 3 4, highest: 40 41 42 43 44\r## --------------------------------------------------------------------------------\r## presence_of_open_auto_loan ## n missing distinct Info Sum Mean Gmd ## 69870 0 2 0.233 5930 0.08487 0.1553 ## ## --------------------------------------------------------------------------------\r## performance_tag ## n missing distinct Info Sum Mean Gmd ## 69870 0 2 0.121 2948 0.04219 0.08083 ## ## --------------------------------------------------------------------------------\r## application_id no_of_times_90_dpd_or_worse_in_last_6_months\r## nbr.val 6.987000e+04 6.987000e+04\r## nbr.null 0.000000e+00 5.466600e+04\r## nbr.na 0.000000e+00 0.000000e+00\r## min 1.004500e+05 0.000000e+00\r## max 1.000084e+09 3.000000e+00\r## range 9.999837e+08 3.000000e+00\r## sum 3.488095e+13 1.739600e+04\r## median 4.980487e+08 0.000000e+00\r## mean 4.992264e+08 2.489767e-01\r## SE.mean 1.091638e+06 1.912985e-03\r## CI.mean.0.95 2.139608e+06 3.749446e-03\r## var 8.326222e+16 2.556900e-01\r## std.dev 2.885519e+08 5.056579e-01\r## coef.var 5.779981e-01 2.030945e+00\r## no_of_times_60_dpd_or_worse_in_last_6_months\r## nbr.val 6.987000e+04\r## nbr.null 5.187100e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 5.000000e+00\r## range 5.000000e+00\r## sum 2.736700e+04\r## median 0.000000e+00\r## mean 3.916846e-01\r## SE.mean 2.920142e-03\r## CI.mean.0.95 5.723472e-03\r## var 5.957974e-01\r## std.dev 7.718791e-01\r## coef.var 1.970665e+00\r## no_of_times_30_dpd_or_worse_in_last_6_months\r## nbr.val 6.987000e+04\r## nbr.null 5.009900e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 7.000000e+00\r## range 7.000000e+00\r## sum 3.657800e+04\r## median 0.000000e+00\r## mean 5.235151e-01\r## SE.mean 3.786024e-03\r## CI.mean.0.95 7.420600e-03\r## var 1.001515e+00\r## std.dev 1.000757e+00\r## coef.var 1.911611e+00\r## no_of_times_90_dpd_or_worse_in_last_12_months\r## nbr.val 6.987000e+04\r## nbr.null 5.049400e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 5.000000e+00\r## range 5.000000e+00\r## sum 2.898500e+04\r## median 0.000000e+00\r## mean 4.148418e-01\r## SE.mean 2.888510e-03\r## CI.mean.0.95 5.661474e-03\r## var 5.829597e-01\r## std.dev 7.635180e-01\r## coef.var 1.840504e+00\r## no_of_times_60_dpd_or_worse_in_last_12_months\r## nbr.val 6.987000e+04\r## nbr.null 4.586900e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 7.000000e+00\r## range 7.000000e+00\r## sum 4.216000e+04\r## median 0.000000e+00\r## mean 6.034063e-01\r## SE.mean 3.865470e-03\r## CI.mean.0.95 7.576314e-03\r## var 1.043988e+00\r## std.dev 1.021757e+00\r## coef.var 1.693315e+00\r## no_of_times_30_dpd_or_worse_in_last_12_months\r## nbr.val 6.987000e+04\r## nbr.null 4.485800e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 9.000000e+00\r## range 9.000000e+00\r## sum 5.127800e+04\r## median 0.000000e+00\r## mean 7.339058e-01\r## SE.mean 4.703158e-03\r## CI.mean.0.95 9.218181e-03\r## var 1.545503e+00\r## std.dev 1.243183e+00\r## coef.var 1.693927e+00\r## avgas_cc_utilization_in_last_12_months\r## nbr.val 6.884700e+04\r## nbr.null 2.310000e+02\r## nbr.na 1.023000e+03\r## min 0.000000e+00\r## max 1.130000e+02\r## range 1.130000e+02\r## sum 2.014822e+06\r## median 1.500000e+01\r## mean 2.926521e+01\r## SE.mean 1.124893e-01\r## CI.mean.0.95 2.204789e-01\r## var 8.711795e+02\r## std.dev 2.951575e+01\r## coef.var 1.008561e+00\r## no_of_trades_opened_in_last_6_months\r## nbr.val 6.986900e+04\r## nbr.null 1.219400e+04\r## nbr.na 1.000000e+00\r## min 0.000000e+00\r## max 1.200000e+01\r## range 1.200000e+01\r## sum 1.596580e+05\r## median 2.000000e+00\r## mean 2.285105e+00\r## SE.mean 7.876077e-03\r## CI.mean.0.95 1.543709e-02\r## var 4.334155e+00\r## std.dev 2.081863e+00\r## coef.var 9.110580e-01\r## no_of_trades_opened_in_last_12_months\r## nbr.val 6.987000e+04\r## nbr.null 4.956000e+03\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 2.800000e+01\r## range 2.800000e+01\r## sum 4.041940e+05\r## median 4.000000e+00\r## mean 5.784943e+00\r## SE.mean 1.927649e-02\r## CI.mean.0.95 3.778188e-02\r## var 2.596251e+01\r## std.dev 5.095342e+00\r## coef.var 8.807937e-01\r## no_of_pl_trades_opened_in_last_6_months\r## nbr.val 6.987000e+04\r## nbr.null 3.108100e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 6.000000e+00\r## range 6.000000e+00\r## sum 8.311500e+04\r## median 1.000000e+00\r## mean 1.189566e+00\r## SE.mean 5.117361e-03\r## CI.mean.0.95 1.003002e-02\r## var 1.829713e+00\r## std.dev 1.352669e+00\r## coef.var 1.137111e+00\r## no_of_pl_trades_opened_in_last_12_months\r## nbr.val 6.987000e+04\r## nbr.null 2.582500e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 1.200000e+01\r## range 1.200000e+01\r## sum 1.651350e+05\r## median 2.000000e+00\r## mean 2.363461e+00\r## SE.mean 9.168974e-03\r## CI.mean.0.95 1.797117e-02\r## var 5.873977e+00\r## std.dev 2.423629e+00\r## coef.var 1.025458e+00\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans\r## nbr.val 6.987000e+04\r## nbr.null 2.506900e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 1.000000e+01\r## range 1.000000e+01\r## sum 1.228300e+05\r## median 1.000000e+00\r## mean 1.757979e+00\r## SE.mean 7.509624e-03\r## CI.mean.0.95 1.471885e-02\r## var 3.940280e+00\r## std.dev 1.985014e+00\r## coef.var 1.129145e+00\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans\r## nbr.val 6.987000e+04\r## nbr.null 2.058100e+04\r## nbr.na 0.000000e+00\r## min 0.000000e+00\r## max 2.000000e+01\r## range 2.000000e+01\r## sum 2.462940e+05\r## median 3.000000e+00\r## mean 3.525032e+00\r## SE.mean 1.366362e-02\r## CI.mean.0.95 2.678067e-02\r## var 1.304435e+01\r## std.dev 3.611696e+00\r## coef.var 1.024585e+00\r## presence_of_open_home_loan outstanding_balance total_no_of_trades\r## nbr.val 6.959800e+04 6.959800e+04 6.987000e+04\r## nbr.null 5.152700e+04 8.090000e+02 9.880000e+02\r## nbr.na 2.720000e+02 2.720000e+02 0.000000e+00\r## min 0.000000e+00 0.000000e+00 0.000000e+00\r## max 1.000000e+00 5.218801e+06 4.400000e+01\r## range 1.000000e+00 5.218801e+06 4.400000e+01\r## sum 1.807100e+04 8.722980e+10 5.711660e+05\r## median 0.000000e+00 7.742345e+05 6.000000e+00\r## mean 2.596483e-01 1.253338e+06 8.174696e+00\r## SE.mean 1.661944e-03 4.875269e+03 2.706017e-02\r## CI.mean.0.95 3.257408e-03 9.555517e+03 5.303788e-02\r## var 1.922338e-01 1.654222e+12 5.116251e+01\r## std.dev 4.384448e-01 1.286166e+06 7.152798e+00\r## coef.var 1.688610e+00 1.026193e+00 8.749925e-01\r## presence_of_open_auto_loan performance_tag\r## nbr.val 6.987000e+04 6.987000e+04\r## nbr.null 6.394000e+04 6.692200e+04\r## nbr.na 0.000000e+00 0.000000e+00\r## min 0.000000e+00 0.000000e+00\r## max 1.000000e+00 1.000000e+00\r## range 1.000000e+00 1.000000e+00\r## sum 5.930000e+03 2.948000e+03\r## median 0.000000e+00 0.000000e+00\r## mean 8.487190e-02 4.219264e-02\r## SE.mean 1.054340e-03 7.605280e-04\r## CI.mean.0.95 2.066504e-03 1.490633e-03\r## var 7.766978e-02 4.041300e-02\r## std.dev 2.786930e-01 2.010299e-01\r## coef.var 3.283689e+00 4.764571e+00\r## The Correlation between no_of_inquiries_in_last_12_months_excluding_home_auto_loans and performance Tag is 0.05602722\r## The Correlation between presence_of_open_home_loan and performance Tag is -0.02547596\r## The Correlation between outstanding_balance and performance Tag is 0.001155206\r## The Correlation between total_no_of_trades and performance Tag is 0.0349171\r## The Correlation between presence_of_open_auto_loan and performance Tag is -0.007971069\r## The Correlation between avgas_cc_utilization_in_last_12_months and performance Tag is 0.08040627\r## The Correlation between no_of_trades_opened_in_last_6_months and performance Tag is 0.05069951\r## The Correlation between no_of_trades_opened_in_last_12_monthss and performance Tag is 0.05921629\r## The Correlation between no_of_pl_trades_opened_in_last_6_months and performance Tag is 0.07211644\r## The Correlation between no_of_pl_trades_opened_in_last_12_months and performance Tag is 0.07780216\r## The Correlation between no_of_inquiries_in_last_6_months_excluding_home_auto_loans and performance Tag is 0.04671557\r## The Correlation between no_of_times_90_dpd_or_worse_in_last_6_months and performance Tag is 0.08814223\r## The Correlation between no_of_times_60_dpd_or_worse_in_last_6_months and performance Tag is 0.09567882\r## The Correlation between no_of_times_30_dpd_or_worse_in_last_6_months and performance Tag is 0.1027773\r## The Correlation between no_of_times_90_dpd_or_worse_in_last_12_months and performance Tag is 0.09772734\r## The Correlation between no_of_times_60_dpd_or_worse_in_last_12_months and performance Tag is 0.09296393\r## The Correlation between no_of_times_30_dpd_or_worse_in_last_12_months and performance Tag is 0.1009908\r## application_id\r## application_id 1.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.00\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.00\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.00\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.00\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.00\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.00\r## avgas_cc_utilization_in_last_12_months 0.00\r## no_of_trades_opened_in_last_6_months 0.00\r## no_of_trades_opened_in_last_12_months 0.00\r## no_of_pl_trades_opened_in_last_6_months 0.00\r## no_of_pl_trades_opened_in_last_12_months 0.00\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.00\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.00\r## presence_of_open_home_loan 0.00\r## outstanding_balance 0.00\r## total_no_of_trades 0.00\r## presence_of_open_auto_loan 0.01\r## performance_tag 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 1.00\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.89\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.84\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.89\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.82\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.80\r## avgas_cc_utilization_in_last_12_months 0.36\r## no_of_trades_opened_in_last_6_months 0.14\r## no_of_trades_opened_in_last_12_months 0.17\r## no_of_pl_trades_opened_in_last_6_months 0.25\r## no_of_pl_trades_opened_in_last_12_months 0.27\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.14\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.15\r## presence_of_open_home_loan -0.11\r## outstanding_balance -0.02\r## total_no_of_trades 0.03\r## presence_of_open_auto_loan -0.03\r## performance_tag 0.09\r## no_of_times_60_dpd_or_worse_in_last_6_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.89\r## no_of_times_60_dpd_or_worse_in_last_6_months 1.00\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.95\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.84\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.92\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.90\r## avgas_cc_utilization_in_last_12_months 0.35\r## no_of_trades_opened_in_last_6_months 0.15\r## no_of_trades_opened_in_last_12_months 0.19\r## no_of_pl_trades_opened_in_last_6_months 0.27\r## no_of_pl_trades_opened_in_last_12_months 0.30\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.15\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.17\r## presence_of_open_home_loan -0.12\r## outstanding_balance -0.02\r## total_no_of_trades 0.04\r## presence_of_open_auto_loan -0.03\r## performance_tag 0.10\r## no_of_times_30_dpd_or_worse_in_last_6_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.84\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.95\r## no_of_times_30_dpd_or_worse_in_last_6_months 1.00\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.83\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.90\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.95\r## avgas_cc_utilization_in_last_12_months 0.35\r## no_of_trades_opened_in_last_6_months 0.16\r## no_of_trades_opened_in_last_12_months 0.20\r## no_of_pl_trades_opened_in_last_6_months 0.28\r## no_of_pl_trades_opened_in_last_12_months 0.31\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.16\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.18\r## presence_of_open_home_loan -0.12\r## outstanding_balance -0.02\r## total_no_of_trades 0.04\r## presence_of_open_auto_loan -0.03\r## performance_tag 0.10\r## no_of_times_90_dpd_or_worse_in_last_12_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.89\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.84\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.83\r## no_of_times_90_dpd_or_worse_in_last_12_months 1.00\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.81\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.80\r## avgas_cc_utilization_in_last_12_months 0.38\r## no_of_trades_opened_in_last_6_months 0.15\r## no_of_trades_opened_in_last_12_months 0.19\r## no_of_pl_trades_opened_in_last_6_months 0.28\r## no_of_pl_trades_opened_in_last_12_months 0.31\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.16\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.18\r## presence_of_open_home_loan -0.13\r## outstanding_balance -0.02\r## total_no_of_trades 0.04\r## presence_of_open_auto_loan -0.03\r## performance_tag 0.10\r## no_of_times_60_dpd_or_worse_in_last_12_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.82\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.92\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.90\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.81\r## no_of_times_60_dpd_or_worse_in_last_12_months 1.00\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.90\r## avgas_cc_utilization_in_last_12_months 0.34\r## no_of_trades_opened_in_last_6_months 0.13\r## no_of_trades_opened_in_last_12_months 0.16\r## no_of_pl_trades_opened_in_last_6_months 0.25\r## no_of_pl_trades_opened_in_last_12_months 0.28\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.13\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.14\r## presence_of_open_home_loan -0.11\r## outstanding_balance -0.02\r## total_no_of_trades 0.01\r## presence_of_open_auto_loan -0.02\r## performance_tag 0.09\r## no_of_times_30_dpd_or_worse_in_last_12_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.80\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.90\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.95\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.80\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.90\r## no_of_times_30_dpd_or_worse_in_last_12_months 1.00\r## avgas_cc_utilization_in_last_12_months 0.34\r## no_of_trades_opened_in_last_6_months 0.14\r## no_of_trades_opened_in_last_12_months 0.18\r## no_of_pl_trades_opened_in_last_6_months 0.26\r## no_of_pl_trades_opened_in_last_12_months 0.29\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.14\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.16\r## presence_of_open_home_loan -0.11\r## outstanding_balance -0.02\r## total_no_of_trades 0.01\r## presence_of_open_auto_loan -0.03\r## performance_tag 0.10\r## avgas_cc_utilization_in_last_12_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.36\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.35\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.35\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.38\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.34\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.34\r## avgas_cc_utilization_in_last_12_months 1.00\r## no_of_trades_opened_in_last_6_months 0.09\r## no_of_trades_opened_in_last_12_months 0.13\r## no_of_pl_trades_opened_in_last_6_months 0.24\r## no_of_pl_trades_opened_in_last_12_months 0.27\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.09\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.10\r## presence_of_open_home_loan -0.13\r## outstanding_balance -0.03\r## total_no_of_trades -0.01\r## presence_of_open_auto_loan -0.04\r## performance_tag 0.08\r## no_of_trades_opened_in_last_6_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.14\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.15\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.16\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.15\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.13\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.14\r## avgas_cc_utilization_in_last_12_months 0.09\r## no_of_trades_opened_in_last_6_months 1.00\r## no_of_trades_opened_in_last_12_months 0.94\r## no_of_pl_trades_opened_in_last_6_months 0.88\r## no_of_pl_trades_opened_in_last_12_months 0.83\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.67\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.73\r## presence_of_open_home_loan -0.12\r## outstanding_balance 0.17\r## total_no_of_trades 0.89\r## presence_of_open_auto_loan -0.04\r## performance_tag 0.05\r## no_of_trades_opened_in_last_12_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.17\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.19\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.20\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.19\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.16\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.18\r## avgas_cc_utilization_in_last_12_months 0.13\r## no_of_trades_opened_in_last_6_months 0.94\r## no_of_trades_opened_in_last_12_months 1.00\r## no_of_pl_trades_opened_in_last_6_months 0.89\r## no_of_pl_trades_opened_in_last_12_months 0.93\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.72\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.79\r## presence_of_open_home_loan -0.14\r## outstanding_balance 0.19\r## total_no_of_trades 0.94\r## presence_of_open_auto_loan -0.05\r## performance_tag 0.06\r## no_of_pl_trades_opened_in_last_6_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.25\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.27\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.28\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.28\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.25\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.26\r## avgas_cc_utilization_in_last_12_months 0.24\r## no_of_trades_opened_in_last_6_months 0.88\r## no_of_trades_opened_in_last_12_months 0.89\r## no_of_pl_trades_opened_in_last_6_months 1.00\r## no_of_pl_trades_opened_in_last_12_months 0.90\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.60\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.65\r## presence_of_open_home_loan -0.14\r## outstanding_balance 0.17\r## total_no_of_trades 0.79\r## presence_of_open_auto_loan -0.05\r## performance_tag 0.07\r## no_of_pl_trades_opened_in_last_12_months\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.27\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.30\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.31\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.31\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.28\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.29\r## avgas_cc_utilization_in_last_12_months 0.27\r## no_of_trades_opened_in_last_6_months 0.83\r## no_of_trades_opened_in_last_12_months 0.93\r## no_of_pl_trades_opened_in_last_6_months 0.90\r## no_of_pl_trades_opened_in_last_12_months 1.00\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.66\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.73\r## presence_of_open_home_loan -0.16\r## outstanding_balance 0.20\r## total_no_of_trades 0.83\r## presence_of_open_auto_loan -0.05\r## performance_tag 0.08\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.14\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.15\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.16\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.16\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.13\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.14\r## avgas_cc_utilization_in_last_12_months 0.09\r## no_of_trades_opened_in_last_6_months 0.67\r## no_of_trades_opened_in_last_12_months 0.72\r## no_of_pl_trades_opened_in_last_6_months 0.60\r## no_of_pl_trades_opened_in_last_12_months 0.66\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 1.00\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.91\r## presence_of_open_home_loan -0.12\r## outstanding_balance 0.11\r## total_no_of_trades 0.71\r## presence_of_open_auto_loan -0.04\r## performance_tag 0.05\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.15\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.17\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.18\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.18\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.14\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.16\r## avgas_cc_utilization_in_last_12_months 0.10\r## no_of_trades_opened_in_last_6_months 0.73\r## no_of_trades_opened_in_last_12_months 0.79\r## no_of_pl_trades_opened_in_last_6_months 0.65\r## no_of_pl_trades_opened_in_last_12_months 0.73\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.91\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 1.00\r## presence_of_open_home_loan -0.13\r## outstanding_balance 0.12\r## total_no_of_trades 0.78\r## presence_of_open_auto_loan -0.05\r## performance_tag 0.06\r## presence_of_open_home_loan\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months -0.11\r## no_of_times_60_dpd_or_worse_in_last_6_months -0.12\r## no_of_times_30_dpd_or_worse_in_last_6_months -0.12\r## no_of_times_90_dpd_or_worse_in_last_12_months -0.13\r## no_of_times_60_dpd_or_worse_in_last_12_months -0.11\r## no_of_times_30_dpd_or_worse_in_last_12_months -0.11\r## avgas_cc_utilization_in_last_12_months -0.13\r## no_of_trades_opened_in_last_6_months -0.12\r## no_of_trades_opened_in_last_12_months -0.14\r## no_of_pl_trades_opened_in_last_6_months -0.14\r## no_of_pl_trades_opened_in_last_12_months -0.16\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans -0.12\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans -0.13\r## presence_of_open_home_loan 1.00\r## outstanding_balance 0.94\r## total_no_of_trades -0.10\r## presence_of_open_auto_loan 0.02\r## performance_tag -0.03\r## outstanding_balance\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months -0.02\r## no_of_times_60_dpd_or_worse_in_last_6_months -0.02\r## no_of_times_30_dpd_or_worse_in_last_6_months -0.02\r## no_of_times_90_dpd_or_worse_in_last_12_months -0.02\r## no_of_times_60_dpd_or_worse_in_last_12_months -0.02\r## no_of_times_30_dpd_or_worse_in_last_12_months -0.02\r## avgas_cc_utilization_in_last_12_months -0.03\r## no_of_trades_opened_in_last_6_months 0.17\r## no_of_trades_opened_in_last_12_months 0.19\r## no_of_pl_trades_opened_in_last_6_months 0.17\r## no_of_pl_trades_opened_in_last_12_months 0.20\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.11\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.12\r## presence_of_open_home_loan 0.94\r## outstanding_balance 1.00\r## total_no_of_trades 0.19\r## presence_of_open_auto_loan 0.05\r## performance_tag 0.00\r## total_no_of_trades\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.03\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.04\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.04\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.04\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.01\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.01\r## avgas_cc_utilization_in_last_12_months -0.01\r## no_of_trades_opened_in_last_6_months 0.89\r## no_of_trades_opened_in_last_12_months 0.94\r## no_of_pl_trades_opened_in_last_6_months 0.79\r## no_of_pl_trades_opened_in_last_12_months 0.83\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.71\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.78\r## presence_of_open_home_loan -0.10\r## outstanding_balance 0.19\r## total_no_of_trades 1.00\r## presence_of_open_auto_loan -0.04\r## performance_tag 0.04\r## presence_of_open_auto_loan\r## application_id 0.01\r## no_of_times_90_dpd_or_worse_in_last_6_months -0.03\r## no_of_times_60_dpd_or_worse_in_last_6_months -0.03\r## no_of_times_30_dpd_or_worse_in_last_6_months -0.03\r## no_of_times_90_dpd_or_worse_in_last_12_months -0.03\r## no_of_times_60_dpd_or_worse_in_last_12_months -0.02\r## no_of_times_30_dpd_or_worse_in_last_12_months -0.03\r## avgas_cc_utilization_in_last_12_months -0.04\r## no_of_trades_opened_in_last_6_months -0.04\r## no_of_trades_opened_in_last_12_months -0.05\r## no_of_pl_trades_opened_in_last_6_months -0.05\r## no_of_pl_trades_opened_in_last_12_months -0.05\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans -0.04\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans -0.05\r## presence_of_open_home_loan 0.02\r## outstanding_balance 0.05\r## total_no_of_trades -0.04\r## presence_of_open_auto_loan 1.00\r## performance_tag -0.01\r## performance_tag\r## application_id 0.00\r## no_of_times_90_dpd_or_worse_in_last_6_months 0.09\r## no_of_times_60_dpd_or_worse_in_last_6_months 0.10\r## no_of_times_30_dpd_or_worse_in_last_6_months 0.10\r## no_of_times_90_dpd_or_worse_in_last_12_months 0.10\r## no_of_times_60_dpd_or_worse_in_last_12_months 0.09\r## no_of_times_30_dpd_or_worse_in_last_12_months 0.10\r## avgas_cc_utilization_in_last_12_months 0.08\r## no_of_trades_opened_in_last_6_months 0.05\r## no_of_trades_opened_in_last_12_months 0.06\r## no_of_pl_trades_opened_in_last_6_months 0.07\r## no_of_pl_trades_opened_in_last_12_months 0.08\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans 0.05\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans 0.06\r## presence_of_open_home_loan -0.03\r## outstanding_balance 0.00\r## total_no_of_trades 0.04\r## presence_of_open_auto_loan -0.01\r## performance_tag 1.00\r## \u0026quot;C:/Program Files/RStudio/bin/pandoc/pandoc\u0026quot; +RTS -K512m -RTS \u0026quot;C:/Users/noura/Documents/Nour/content/post/2021-08-02-project/report.knit.md\u0026quot; --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash --output pandocfa4365b4ab4.html --lua-filter \u0026quot;C:\\Users\\noura\\Documents\\R\\win-library\\4.0\\rmarkdown\\rmarkdown\\lua\\pagebreak.lua\u0026quot; --lua-filter \u0026quot;C:\\Users\\noura\\Documents\\R\\win-library\\4.0\\rmarkdown\\rmarkdown\\lua\\latex-div.lua\u0026quot; --self-contained --variable bs3=TRUE --standalone --section-divs --table-of-contents --toc-depth 6 --template \u0026quot;C:\\Users\\noura\\Documents\\R\\win-library\\4.0\\rmarkdown\\rmd\\h\\default.html\u0026quot; --no-highlight --variable highlightjs=1 --variable theme=yeti --include-in-header \u0026quot;C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\RtmpQx62rR\\rmarkdown-strfa41a2d1371.html\u0026quot; --mathjax --variable \u0026quot;mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\u0026quot;\r## [1] TRUE\r## \u0026quot;C:/Program Files/RStudio/bin/pandoc/pandoc\u0026quot; +RTS -K512m -RTS \u0026quot;C:/Users/noura/Documents/Nour/content/post/2021-08-02-project/report.knit.md\u0026quot; --to html4 --from markdown+autolink_bare_uris+tex_math_single_backslash --output pandocfa474d44001.html --lua-filter \u0026quot;C:\\Users\\noura\\Documents\\R\\win-library\\4.0\\rmarkdown\\rmarkdown\\lua\\pagebreak.lua\u0026quot; --lua-filter \u0026quot;C:\\Users\\noura\\Documents\\R\\win-library\\4.0\\rmarkdown\\rmarkdown\\lua\\latex-div.lua\u0026quot; --self-contained --variable bs3=TRUE --standalone --section-divs --table-of-contents --toc-depth 6 --template \u0026quot;C:\\Users\\noura\\Documents\\R\\win-library\\4.0\\rmarkdown\\rmd\\h\\default.html\u0026quot; --no-highlight --variable highlightjs=1 --variable theme=yeti --include-in-header \u0026quot;C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\RtmpQx62rR\\rmarkdown-strfa490c38c9.html\u0026quot; --mathjax --variable \u0026quot;mathjax-url:https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\u0026quot;\r## [1] TRUE\r\rSplitting the data to training and testing splits\rIn this step we are going to split the data, so that 3/4 of the data would be assigned for training and the remaining 1/4 of the data would be assigned for testing.\n\rInformation Vlaue of the predictors\rInformation value is used to select important predictors in the predictive model, So for the first model that used only the Demographic data we can see that the predictors with high information value (greater than 0.02) are no_of_months_in_current_residence,income, age, no_of_months_in_current_company, and for the second model the predictors with the high information value are all the predictors except application_id,gender_profession, marital_status_at_the_time_of_application, education, presence_of_open_auto_loan, type_of_residence and no_of_dependents.\nBased on the table shown below, any predictors whose information value is less than 0.02 is considered not useful and accordingly will be discarded and not used in our predictive models.\nFrom the shown table of the information values of the first model predictors we can see that out of the 11 predictors the first 4 predictors only will be used as their information value is greater than 0.02.\nFrom the shown table of the information values of the second model predictors we can see that out of the 28 predictors the first 20 predictors only will be used as their information value is greater than 0.02.\n\rCreating bins for each predictor\rIn this part we will create number of bins for each predictor in the twos models.The bins for each predictor is created based on similarity of dependent variable distribution.\n## [INFO] creating woe binning ...\r## [INFO] creating woe binning ...\r\rWeight of evidence tranformation\rIn this part we are going to calculate the weight of evidence (WOE) of the continuous predictors that helps to transform a continuous independent variable into a set of bins. Woe tells the predictive power of an independent variable in relation to the dependent variable.\n## [INFO] converting into woe values ...\r## [INFO] converting into woe values ...\r## [INFO] converting into woe values ...\r## [INFO] converting into woe values ...\r\rCheck correct binning with weight of evidence (WOE)\rTo check that the binning with weight of evidence is correct the binning need to satisfy two criteria:\n1.The WOE should be monotonic.\r2.The slope resulted from a logistic regression with one independent variable vs the dependent variable must be equal to one or the intercept must be ln(% of non-events / % of events) .\nThe graphs below show the binning and WOE transformation associated with all the predictors in the two models, From the shown graphs, we can see that the Woe transformation of all the predictors in the two models satisfied the first criteria that all are monotonic.\n## $no_of_months_in_current_residence\r## $no_of_months_in_current_company\r## $age\r## $income\r## $no_of_times_90_dpd_or_worse_in_last_6_months\r## $no_of_times_60_dpd_or_worse_in_last_6_months\r## $no_of_times_30_dpd_or_worse_in_last_6_months\r## $no_of_times_90_dpd_or_worse_in_last_12_months\r## $no_of_times_60_dpd_or_worse_in_last_12_months\r## $no_of_times_30_dpd_or_worse_in_last_12_months\r## $avgas_cc_utilization_in_last_12_months\r## $no_of_trades_opened_in_last_6_months\r## $no_of_trades_opened_in_last_12_months\r## $no_of_pl_trades_opened_in_last_6_months\r## $no_of_pl_trades_opened_in_last_12_months\r## $no_of_inquiries_in_last_6_months_excluding_home_auto_loans\r## $no_of_inquiries_in_last_12_months_excluding_home_auto_loans\r## $total_no_of_trades\r## $no_of_months_in_current_residence\r## $no_of_months_in_current_company\r## $income\r## $age\r## $outstanding_balance\rThe logistic regressions resulted from each predictor in the two models Vs the dependent variable show that the slope associated with each regression is one as required.\n## ## Call: glm(formula = performance_tag ~ no_of_months_in_current_residence_woe, ## family = binomial, data = woe_demogs_training_data)\r## ## Coefficients:\r## (Intercept) no_of_months_in_current_residence_woe ## -3.123 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18310 ## Residual Deviance: 18100 AIC: 18110\r## ## Call: glm(formula = performance_tag ~ no_of_months_in_current_residence_woe, ## family = binomial, data = woe_demogs_training_data)\r## ## Coefficients:\r## (Intercept) no_of_months_in_current_residence_woe ## -3.123 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18310 ## Residual Deviance: 18100 AIC: 18110\r## ## Call: glm(formula = performance_tag ~ age_woe, family = binomial, data = woe_demogs_training_data)\r## ## Coefficients:\r## (Intercept) age_woe ## -3.123 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18310 ## Residual Deviance: 18300 AIC: 18300\r## ## Call: glm(formula = performance_tag ~ income_woe, family = binomial, ## data = woe_demogs_training_data)\r## ## Coefficients:\r## (Intercept) income_woe ## -3.123 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18310 ## Residual Deviance: 18240 AIC: 18240\r## ## Call: glm(formula = performance_tag ~ no_of_times_90_dpd_or_worse_in_last_6_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_times_90_dpd_or_worse_in_last_6_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 18020 AIC: 18030\r## ## Call: glm(formula = performance_tag ~ no_of_times_60_dpd_or_worse_in_last_6_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_times_60_dpd_or_worse_in_last_6_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17930 AIC: 17930\r## ## Call: glm(formula = performance_tag ~ no_of_times_30_dpd_or_worse_in_last_6_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_times_30_dpd_or_worse_in_last_6_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17860 AIC: 17860\r## ## Call: glm(formula = performance_tag ~ no_of_times_90_dpd_or_worse_in_last_12_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_times_90_dpd_or_worse_in_last_12_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17900 AIC: 17910\r## ## Call: glm(formula = performance_tag ~ no_of_times_60_dpd_or_worse_in_last_12_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_times_60_dpd_or_worse_in_last_12_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17970 AIC: 17980\r## ## Call: glm(formula = performance_tag ~ no_of_times_30_dpd_or_worse_in_last_12_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_times_30_dpd_or_worse_in_last_12_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17920 AIC: 17920\r## ## Call: glm(formula = performance_tag ~ avgas_cc_utilization_in_last_12_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## avgas_cc_utilization_in_last_12_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17760 AIC: 17760\r## ## Call: glm(formula = performance_tag ~ no_of_trades_opened_in_last_6_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_trades_opened_in_last_6_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 18010 AIC: 18020\r## ## Call: glm(formula = performance_tag ~ no_of_trades_opened_in_last_12_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_trades_opened_in_last_12_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17850 AIC: 17850\r## ## Call: glm(formula = performance_tag ~ no_of_pl_trades_opened_in_last_6_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_pl_trades_opened_in_last_6_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17950 AIC: 17950\r## ## Call: glm(formula = performance_tag ~ no_of_pl_trades_opened_in_last_12_months_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_pl_trades_opened_in_last_12_months_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17810 AIC: 17820\r## ## Call: glm(formula = performance_tag ~ no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 18000 AIC: 18010\r## ## Call: glm(formula = performance_tag ~ no_of_inquiries_in_last_12_months_excluding_home_auto_loans_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) ## -3.117 ## no_of_inquiries_in_last_12_months_excluding_home_auto_loans_woe ## 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17880 AIC: 17880\r## ## Call: glm(formula = performance_tag ~ total_no_of_trades_woe, family = binomial, ## data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) total_no_of_trades_woe ## -3.117 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 17900 AIC: 17900\r## ## Call: glm(formula = performance_tag ~ no_of_months_in_current_residence_woe, ## family = binomial, data = woe_both_training_data)\r## ## Coefficients:\r## (Intercept) no_of_months_in_current_residence_woe ## -3.117 1.000 ## ## Degrees of Freedom: 52401 Total (i.e. Null); 52400 Residual\r## Null Deviance: 18390 ## Residual Deviance: 18210 AIC: 18220\r\rData Balance\rAs an important step before using the data in predictive models is to check the data balance, where the imbalanced data refers to the number of observations in the training dataset for each class is not balanced. If the data is imbalance it would result in poor performance for the predictive models because of the severely skewed class distribution.\nThe training splits for the both the demographic and combined data are of 52402 observations out of which 50193 observations for 0 performance_tag class and 2209 observations for 1 performance_tag class.so the percentage of the customers with performance tag equal to one is around 4.2 % in our training data sets for both the Demographic and combined dataframe, which is considered very small portion and resulted in imbalanced data sets.\nperformance_tag n\r \r0 50193\r1 2209\n ## Checking the balance of the data with respect to the performance_tag # first the demogs training data data\rcount_demogs_data_train\u0026lt;- training_data_d%\u0026gt;% count(performance_tag)\rbad_percent_demogs_train\u0026lt;-count_demogs_data_train$n[2]*100/(count_demogs_data_train$n[1]+count_demogs_data_train$n[2])\r##################################################################################################################################\r# Third for the combined training data count_both_data_train\u0026lt;- training_data_both%\u0026gt;% count(performance_tag)\rbad_percent_both_train\u0026lt;-count_both_data_train$n[2]*100/(count_both_data_train$n[1]+count_both_data_train$n[2])\r\rBalance the imbalanced data\rTo balance the imbalanced data we need to artificially create 1 performace_tag observations. Smote function is used to handles unbalanced classification problems using the SMOTE method. Namely, it can generate a new “SMOTEd” data set that addresses the class unbalance problem.\nThe resulted balanced data is as shown:\n0 1\r4418 4418\n0 1\r4444 4444\n ## Handle the unbalanced data\rset.seed(1000)\rtrain_data_balanced_over \u0026lt;- smote(performance_tag ~ ., woe_demogs_training_data, perc.over = 1,perc.under=2)\rtable(train_data_balanced_over$performance_tag)\r## ## 0 1 ## 4418 4418\r set.seed(2000)\rtrain_data_balanced_over_both \u0026lt;- smote(performance_tag ~ ., woe_both_training_data, perc.over =1 ,perc.under=2)\rtable(train_data_balanced_over_both$performance_tag)\r## ## 0 1 ## 4444 4444\rLogistic Regression Model\rTwo logistic regression models are created.First logistic regression predictive model is created that used only the demographic balanced WOE transformed training data split for training the model and the second logistic regression predictive model is created that used combined balanced WOE transformed training data split for training the model.\n1st model (using only the demographic data) gave the following results on the testing data\nAccuracy: 0.569\rSensitivity: 0.569\rSpecificity: 0.569\n2nd model (using the combined data) gave the following results on the testing data\nAccuracy: 0.625\rSensitivity: 0.626\rSpecificity: 0.606\n ## Logistic Regression Model based on the demographic data only\rmdl_demogs \u0026lt;- glm(performance_tag ~ ., data = train_data_balanced_over, family = \u0026quot;binomial\u0026quot;)\rsummary(mdl_demogs)\r## ## Call:\r## glm(formula = performance_tag ~ ., family = \u0026quot;binomial\u0026quot;, data = train_data_balanced_over)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.70623 -1.11655 -0.03809 1.14711 1.61500 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) 0.002424 0.021718 0.112 0.911139 ## no_of_months_in_current_residence_woe 0.925603 0.070452 13.138 \u0026lt; 2e-16 ***\r## no_of_months_in_current_company_woe 0.878699 0.116891 7.517 5.59e-14 ***\r## income_woe 0.905844 0.115906 7.815 5.48e-15 ***\r## age_woe 0.962500 0.275344 3.496 0.000473 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 12249 on 8835 degrees of freedom\r## Residual deviance: 11891 on 8831 degrees of freedom\r## AIC: 11901\r## ## Number of Fisher Scoring iterations: 4\rvif(mdl_demogs)\r## no_of_months_in_current_residence_woe no_of_months_in_current_company_woe ## 1.017258 1.003238 ## income_woe age_woe ## 1.017341 1.002854\r# Predict on the testing data\rpred_demogs\u0026lt;- woe_demogs_testing_data %\u0026gt;%\rselect(performance_tag)%\u0026gt;%\rmutate(demogs_class=(predict(mdl_demogs, woe_demogs_testing_data,type=\u0026quot;response\u0026quot;)))\r# Roc Curve\rdemogs_cl=(predict(mdl_demogs, woe_demogs_testing_data,type=\u0026quot;response\u0026quot;))\rROCRpred \u0026lt;-prediction(demogs_cl, woe_demogs_testing_data$performance_tag)\r# Performance function\rROCRperf = performance(ROCRpred, \u0026quot;tpr\u0026quot;, \u0026quot;fpr\u0026quot;)\r# Plot ROC curve,add colors and threshold labels plot(ROCRperf, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))\r# optimizing the model through threshold value\rpred_demogs$demogs_class\u0026lt;-ifelse(pred_demogs$demogs_class\u0026gt;=0.487,1,0)\rpred_demogs$demogs_class\u0026lt;- as.factor(pred_demogs$demogs_class)\r# Evaluating the model performance on the training data\r# Confusion Matrix:\rconf_mt_demogs\u0026lt;- conf_mat(pred_demogs,truth = performance_tag,estimate = demogs_class)\rautoplot(conf_mt_demogs,type=\u0026#39;heatmap\u0026#39;)\r# Accuracy:\rAcc_demogs\u0026lt;- accuracy(pred_demogs,truth = performance_tag,estimate = demogs_class)\r# Sensitivity:\rsens_demogs\u0026lt;- sens(pred_demogs,truth = performance_tag,estimate = demogs_class)\r# specificity:\rspec_demogs\u0026lt;-spec(pred_demogs,truth = performance_tag,estimate = demogs_class)\r ## Logistic Regression Model based on the combined full data mdl_both \u0026lt;- glm(performance_tag ~ ., data = train_data_balanced_over_both, family = \u0026quot;binomial\u0026quot;)\rsummary(mdl_both)\r## ## Call:\r## glm(formula = performance_tag ~ ., family = \u0026quot;binomial\u0026quot;, data = train_data_balanced_over_both)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.78966 -1.09130 0.01439 1.02457 1.83339 ## ## Coefficients:\r## Estimate\r## (Intercept) 0.002271\r## no_of_times_90_dpd_or_worse_in_last_6_months_woe 0.161435\r## no_of_times_60_dpd_or_worse_in_last_6_months_woe -0.417841\r## no_of_times_30_dpd_or_worse_in_last_6_months_woe 0.415634\r## no_of_times_90_dpd_or_worse_in_last_12_months_woe 0.115053\r## no_of_times_60_dpd_or_worse_in_last_12_months_woe -0.091736\r## no_of_times_30_dpd_or_worse_in_last_12_months_woe 0.292451\r## avgas_cc_utilization_in_last_12_months_woe 0.470732\r## no_of_trades_opened_in_last_6_months_woe 0.113587\r## no_of_trades_opened_in_last_12_months_woe 0.433156\r## no_of_pl_trades_opened_in_last_6_months_woe -0.004741\r## no_of_pl_trades_opened_in_last_12_months_woe -0.179778\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe 0.136610\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans_woe 0.210412\r## outstanding_balance_woe 0.155117\r## total_no_of_trades_woe -0.196505\r## presence_of_open_auto_loan_woe 0.731419\r## age_woe 1.007612\r## income_woe 0.357613\r## no_of_months_in_current_residence_woe -0.154731\r## no_of_months_in_current_company_woe 0.534516\r## Std. Error\r## (Intercept) 0.022538\r## no_of_times_90_dpd_or_worse_in_last_6_months_woe 0.138386\r## no_of_times_60_dpd_or_worse_in_last_6_months_woe 0.202286\r## no_of_times_30_dpd_or_worse_in_last_6_months_woe 0.190004\r## no_of_times_90_dpd_or_worse_in_last_12_months_woe 0.107419\r## no_of_times_60_dpd_or_worse_in_last_12_months_woe 0.141174\r## no_of_times_30_dpd_or_worse_in_last_12_months_woe 0.147437\r## avgas_cc_utilization_in_last_12_months_woe 0.060345\r## no_of_trades_opened_in_last_6_months_woe 0.097469\r## no_of_trades_opened_in_last_12_months_woe 0.140722\r## no_of_pl_trades_opened_in_last_6_months_woe 0.109121\r## no_of_pl_trades_opened_in_last_12_months_woe 0.145382\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe 0.085239\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans_woe 0.095570\r## outstanding_balance_woe 0.090264\r## total_no_of_trades_woe 0.096287\r## presence_of_open_auto_loan_woe 0.674052\r## age_woe 0.273021\r## income_woe 0.111428\r## no_of_months_in_current_residence_woe 0.094014\r## no_of_months_in_current_company_woe 0.122392\r## z value\r## (Intercept) 0.101\r## no_of_times_90_dpd_or_worse_in_last_6_months_woe 1.167\r## no_of_times_60_dpd_or_worse_in_last_6_months_woe -2.066\r## no_of_times_30_dpd_or_worse_in_last_6_months_woe 2.187\r## no_of_times_90_dpd_or_worse_in_last_12_months_woe 1.071\r## no_of_times_60_dpd_or_worse_in_last_12_months_woe -0.650\r## no_of_times_30_dpd_or_worse_in_last_12_months_woe 1.984\r## avgas_cc_utilization_in_last_12_months_woe 7.801\r## no_of_trades_opened_in_last_6_months_woe 1.165\r## no_of_trades_opened_in_last_12_months_woe 3.078\r## no_of_pl_trades_opened_in_last_6_months_woe -0.043\r## no_of_pl_trades_opened_in_last_12_months_woe -1.237\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe 1.603\r## no_of_inquiries_in_last_12_months_excluding_home_auto_loans_woe 2.202\r## outstanding_balance_woe 1.718\r## total_no_of_trades_woe -2.041\r## presence_of_open_auto_loan_woe 1.085\r## age_woe 3.691\r## income_woe 3.209\r## no_of_months_in_current_residence_woe -1.646\r## no_of_months_in_current_company_woe 4.367\r## Pr(\u0026gt;|z|) ## (Intercept) 0.919755 ## no_of_times_90_dpd_or_worse_in_last_6_months_woe 0.243389 ## no_of_times_60_dpd_or_worse_in_last_6_months_woe 0.038866 * ## no_of_times_30_dpd_or_worse_in_last_6_months_woe 0.028706 * ## no_of_times_90_dpd_or_worse_in_last_12_months_woe 0.284135 ## no_of_times_60_dpd_or_worse_in_last_12_months_woe 0.515817 ## no_of_times_30_dpd_or_worse_in_last_12_months_woe 0.047304 * ## avgas_cc_utilization_in_last_12_months_woe 6.16e-15 ***\r## no_of_trades_opened_in_last_6_months_woe 0.243873 ## no_of_trades_opened_in_last_12_months_woe 0.002083 ** ## no_of_pl_trades_opened_in_last_6_months_woe 0.965345 ## no_of_pl_trades_opened_in_last_12_months_woe 0.216238 ## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe 0.109009 ## no_of_inquiries_in_last_12_months_excluding_home_auto_loans_woe 0.027689 * ## outstanding_balance_woe 0.085711 . ## total_no_of_trades_woe 0.041269 * ## presence_of_open_auto_loan_woe 0.277874 ## age_woe 0.000224 ***\r## income_woe 0.001330 ** ## no_of_months_in_current_residence_woe 0.099800 . ## no_of_months_in_current_company_woe 1.26e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 12321 on 8887 degrees of freedom\r## Residual deviance: 11316 on 8867 degrees of freedom\r## AIC: 11358\r## ## Number of Fisher Scoring iterations: 4\rvif(mdl_both)\r## no_of_times_90_dpd_or_worse_in_last_6_months_woe ## 6.371268 ## no_of_times_60_dpd_or_worse_in_last_6_months_woe ## 17.414091 ## no_of_times_30_dpd_or_worse_in_last_6_months_woe ## 17.679266 ## no_of_times_90_dpd_or_worse_in_last_12_months_woe ## 5.196993 ## no_of_times_60_dpd_or_worse_in_last_12_months_woe ## 7.691027 ## no_of_times_30_dpd_or_worse_in_last_12_months_woe ## 9.389353 ## avgas_cc_utilization_in_last_12_months_woe ## 2.260409 ## no_of_trades_opened_in_last_6_months_woe ## 3.445880 ## no_of_trades_opened_in_last_12_months_woe ## 10.710460 ## no_of_pl_trades_opened_in_last_6_months_woe ## 5.100970 ## no_of_pl_trades_opened_in_last_12_months_woe ## 12.119244 ## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe ## 2.714159 ## no_of_inquiries_in_last_12_months_excluding_home_auto_loans_woe ## 4.656748 ## outstanding_balance_woe ## 3.984862 ## total_no_of_trades_woe ## 4.366908 ## presence_of_open_auto_loan_woe ## 1.072847 ## age_woe ## 1.005329 ## income_woe ## 1.090991 ## no_of_months_in_current_residence_woe ## 1.466685 ## no_of_months_in_current_company_woe ## 1.034017\r# updating the logistic model with the predictors vif values \u0026lt;= 2\rupdated1_mdl_both \u0026lt;- glm(performance_tag ~no_of_times_30_dpd_or_worse_in_last_12_months_woe+avgas_cc_utilization_in_last_12_months_woe+no_of_trades_opened_in_last_12_months_woe+no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe+age_woe+no_of_months_in_current_company_woe+income_woe+no_of_months_in_current_residence_woe , data = train_data_balanced_over_both, family = \u0026quot;binomial\u0026quot;)\rsummary(updated1_mdl_both)\r## ## Call:\r## glm(formula = performance_tag ~ no_of_times_30_dpd_or_worse_in_last_12_months_woe + ## avgas_cc_utilization_in_last_12_months_woe + no_of_trades_opened_in_last_12_months_woe + ## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe + ## age_woe + no_of_months_in_current_company_woe + income_woe + ## no_of_months_in_current_residence_woe, family = \u0026quot;binomial\u0026quot;, ## data = train_data_balanced_over_both)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7363 -1.0830 0.0248 1.0269 1.8222 ## ## Coefficients:\r## Estimate\r## (Intercept) 0.00110\r## no_of_times_30_dpd_or_worse_in_last_12_months_woe 0.42175\r## avgas_cc_utilization_in_last_12_months_woe 0.50465\r## no_of_trades_opened_in_last_12_months_woe 0.41858\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe 0.24610\r## age_woe 0.99032\r## no_of_months_in_current_company_woe 0.53265\r## income_woe 0.36258\r## no_of_months_in_current_residence_woe -0.16932\r## Std. Error\r## (Intercept) 0.02248\r## no_of_times_30_dpd_or_worse_in_last_12_months_woe 0.05779\r## avgas_cc_utilization_in_last_12_months_woe 0.05606\r## no_of_trades_opened_in_last_12_months_woe 0.05931\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe 0.06575\r## age_woe 0.27247\r## no_of_months_in_current_company_woe 0.12209\r## income_woe 0.11100\r## no_of_months_in_current_residence_woe 0.09166\r## z value Pr(\u0026gt;|z|)\r## (Intercept) 0.049 0.960987\r## no_of_times_30_dpd_or_worse_in_last_12_months_woe 7.298 2.92e-13\r## avgas_cc_utilization_in_last_12_months_woe 9.002 \u0026lt; 2e-16\r## no_of_trades_opened_in_last_12_months_woe 7.058 1.69e-12\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe 3.743 0.000182\r## age_woe 3.635 0.000278\r## no_of_months_in_current_company_woe 4.363 1.28e-05\r## income_woe 3.266 0.001090\r## no_of_months_in_current_residence_woe -1.847 0.064715\r## ## (Intercept) ## no_of_times_30_dpd_or_worse_in_last_12_months_woe ***\r## avgas_cc_utilization_in_last_12_months_woe ***\r## no_of_trades_opened_in_last_12_months_woe ***\r## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe ***\r## age_woe ***\r## no_of_months_in_current_company_woe ***\r## income_woe ** ## no_of_months_in_current_residence_woe . ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 12321 on 8887 degrees of freedom\r## Residual deviance: 11340 on 8879 degrees of freedom\r## AIC: 11358\r## ## Number of Fisher Scoring iterations: 4\rvif(updated1_mdl_both)\r## no_of_times_30_dpd_or_worse_in_last_12_months_woe ## 1.440298 ## avgas_cc_utilization_in_last_12_months_woe ## 1.959183 ## no_of_trades_opened_in_last_12_months_woe ## 1.913825 ## no_of_inquiries_in_last_6_months_excluding_home_auto_loans_woe ## 1.629539 ## age_woe ## 1.003555 ## no_of_months_in_current_company_woe ## 1.032024 ## income_woe ## 1.085803 ## no_of_months_in_current_residence_woe ## 1.396972\rupdated1_mdl_both$performance_tag\u0026lt;- as.factor(updated1_mdl_both$performance_tag)\r# Predict on the testing data\rpred_both\u0026lt;- woe_both_testing_data %\u0026gt;%\rselect(performance_tag)%\u0026gt;%\rmutate(both_class=(predict(updated1_mdl_both, woe_both_testing_data,type=\u0026quot;response\u0026quot;)))\r# Roc Curve\rboth_cl=(predict(updated1_mdl_both, woe_both_testing_data,type=\u0026quot;response\u0026quot;))\rROCRpred_both \u0026lt;-prediction(both_cl, woe_both_testing_data$performance_tag)\r# Performance function\rROCRperf_both = performance(ROCRpred_both, \u0026quot;tpr\u0026quot;, \u0026quot;fpr\u0026quot;)\r# Plot ROC curve,add colors and threshold labels plot(ROCRperf_both, colorize=TRUE, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))\r# optimizing the model through threshold value\rpred_both$both_class\u0026lt;-ifelse(pred_both$both_class\u0026gt;=0.53,1,0)\rpred_both$both_class\u0026lt;- as.factor(pred_both$both_class)\r# Evaluating the model performance on the training data\r# Confusion Matrix:\rconf_mt_both\u0026lt;- conf_mat(pred_both,truth = performance_tag,estimate = both_class)\rautoplot(conf_mt_both,type=\u0026#39;heatmap\u0026#39;)\r# Accuracy:\rAcc_both\u0026lt;- accuracy(pred_both,truth = performance_tag,estimate = both_class)\r# Sensitivity:\rsens_both\u0026lt;- sens(pred_both,truth = performance_tag,estimate = both_class)\r# specificity:\rspec_both\u0026lt;-spec(pred_both,truth = performance_tag,estimate = both_class)\r\r\rCross validation\rThe Cross validation is used for resampling and evaluation of the performance of our logistic regression models.\nFor the 1st logistic regression model running the cross validation with 10 folds gave us an average accuracy of 0.587%, while for the 2nd logistic regression model running the cross validation with 10 folds gave us an average accuracy of 0.631%\n# Random seed for reproducibility\rset.seed(1)\rmdl_cv_demogs \u0026lt;- logistic_reg()%\u0026gt;%\rset_engine(\u0026#39;glm\u0026#39;) %\u0026gt;%\rset_mode(\u0026#39;classification\u0026#39;)\r# Create 10 folds of the dataset\rfolds_demogs \u0026lt;- vfold_cv(train_data_balanced_over, v = 10,strata = performance_tag)\r# Fit a model for every fold and calculate MAE and RMSE\rfits_cv_demogs \u0026lt;- fit_resamples(mdl_cv_demogs,\rperformance_tag ~ .,\rresamples = folds_demogs,\rmetrics = metric_set(accuracy, spec,sens))\r# Collect raw errors of all model runs\rall_errors_demogs\u0026lt;- collect_metrics(fits_cv_demogs,summarize = FALSE)\rprint(all_errors_demogs)\r## Found more than one class \u0026quot;tbl_df\u0026quot; in cache; using the first, from namespace \u0026#39;tibble\u0026#39;\r## Also defined by \u0026#39;memisc\u0026#39;\r## Found more than one class \u0026quot;tbl_df\u0026quot; in cache; using the first, from namespace \u0026#39;tibble\u0026#39;\r## Also defined by \u0026#39;memisc\u0026#39;\r## # A tibble: 30 x 5\r## id .metric .estimator .estimate .config ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Fold01 accuracy binary 0.572 Preprocessor1_Model1\r## 2 Fold01 spec binary 0.534 Preprocessor1_Model1\r## 3 Fold01 sens binary 0.611 Preprocessor1_Model1\r## 4 Fold02 accuracy binary 0.580 Preprocessor1_Model1\r## 5 Fold02 spec binary 0.550 Preprocessor1_Model1\r## 6 Fold02 sens binary 0.611 Preprocessor1_Model1\r## 7 Fold03 accuracy binary 0.586 Preprocessor1_Model1\r## 8 Fold03 spec binary 0.575 Preprocessor1_Model1\r## 9 Fold03 sens binary 0.597 Preprocessor1_Model1\r## 10 Fold04 accuracy binary 0.597 Preprocessor1_Model1\r## # ... with 20 more rows\rlibrary(ggplot2)\rggplot(all_errors_demogs, aes(x = .estimate,fill = .metric)) +\rgeom_histogram()\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r# Collect and summarize errors of all model runs\rcollect_metrics(fits_cv_demogs)\r## # A tibble: 3 x 6\r## .metric .estimator mean n std_err .config ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 accuracy binary 0.591 10 0.00528 Preprocessor1_Model1\r## 2 sens binary 0.625 10 0.00739 Preprocessor1_Model1\r## 3 spec binary 0.558 10 0.00720 Preprocessor1_Model1\r# Random seed for reproducibility\rset.seed(131)\rmdl_cv_both\u0026lt;-logistic_reg()%\u0026gt;%\rset_engine(\u0026#39;glm\u0026#39;) %\u0026gt;%\rset_mode(\u0026#39;classification\u0026#39;)\r# Create 10 folds of the dataset\rfolds_both \u0026lt;- vfold_cv(train_data_balanced_over_both, v = 10,strata = performance_tag)\r# Fit a model for every fold and calculate MAE and RMSE\rfits_cv_both \u0026lt;- fit_resamples(mdl_cv_both,\rperformance_tag ~ .,\rresamples = folds_both,\rmetrics = metric_set(accuracy, spec,sens))\r# Collect raw errors of all model runs\rall_errors_both\u0026lt;- collect_metrics(fits_cv_both,summarize = FALSE)\rprint(all_errors_both)\r## # A tibble: 30 x 5\r## id .metric .estimator .estimate .config ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Fold01 accuracy binary 0.625 Preprocessor1_Model1\r## 2 Fold01 spec binary 0.670 Preprocessor1_Model1\r## 3 Fold01 sens binary 0.580 Preprocessor1_Model1\r## 4 Fold02 accuracy binary 0.620 Preprocessor1_Model1\r## 5 Fold02 spec binary 0.663 Preprocessor1_Model1\r## 6 Fold02 sens binary 0.578 Preprocessor1_Model1\r## 7 Fold03 accuracy binary 0.635 Preprocessor1_Model1\r## 8 Fold03 spec binary 0.679 Preprocessor1_Model1\r## 9 Fold03 sens binary 0.591 Preprocessor1_Model1\r## 10 Fold04 accuracy binary 0.652 Preprocessor1_Model1\r## # ... with 20 more rows\rlibrary(ggplot2)\rggplot(all_errors_both, aes(x = .estimate,fill = .metric)) +\rgeom_histogram()\r## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\r# Collect and summarize errors of all model runs\rcollect_metrics(fits_cv_both)\r## # A tibble: 3 x 6\r## .metric .estimator mean n std_err .config ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 accuracy binary 0.634 10 0.00361 Preprocessor1_Model1\r## 2 sens binary 0.579 10 0.00438 Preprocessor1_Model1\r## 3 spec binary 0.690 10 0.00984 Preprocessor1_Model1\r\rConclusion from the Logistic regression models\r\rComparing the first model accuracy of 0.569% to the cross validation accuracy of 0.587% , we can see that our model is well built.\n\rComparing the second model accuracy of 0.625% to the cross validation accuracy of 0.631% , we can see that our model is well built.\n\rComparing the first model accuracy of 0.569% to the second model accuracy of 0.625% , we can see that the second model is better in prediction than the first model, so that the second model that used the combined data was better than the first model that used only the demographic data.\n\r\r\rBuilding Support Vector Machine Model using demographics dataset\rThe support vector Machine model is built for performance tag vs other predictors from the demographic dataset with the balanced training data with the C-classification type and radial kernel.\nThe hyperplane of the previous classifier is used here to predict the performance tag of demographic testing data.\n\rBuilding Support Vector Machine Model using both demographics and credit bureau datasets\rThe support vector Machine model is built for performance tag vs other predictors from the demographic dataset with the balanced training data with the C-classification type and radial kernel.\nThe hyperplane of the previous classifier is used here to predict the performance tag of combined testing data.\nSensitivity and Specificity check of SVM\r\rAccuracy Check of SVM\r\r\r","date":1627862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627957489,"objectID":"35cc28e7f9c43fff9187b46f1a5cc7e5","permalink":"/post/project/","publishdate":"2021-08-02T00:00:00Z","relpermalink":"/post/project/","section":"post","summary":"The Goal\rThe Goal of this study is to help the credit card provider company CredX to identify the right customer based on predictive model where the past data of the bank’s applicants was used to built the predictive model and determine the factors affecting credit risk.","tags":[],"title":"project","type":"post"},{"authors":[],"categories":["ggplot2","Basics","caret","dplyr","tidymodels","yardstick"],"content":"\r\rThe Goal\rThe goal behind this study is to use the decision tree to build a tree-based model for prediction, so that we will use the provided data set to identify whether a customer will subscribe to a term deposit or not.\n\rThe data\rThe data used in this study was obtained from UCI website and we specifically used the Bank-full.csv dataset.\n\rSplitting the data to training and testing splits\rIn this step we are going to split the data so that 3/4 of the data would be assigned for training and the remaining 1/4 of the data would be assigned for testing.\n\rCreate the decision tree model\rCreating the decision tree model that will be used to identify whether a customer will subscribe to a term deposit or not.\ndecision_tree_mdl \u0026lt;- decision_tree() %\u0026gt;%\r# Specify the engine\rset_engine(\u0026#39;rpart\u0026#39;) %\u0026gt;%\r# Specify the mode\rset_mode(\u0026#39;classification\u0026#39;)\rdecision_tree_mdl\r## Decision Tree Model Specification (classification)\r## ## Computational engine: rpart\r\rCreating the recipe\rCreating the suitable recipe to apply feature engineering to our data\n## Data Recipe\r## ## Inputs:\r## ## role #variables\r## outcome 1\r## predictor 16\r## ## Operations:\r## ## Correlation filter on all_numeric()\r## Centering and scaling for all_numeric()\r## Dummy variables from all_nominal(), -all_outcomes()\r## Zero variance filter on all_predictors()\r\rCreate workflow\rCreate a workflow that combine the model and the recipe.\n## == Workflow ====================================================================\r## Preprocessor: Recipe\r## Model: decision_tree()\r## ## -- Preprocessor ----------------------------------------------------------------\r## 4 Recipe Steps\r## ## * step_corr()\r## * step_normalize()\r## * step_dummy()\r## * step_zv()\r## ## -- Model -----------------------------------------------------------------------\r## Decision Tree Model Specification (classification)\r## ## Computational engine: rpart\r\rFit the model\rTrain and fitting the model using the training split of the data.\n## == Workflow [trained] ==========================================================\r## Preprocessor: Recipe\r## Model: decision_tree()\r## ## -- Preprocessor ----------------------------------------------------------------\r## 4 Recipe Steps\r## ## * step_corr()\r## * step_normalize()\r## * step_dummy()\r## * step_zv()\r## ## -- Model -----------------------------------------------------------------------\r## n= 33907 ## ## node), split, n, loss, yval, (yprob)\r## * denotes terminal node\r## ## 1) root 33907 3966 no (0.88303300 0.11696700) ## 2) duration\u0026lt; 1.013834 30124 2309 no (0.92335015 0.07664985) ## 4) poutcome_success\u0026lt; 0.5 29150 1715 no (0.94116638 0.05883362) *\r## 5) poutcome_success\u0026gt;=0.5 974 380 yes (0.39014374 0.60985626) ## 10) duration\u0026lt; -0.4872674 190 38 no (0.80000000 0.20000000) *\r## 11) duration\u0026gt;=-0.4872674 784 228 yes (0.29081633 0.70918367) *\r## 3) duration\u0026gt;=1.013834 3783 1657 no (0.56198784 0.43801216) ## 6) duration\u0026lt; 2.215492 2461 881 no (0.64201544 0.35798456) ## 12) poutcome_success\u0026lt; 0.5 2344 785 no (0.66510239 0.33489761) *\r## 13) poutcome_success\u0026gt;=0.5 117 21 yes (0.17948718 0.82051282) *\r## 7) duration\u0026gt;=2.215492 1322 546 yes (0.41301059 0.58698941) *\r\rMake predictions for training data\rUsing the trained model for prediction on the training data.\n## # A tibble: 33,907 x 18\r## .pred_class age job marital education default balance housing loan ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt;\r## 1 no 58 management married tertiary no 2143 yes no ## 2 no 44 technician single secondary no 29 yes no ## 3 no 47 blue-collar married unknown no 1506 yes no ## 4 no 35 management married tertiary no 231 yes no ## 5 no 28 management single tertiary no 447 yes yes ## 6 no 42 entreprene~ divorc~ tertiary yes 2 yes no ## 7 no 58 retired married primary no 121 yes no ## 8 no 43 technician single secondary no 593 yes no ## 9 no 29 admin. single secondary no 390 yes no ## 10 no 53 technician married secondary no 6 yes no ## # ... with 33,897 more rows, and 9 more variables: contact \u0026lt;fct\u0026gt;, day \u0026lt;chr\u0026gt;,\r## # month \u0026lt;fct\u0026gt;, duration \u0026lt;int\u0026gt;, campaign \u0026lt;int\u0026gt;, pdays \u0026lt;int\u0026gt;, previous \u0026lt;int\u0026gt;,\r## # poutcome \u0026lt;fct\u0026gt;, y \u0026lt;fct\u0026gt;\r\rEvaluate performance on tarining data\rEvaluate the performance of the model on the training data.The accuracy of the model on the training data is 90.2%.\n## Truth\r## Prediction no yes\r## no 29146 2538\r## yes 795 1428\r## # A tibble: 3 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.902\r## 2 sens binary 0.973\r## 3 spec binary 0.360\r\rMake predictions for testing data\rUsing the trained model for prediction on the testing data.\n## # A tibble: 11,304 x 18\r## .pred_class age job marital education default balance housing loan ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt;\r## 1 no 33 entreprene~ married secondary no 2 yes yes ## 2 no 33 unknown single unknown no 1 no no ## 3 no 41 admin. divorc~ secondary no 270 yes no ## 4 no 57 services married secondary no 162 yes no ## 5 no 45 admin. single unknown no 13 yes no ## 6 no 56 management married tertiary no 779 yes no ## 7 no 40 retired married primary no 0 yes yes ## 8 no 46 management single secondary no -246 yes no ## 9 no 36 technician single secondary no 265 yes yes ## 10 no 57 technician married secondary no 839 no yes ## # ... with 11,294 more rows, and 9 more variables: contact \u0026lt;fct\u0026gt;, day \u0026lt;chr\u0026gt;,\r## # month \u0026lt;fct\u0026gt;, duration \u0026lt;int\u0026gt;, campaign \u0026lt;int\u0026gt;, pdays \u0026lt;int\u0026gt;, previous \u0026lt;int\u0026gt;,\r## # poutcome \u0026lt;fct\u0026gt;, y \u0026lt;fct\u0026gt;\r\rEvaluate performance on testing data\rEvaluate the performance of the model on the testing data.The accuracy of the model on the testing data is 90%.\n## Truth\r## Prediction no yes\r## no 9698 847\r## yes 283 476\r## # A tibble: 3 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.900\r## 2 sens binary 0.972\r## 3 spec binary 0.360\r\rApply Cross Validation method for model evaluation\rWe will use the cross-validation resampling method to evaluate the performance of our decision tree model.\nSplit data into folds\rThe train data is split-ted into 10 folds\n## # 10-fold cross-validation using stratification ## # A tibble: 10 x 2\r## splits id ## \u0026lt;list\u0026gt; \u0026lt;chr\u0026gt; ## 1 \u0026lt;split [30515/3392]\u0026gt; Fold01\r## 2 \u0026lt;split [30516/3391]\u0026gt; Fold02\r## 3 \u0026lt;split [30516/3391]\u0026gt; Fold03\r## 4 \u0026lt;split [30516/3391]\u0026gt; Fold04\r## 5 \u0026lt;split [30516/3391]\u0026gt; Fold05\r## 6 \u0026lt;split [30516/3391]\u0026gt; Fold06\r## 7 \u0026lt;split [30517/3390]\u0026gt; Fold07\r## 8 \u0026lt;split [30517/3390]\u0026gt; Fold08\r## 9 \u0026lt;split [30517/3390]\u0026gt; Fold09\r## 10 \u0026lt;split [30517/3390]\u0026gt; Fold10\r\rFit resamples\rFit the folds into the workflow.\n\rCollect CV metrics\rShow the metrics resulted from Cross-Validation process. The accurcay achieved after using the 10 folds cross validation for resampling is 90.1% .\n## # A tibble: 2 x 6\r## .metric .estimator mean n std_err .config ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 accuracy binary 0.901 10 0.00143 Preprocessor1_Model1\r## 2 roc_auc binary 0.747 10 0.00361 Preprocessor1_Model1\r\r\rHyperparameter tuning\rIn this part we will tune the hyperparameters of the decision tree and check the tuning effect on the model accuracy.\nSet the hyperparameters for tuning in the decision tree\rWe will modify the model so that the hyperparameters will be undergo tuuining and accordingly the workflow would be updated with the new model.\ntune_model \u0026lt;- decision_tree(cost_complexity = tune(),\rtree_depth = tune(),\rmin_n = tune()) %\u0026gt;%\rset_engine(\u0026#39;rpart\u0026#39;) %\u0026gt;%\rset_mode(\u0026#39;classification\u0026#39;)\rtune_wkfl \u0026lt;- wkfl %\u0026gt;%\rupdate_model(tune_model)\r\rCreate a tuning grid\rCreate the tuning grid with 3 levels for each parameter.\nset.seed(216)\rgrid\u0026lt;-grid_regular(parameters(tune_model),levels = 3)\r\rHyperparameter tuning with cross validation\rFrom the shown graph we can see the accuracy Vs. the different combination of the 3 hyperparameters levels.\ntune_results \u0026lt;- tune_wkfl %\u0026gt;%\rtune_grid(resamples = folds,\rgrid = grid,\rmetrics = metric_set(accuracy))\rautoplot(tune_results)\rtune_results %\u0026gt;%\rcollect_metrics()\r## # A tibble: 27 x 9\r## cost_complexity tree_depth min_n .metric .estimator mean n std_err\r## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0.0000000001 1 2 accuracy binary 0.883 10 0.0000417\r## 2 0.00000316 1 2 accuracy binary 0.883 10 0.0000417\r## 3 0.1 1 2 accuracy binary 0.883 10 0.0000417\r## 4 0.0000000001 8 2 accuracy binary 0.899 10 0.000797 ## 5 0.00000316 8 2 accuracy binary 0.899 10 0.000797 ## 6 0.1 8 2 accuracy binary 0.883 10 0.0000417\r## 7 0.0000000001 15 2 accuracy binary 0.891 10 0.00151 ## 8 0.00000316 15 2 accuracy binary 0.891 10 0.00151 ## 9 0.1 15 2 accuracy binary 0.883 10 0.0000417\r## 10 0.0000000001 1 21 accuracy binary 0.883 10 0.0000417\r## # ... with 17 more rows, and 1 more variable: .config \u0026lt;chr\u0026gt;\r\rSelect the best parameters\rFrom the above graph we can see that the best hyperparameters combination is minimum node size of 40, tree depth of 8 and the cost complexity of 0.0000000001, where this combination result in an accuracy of 92.5%\nfinal_params \u0026lt;- select_best(tune_results)\rfinal_params\r## # A tibble: 1 x 4\r## cost_complexity tree_depth min_n .config ## \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; ## 1 0.0000000001 8 40 Preprocessor1_Model22\r\rFinalize the model\rFinalize the model with the best value for each hyperparameter.\nbest_spec \u0026lt;- finalize_model(tune_model,\rfinal_params)\rbest_spec\r## Decision Tree Model Specification (classification)\r## ## Main Arguments:\r## cost_complexity = 1e-10\r## tree_depth = 8\r## min_n = 40\r## ## Computational engine: rpart\r\r\rPassing the testing data to the tuned model\rMake prediction with the tuned model on the testing data.\ntuned_decision_tree_mdl \u0026lt;- decision_tree(cost_complexity = 1e-10,tree_depth = 8,min_n = 2) %\u0026gt;%\r# Specify the engine\rset_engine(\u0026#39;rpart\u0026#39;) %\u0026gt;%\r# Specify the mode\rset_mode(\u0026#39;classification\u0026#39;)\rtuned_wkflw \u0026lt;- workflow() %\u0026gt;%\r# Add model\radd_model(tuned_decision_tree_mdl) %\u0026gt;%\r# Add recipe\radd_recipe(the_recipe)\rfit_tuned_mdl \u0026lt;- tuned_wkflw %\u0026gt;%\rfit(data=test_data)\rtuned_prediction\u0026lt;-predict(fit_tuned_mdl, test_data,type = \u0026#39;class\u0026#39;) %\u0026gt;%\rbind_cols(test_data %\u0026gt;% select(everything()))\rtuned_prediction\r## # A tibble: 11,304 x 18\r## .pred_class age job marital education default balance housing loan ## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt;\r## 1 no 33 entreprene~ married secondary no 2 yes yes ## 2 no 33 unknown single unknown no 1 no no ## 3 no 41 admin. divorc~ secondary no 270 yes no ## 4 no 57 services married secondary no 162 yes no ## 5 no 45 admin. single unknown no 13 yes no ## 6 no 56 management married tertiary no 779 yes no ## 7 no 40 retired married primary no 0 yes yes ## 8 no 46 management single secondary no -246 yes no ## 9 no 36 technician single secondary no 265 yes yes ## 10 no 57 technician married secondary no 839 no yes ## # ... with 11,294 more rows, and 9 more variables: contact \u0026lt;fct\u0026gt;, day \u0026lt;chr\u0026gt;,\r## # month \u0026lt;fct\u0026gt;, duration \u0026lt;int\u0026gt;, campaign \u0026lt;int\u0026gt;, pdays \u0026lt;int\u0026gt;, previous \u0026lt;int\u0026gt;,\r## # poutcome \u0026lt;fct\u0026gt;, y \u0026lt;fct\u0026gt;\rEvaluate the performance of the tuned model\rEvaluate the performance of the tuned model on the testing data.\nconf_mat(tuned_prediction, truth = y, estimate = .pred_class)\r## Truth\r## Prediction no yes\r## no 9821 688\r## yes 160 635\rtest_metric_tuned\u0026lt;- custom_metrics(tuned_prediction,\rtruth = y,\restimate = .pred_class)\rtest_metric_tuned\r## # A tibble: 3 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.925\r## 2 sens binary 0.984\r## 3 spec binary 0.480\r\rConclusion\rThe accuracy associated with the untuned model on the training data is 90.2%, and that associated with the untuned model on the testing data is 90% and that associated with the same model using cross validation resampling method is 90.1%. The accuracy in the testing data is quite good and it is slightly less than the accuracy in the training data which indicates that our model is not exposed to overfitting on the training data, also the accuracy resulted from the testing data is comparable with the accuracy we got from cross-validation resampling method that indicates that our model is working properly. After tuning the model with the best combination of the 3 hyperparameters that are min_n=40 , tree_depth=8 and the cost_complexity=0.0000000001 we got an accuracy of 92.5% that is considered the highest accuracy. This shows us the importance and the effect of tuning the decision tree model hyperparameters on the accuracy of the model.\n\r\r","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627666065,"objectID":"840ca933fbe1c4d2ffc384dec8134b08","permalink":"/post/nouran/","publishdate":"2021-07-30T00:00:00Z","relpermalink":"/post/nouran/","section":"post","summary":"The Goal\rThe goal behind this study is to use the decision tree to build a tree-based model for prediction, so that we will use the provided data set to identify whether a customer will subscribe to a term deposit or not.","tags":[],"title":"Tree Based Method A04","type":"post"},{"authors":[],"categories":["ggplot2","tidymodels","Tideverse","caret","yardstick","dplyr"],"content":"\r\rThe Goal\rThe goal behind this study is to discuss two of the most commonly used resampling methods, cross-validation and the bootstrap and we would apply both resampling methods on linear and logistic regression models.\n\rFirst For Logistic Regression Model:\rThe data\rThe data used in this study was obtained from UCI website and we specifically used the Bank-full.csv dataset.\n\rSplitting the data to training and testing splits\rIn this step we are going to split the data so that 3/4 of the data would be assigned for training and the remaining 1/4 of the data would be assigned for testing.\n\rCreate the logistic regression model\rCreating the logistic regression model that will be used to identify whether a customer will subscribe to a term deposit or not.\n\rCreating the recipe\rCreating the suitable recipe to apply feature engineering to our data\n\rCreate workflow\rCreate a workflow that combine the model and the recipe.\n\rFit the model\rTrain and fitting the model using the training split of the data.\n## # A tibble: 35 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) -1.65 0.126 -13.1 3.88e-39\r## 2 duration 1.06 0.0189 56.3 0 ## 3 campaign -0.272 0.0358 -7.60 2.94e-14\r## 4 job_blue.collar -0.368 0.0833 -4.41 1.01e- 5\r## 5 job_entrepreneur -0.367 0.143 -2.57 1.01e- 2\r## 6 job_housemaid -0.441 0.150 -2.95 3.20e- 3\r## 7 job_management -0.243 0.0838 -2.90 3.72e- 3\r## 8 job_retired 0.166 0.0997 1.67 9.57e- 2\r## 9 job_self.employed -0.330 0.128 -2.57 1.03e- 2\r## 10 job_services -0.242 0.0971 -2.49 1.27e- 2\r## 11 job_student 0.521 0.119 4.37 1.23e- 5\r## 12 job_technician -0.183 0.0794 -2.31 2.11e- 2\r## # ... with 23 more rows\r\rMake predictions for training data\rUsing the trained model for prediction on the training data.\n## # A tibble: 33,907 x 11\r## .pred_class y job education housing loan contact month duration\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 no no management tertiary yes no unknown may 261\r## 2 no no entrepreneur secondary yes yes unknown may 76\r## 3 no no blue-collar unknown yes no unknown may 92\r## 4 no no unknown unknown no no unknown may 198\r## 5 no no management tertiary yes no unknown may 139\r## 6 no no technician secondary yes no unknown may 55\r## 7 no no admin. secondary yes no unknown may 137\r## 8 no no technician secondary yes no unknown may 517\r## 9 no no technician unknown yes no unknown may 71\r## 10 no no services secondary yes no unknown may 174\r## # ... with 33,897 more rows, and 2 more variables: campaign \u0026lt;int\u0026gt;,\r## # poutcome \u0026lt;fct\u0026gt;\r\rEvaluate performance on tarining data\rEvaluate the performance of the model on the training data.\n## Truth\r## Prediction no yes\r## no 29193 2595\r## yes 748 1371\r## # A tibble: 3 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.901\r## 2 sens binary 0.975\r## 3 spec binary 0.346\r\rMake predictions for testing data\rUsing the trained model for prediction on the testing data.\n## # A tibble: 11,304 x 11\r## .pred_class y job education housing loan contact month duration\r## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 no no technician secondary yes no unknown may 151\r## 2 no no management tertiary yes yes unknown may 217\r## 3 no no entrepreneur tertiary yes no unknown may 380\r## 4 no no retired primary yes no unknown may 50\r## 5 no no admin. secondary yes no unknown may 222\r## 6 no no retired primary yes no unknown may 353\r## 7 no no admin. unknown yes no unknown may 98\r## 8 no no blue-collar primary yes no unknown may 38\r## 9 no no retired primary yes no unknown may 219\r## 10 no no technician secondary yes yes unknown may 348\r## # ... with 11,294 more rows, and 2 more variables: campaign \u0026lt;int\u0026gt;,\r## # poutcome \u0026lt;fct\u0026gt;\r\rEvaluate performance on testing data\rEvaluate the performance of the model on the testing data.\n## Truth\r## Prediction no yes\r## no 9754 875\r## yes 227 448\r## # A tibble: 3 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.903\r## 2 sens binary 0.977\r## 3 spec binary 0.339\r\r\rApply Cross Validation method for model evaluation\rWe will use the cross-validation resampling method to evaluate the performance of our logestic regression model.\nSplit data into folds\rThe train data is split-ted into 5 folds\n\rFit resamples\rFit the folds into the workflow.\n\rCollect CV metrics\rShow the metrics resulted from Cross-Validation process\n## # A tibble: 2 x 6\r## .metric .estimator mean n std_err .config ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 accuracy binary 0.902 5 0.00165 Preprocessor1_Model1\r## 2 roc_auc binary 0.904 5 0.00174 Preprocessor1_Model1\r\rConclusion\rThe accuracy associated with the model using the training data is 90.1%, that associated with the model using the testing data is 90.3% and that associated with the model using cross validation resampling method is 90.1%. The accuracy in the testing data is quite good and it is slightly better than the accuracy in the training data which indicates that our model is not exposed to overfitting on the training data, also the accuracy resulted from the testing data is comparable with the accuracy we got from cross-validation resampling method that indicates that our model is working properly.\n\r\rApplying Bootstrap resampling method\rIn this part we are estimating the accuracy of a Logestic Regression Model through the bootstrap approach which can be used to assess the variability of the coefficient estimates (Betas) and predictions from a statistical learning method.\nThe standard error reflects the variability between the estimates we would obtain if we repeatedly took samples from the population. The standard error associated with the coefficients of both the model and that obtained through bootstrapping are shown below:\n## (Intercept) jobblue-collar jobentrepreneur jobhousemaid ## -2.416485382 -0.346072166 -0.413788618 -0.538843667 ## jobmanagement jobretired jobself-employed jobservices ## -0.189089703 0.196164320 -0.311713723 -0.236414425 ## jobstudent jobtechnician jobunemployed jobunknown ## 0.511192792 -0.173019378 -0.177429381 -0.361506746 ## educationsecondary educationtertiary educationunknown housingyes ## 0.203955264 0.436149134 0.269835962 -0.700007468 ## loanyes contacttelephone contactunknown monthaug ## -0.444875217 -0.175980434 -1.605880094 -0.763099074 ## monthdec monthfeb monthjan monthjul ## 0.646688881 -0.251511233 -1.163911847 -0.843156031 ## monthjun monthmar monthmay monthnov ## 0.366545869 1.553360619 -0.425837991 -0.874011401 ## monthoct monthsep duration campaign ## 0.888103673 0.811484702 0.004190172 -0.086589973 ## poutcomeother poutcomesuccess poutcomeunknown ## 0.233821843 2.303692088 -0.078536962\r## Estimate Std. Error z value Pr(\u0026gt;|z|)\r## (Intercept) -2.416485382 1.113856e-01 -21.694773 2.298578e-104\r## jobblue-collar -0.346072166 7.226905e-02 -4.788664 1.678955e-06\r## jobentrepreneur -0.413788618 1.249918e-01 -3.310525 9.312098e-04\r## jobhousemaid -0.538843667 1.354648e-01 -3.977740 6.957349e-05\r## jobmanagement -0.189089703 7.293692e-02 -2.592510 9.527837e-03\r## jobretired 0.196164320 8.618864e-02 2.275988 2.284672e-02\r## jobself-employed -0.311713723 1.114374e-01 -2.797210 5.154603e-03\r## jobservices -0.236414425 8.390921e-02 -2.817503 4.839869e-03\r## jobstudent 0.511192792 1.044029e-01 4.896346 9.763496e-07\r## jobtechnician -0.173019378 6.880062e-02 -2.514794 1.191020e-02\r## jobunemployed -0.177429381 1.114466e-01 -1.592058 1.113717e-01\r## jobunknown -0.361506746 2.329359e-01 -1.551958 1.206723e-01\r## educationsecondary 0.203955264 6.411055e-02 3.181306 1.466128e-03\r## educationtertiary 0.436149134 7.395298e-02 5.897654 3.687051e-09\r## educationunknown 0.269835962 1.035801e-01 2.605095 9.184878e-03\r## housingyes -0.700007468 4.329460e-02 -16.168472 8.415943e-59\r## loanyes -0.444875217 5.967071e-02 -7.455504 8.952482e-14\r## contacttelephone -0.175980434 7.410680e-02 -2.374687 1.756384e-02\r## contactunknown -1.605880094 7.237608e-02 -22.187993 4.485657e-109\r## monthaug -0.763099074 7.747181e-02 -9.850022 6.852875e-23\r## monthdec 0.646688881 1.757141e-01 3.680347 2.329164e-04\r## monthfeb -0.251511233 8.412492e-02 -2.989735 2.792195e-03\r## monthjan -1.163911847 1.196732e-01 -9.725755 2.341610e-22\r## monthjul -0.843156031 7.706815e-02 -10.940395 7.387702e-28\r## monthjun 0.366545869 9.019358e-02 4.063991 4.824078e-05\r## monthmar 1.553360619 1.187454e-01 13.081435 4.204289e-39\r## monthmay -0.425837991 7.108191e-02 -5.990807 2.088027e-09\r## monthnov -0.874011401 8.371180e-02 -10.440719 1.615814e-25\r## monthoct 0.888103673 1.076217e-01 8.252090 1.556480e-16\r## monthsep 0.811484702 1.180207e-01 6.875780 6.165138e-12\r## duration 0.004190172 6.442787e-05 65.036632 0.000000e+00\r## campaign -0.086589973 1.003337e-02 -8.630197 6.124664e-18\r## poutcomeother 0.233821843 8.916180e-02 2.622444 8.730153e-03\r## poutcomesuccess 2.303692088 7.959003e-02 28.944480 3.293120e-184\r## poutcomeunknown -0.078536962 5.731654e-02 -1.370232 1.706144e-01\r## ## ORDINARY NONPARAMETRIC BOOTSTRAP\r## ## ## Call:\r## boot(data = bank_data, statistic = boot_fn_bank, R = 100)\r## ## ## Bootstrap Statistics :\r## original bias std. error\r## t1* -2.416485382 -8.028300e-03 0.1255282876\r## t2* -0.346072166 6.884443e-03 0.0656667243\r## t3* -0.413788618 -1.746870e-02 0.1250063163\r## t4* -0.538843667 -2.632170e-02 0.1506685985\r## t5* -0.189089703 -1.946738e-03 0.0703688067\r## t6* 0.196164320 9.726636e-03 0.1007565978\r## t7* -0.311713723 6.681341e-03 0.1118123533\r## t8* -0.236414425 4.509866e-03 0.0777245698\r## t9* 0.511192792 1.878887e-02 0.0999970178\r## t10* -0.173019378 -2.964749e-03 0.0639570480\r## t11* -0.177429381 -1.918871e-02 0.1279365248\r## t12* -0.361506746 2.929020e-02 0.2202905524\r## t13* 0.203955264 3.677783e-03 0.0710370345\r## t14* 0.436149134 2.678866e-03 0.0783973333\r## t15* 0.269835962 -7.098522e-03 0.1134082599\r## t16* -0.700007468 2.614857e-03 0.0437524671\r## t17* -0.444875217 2.541731e-03 0.0564787713\r## t18* -0.175980434 -1.769945e-02 0.0788122518\r## t19* -1.605880094 2.710617e-03 0.0776266569\r## t20* -0.763099074 2.196291e-04 0.0861235310\r## t21* 0.646688881 -1.281272e-03 0.2189127383\r## t22* -0.251511233 -3.810631e-03 0.0928764444\r## t23* -1.163911847 -6.144847e-03 0.1196075241\r## t24* -0.843156031 -4.977336e-03 0.0803586902\r## t25* 0.366545869 -1.096552e-02 0.0949528655\r## t26* 1.553360619 4.760479e-03 0.1289172848\r## t27* -0.425837991 -2.116673e-03 0.0731852404\r## t28* -0.874011401 -5.277234e-03 0.0887677215\r## t29* 0.888103673 1.226003e-02 0.1220745778\r## t30* 0.811484702 -3.511322e-02 0.1457014049\r## t31* 0.004190172 -5.900232e-07 0.0001029556\r## t32* -0.086589973 -2.098398e-03 0.0115775812\r## t33* 0.233821843 1.028080e-02 0.0957398188\r## t34* 2.303692088 1.141992e-02 0.0869714862\r## t35* -0.078536962 3.761925e-03 0.0625711164\r\rSecond For Linear Regression Model:\rThe diamond ring data\rIn this part we used the diamond ring data.\n## Rows: 440\r## Columns: 9\r## $ price \u0026lt;dbl\u0026gt; 3000, 3000, 3004, 3004, 3006, 3007, 3008, 3010, 3012, 30~\r## $ carat \u0026lt;dbl\u0026gt; 0.92, 0.92, 0.82, 0.81, 0.90, 0.87, 0.80, 0.84, 0.80, 0.~\r## $ colour \u0026lt;chr\u0026gt; \u0026quot;I\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;J\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;~\r## $ clarity \u0026lt;fct\u0026gt; SI2, SI2, SI2, SI1, VS2, SI2, SI2, SI1, SI2, SI2, SI2, S~\r## $ cut \u0026lt;chr\u0026gt; \u0026quot;G\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;~\r## $ certification \u0026lt;chr\u0026gt; \u0026quot;AGS\u0026quot;, \u0026quot;AGS\u0026quot;, \u0026quot;GIA\u0026quot;, \u0026quot;GIA\u0026quot;, \u0026quot;GIA\u0026quot;, \u0026quot;AGS\u0026quot;, \u0026quot;GIA\u0026quot;, \u0026quot;GIA\u0026quot;, ~\r## $ polish \u0026lt;chr\u0026gt; \u0026quot;V\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;~\r## $ symmetry \u0026lt;chr\u0026gt; \u0026quot;V\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;~\r## $ wholesaler \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;~\r\rsplitting the data to training and testing splits\rIn this step we are going to split the data on hands so that 3/4 of the data would be assigned for training and 1/4 of the data would be assigned for testing.\n\rcreate the linear regression model\rCreating the linear regression model that will be used to predict the price of the diamond ring of interest.\n\rcreating the recipe\rcreating the suitable recipe to apply feature engineering to our data\n\rCreate workflow\rCreate a workflow that combine the model and the recipe.\n\rFit the model\rTrain and fitting the model using the training split of the data.\n## # A tibble: 36 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 2141. 128. 16.7 2.29e-44\r## 2 carat 709. 58.5 12.1 1.14e-27\r## 3 colour_E -215. 45.1 -4.75 3.13e- 6\r## 4 colour_F -280. 45.2 -6.20 1.94e- 9\r## 5 colour_G -276. 46.7 -5.91 9.47e- 9\r## 6 colour_H -343. 46.0 -7.46 9.79e-13\r## 7 colour_I -380. 46.4 -8.19 8.25e-15\r## 8 colour_J -435. 49.6 -8.76 1.65e-16\r## 9 colour_K -666. 57.1 -11.7 4.62e-26\r## 10 colour_L -878. 68.3 -12.8 2.92e-30\r## # ... with 26 more rows\r\rMake predictions for training data\rUsing the trained model for prediction on the training data.\n## # A tibble: 328 x 10\r## .pred price carat colour clarity cut certification polish symmetry\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 350. 190 0.09 D SI1 X IGI G G ## 2 134. 160 0.09 E SI1 I IGI G G ## 3 85.6 160 0.09 F SI1 I IGI V G ## 4 134. 180 0.09 E SI1 I IGI G G ## 5 227. 190 0.09 E VS1 I IGI V G ## 6 196. 330 0.1 E VS2 V IGI V G ## 7 192. 180 0.1 E VS2 X IGI G G ## 8 88.9 160 0.1 F SI1 X IGI G G ## 9 142. 160 0.1 E SI1 V IGI G G ## 10 60.7 190 0.1 E SI2 G IGI V G ## # ... with 318 more rows, and 1 more variable: wholesaler \u0026lt;chr\u0026gt;\r\rEvaluate performance on tarining data\rEvaluate the performance of the model on the training data.\nR-squared\n## # A tibble: 1 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 rsq standard 0.986\rRMSE\n## # A tibble: 1 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 rmse standard 136.\r\rMake predictions for testing data\rUsing the trained model for prediction on the testing data.\n## # A tibble: 112 x 10\r## .pred price carat colour clarity cut certification polish symmetry\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 3035. 3000 0.92 I SI2 G AGS V V ## 2 3028. 3000 0.92 I SI2 V AGS G G ## 3 3088. 3006 0.9 J VS2 V GIA V V ## 4 3027. 3027 0.81 F SI1 V AGS V V ## 5 2957. 3036 0.81 H SI1 V GIA G V ## 6 3235. 3041 0.83 D SI2 G GIA G G ## 7 3052. 3044 0.91 I SI2 I GIA V V ## 8 3000. 3062 0.8 E SI2 V GIA V V ## 9 NA 3081 0.9 F SI2 F GIA v G ## 10 2956. 3089 0.91 H SI2 F GIA V V ## # ... with 102 more rows, and 1 more variable: wholesaler \u0026lt;chr\u0026gt;\r\rEvaluate performance on testing data\rEvaluate the performance of the model on the testing data.\nR-squared\n## # A tibble: 1 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 rsq standard 0.983\rRMSE\n## # A tibble: 1 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 rmse standard 157.\r\r\rApply Cross Validation method for model evaluation\rWe will use the cross-validation resampling method to evaluate the performance of our linear regression model.\nSplit data into folds\rThe train data is split-ted into 5 folds\n\rFit resamples\rFit the folds into the workflow.\n\rCollect CV metrics\rShow the metrics resulted from Cross-Validation process\n## # A tibble: 2 x 6\r## .metric .estimator mean n std_err .config ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 rmse standard 153. 5 5.07 Preprocessor1_Model1\r## 2 rsq standard 0.983 5 0.00123 Preprocessor1_Model1\r\rConclusion\rThe rsq associated with the model on the training data is 0.99, that associated with the model in the testing data is 0.98 and that associated with the model using cross validation is 0.98. From that we can see that our model is performing good.\n\r\rApplying Bootstrap resampling method\rIn this part we are estimating the accuracy of a Linear Regression Model through the bootstrap approach which can be used to assess the variability of the coefficient estimates (Betas) and predictions from a statistical learning method.\nThe standard error reflects the variability between the estimates we would obtain if we repeatedly took samples from the population. The standard error associated with the coefficients of both the model and that obtained through bootstrapping are shown below:\n# boot_fn_rings= function (data ,index){\r# return(tidy( wkfl_rings %\u0026gt;%fit(data=data[index,]))) }\rboot_fn_rings= function (data ,index ){\rreturn (coef (lm(price ~ .,data =data , subset =index )))}\rdf2 \u0026lt;- model.matrix( ~ price + carat + colour + clarity+cut + certification + polish + symmetry + wholesaler-1, data = rings_data)\rset.seed(103)\rsummary(lm(price ~ .,data =as.data.frame(df2)))$coef\r## Estimate Std. Error t value Pr(\u0026gt;|t|)\r## (Intercept) -106.56155 175.79890 -0.6061560 5.447516e-01\r## carat 1885.30336 133.35039 14.1379667 3.742257e-37\r## colourD 895.44460 59.30734 15.0983785 3.695004e-41\r## colourE 728.83798 52.92247 13.7718071 1.196791e-35\r## colourF 648.29530 52.36663 12.3799311 4.580770e-30\r## colourG 665.84244 51.58213 12.9083948 3.708078e-32\r## colourH 595.60521 50.56202 11.7796946 9.764476e-28\r## colourI 557.81920 49.66629 11.2313443 1.170879e-25\r## colourJ 461.05137 49.26745 9.3581342 5.704793e-19\r## colourK 199.01441 50.96007 3.9053008 1.102762e-04\r## clarityI2 -590.10469 39.59374 -14.9039905 2.424038e-40\r## claritySI1 652.27743 37.30949 17.4828817 2.280084e-51\r## claritySI2 560.50704 30.84165 18.1737061 2.265758e-54\r## claritySI3 290.45599 37.17977 7.8122046 4.902422e-14\r## clarityVS1 743.08136 49.56030 14.9934784 1.020529e-40\r## clarityVS2 689.49914 43.96568 15.6826658 1.246192e-43\r## clarityVVS1 1015.21402 117.33428 8.6523228 1.208576e-16\r## clarityVVS2 760.49537 77.07807 9.8665595 1.025130e-20\r## cutG 48.81986 30.73652 1.5883339 1.129931e-01\r## cutI 84.50708 30.00680 2.8162641 5.096725e-03\r## cutV 78.44805 30.50328 2.5717904 1.047401e-02\r## cutX 93.37420 26.51667 3.5213390 4.783374e-04\r## certificationDOW -271.24204 164.60408 -1.6478452 1.001619e-01\r## certificationEGL -307.28583 67.95169 -4.5221220 8.057030e-06\r## certificationGIA 12.15763 60.44293 0.2011423 8.406885e-01\r## certificationIGI -118.26743 72.53254 -1.6305431 1.037661e-01\r## polishG 66.57386 78.84945 0.8443161 3.989925e-01\r## polishI 247.69579 115.61368 2.1424436 3.275488e-02\r## polishv 136.45426 169.48598 0.8051065 4.212318e-01\r## polishV 78.39863 81.67731 0.9598581 3.377007e-01\r## polishX 84.85158 83.94039 1.0108553 3.126907e-01\r## symmetryG 133.37429 42.42139 3.1440340 1.789425e-03\r## symmetryV 151.49072 45.27573 3.3459583 8.967355e-04\r## symmetryX 137.39985 50.44279 2.7238747 6.732040e-03\r## wholesaler2 112.07125 49.94276 2.2439938 2.537367e-02\r## wholesaler3 -1460.66895 78.09862 -18.7028787 1.115682e-56\rboot(statistic = boot_fn_rings,\rdata = as.data.frame(df2), R = 1000)\r## ## ORDINARY NONPARAMETRIC BOOTSTRAP\r## ## ## Call:\r## boot(data = as.data.frame(df2), statistic = boot_fn_rings, R = 1000)\r## ## ## Bootstrap Statistics :\r## original bias std. error\r## t1* -106.56155 -9.35456333 261.67043\r## t2* 1885.30336 16.89859427 177.59923\r## t3* 895.44460 2.78838482 99.78342\r## t4* 728.83798 1.96797173 88.88759\r## t5* 648.29530 0.44403917 88.81436\r## t6* 665.84244 -0.87732839 86.79063\r## t7* 595.60521 1.51940839 86.22062\r## t8* 557.81920 1.08977842 85.60847\r## t9* 461.05137 -1.31429699 83.33705\r## t10* 199.01441 -5.71033660 81.01659\r## t12* -590.10469 -5.62400852 57.95193\r## t13* 652.27743 4.13415624 53.01940\r## t14* 560.50704 3.28170559 46.95164\r## t15* 290.45599 5.96937782 58.04980\r## t16* 743.08136 5.88777718 62.79755\r## t17* 689.49914 3.99395348 58.81016\r## t18* 1015.21402 10.65485966 80.97264\r## t19* 760.49537 4.40900611 103.99886\r## t20* 48.81986 -3.96590696 42.73852\r## t21* 84.50708 -2.33151751 37.91071\r## t22* 78.44805 -2.30491726 32.90870\r## t23* 93.37420 -1.88620077 34.89901\r## t24* -271.24204 -1.08968639 78.50134\r## t25* -307.28583 -2.51905597 53.47293\r## t26* 12.15763 0.16925914 30.04501\r## t27* -118.26743 2.50044800 49.52972\r## t28* 66.57386 -7.36747324 136.36556\r## t29* 247.69579 -8.64360605 133.21323\r## t30* 136.45426 -7.91079733 139.81229\r## t31* 78.39863 -7.36281044 140.63048\r## t32* 84.85158 -6.72228927 141.11272\r## t33* 133.37429 0.59236148 62.87303\r## t35* 151.49072 -0.05086443 65.82882\r## t36* 137.39985 1.01165019 67.30974\r## t37* 112.07125 0.30607925 62.57052\r## t38* -1460.66895 8.51131528 99.28839\r## WARNING: All values of t11* are NA\r## WARNING: All values of t34* are NA\r.\n\r","date":1626912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626957113,"objectID":"9f0cc6489905ed71be1e3a1268234ba3","permalink":"/post/a03-resampling-methods/","publishdate":"2021-07-22T00:00:00Z","relpermalink":"/post/a03-resampling-methods/","section":"post","summary":"The Goal\rThe goal behind this study is to discuss two of the most commonly used resampling methods, cross-validation and the bootstrap and we would apply both resampling methods on linear and logistic regression models.","tags":[],"title":"A03 Resampling Methods","type":"post"},{"authors":[],"categories":["Basics","ggplot2"],"content":"\r\rThe Goal\rThe goal behind this study is to use the provided data set to identify whether a customer will subscribe to a term deposit or not.\n\rThe Bank data\rThe data used in this study was obtained from UCI  website and we specifically used the Bank-full.csv dataset, to access the source of the data use the link below\nLink to the dataset.\nGetting a glimpse into the data we can see that it is composed of 45211 observations and 17 variables.\n## Rows: 45,211\r## Columns: 17\r## $ age \u0026lt;int\u0026gt; 58, 44, 33, 47, 33, 35, 28, 42, 58, 43, 41, 29, 53, 58, 57, ~\r## $ job \u0026lt;fct\u0026gt; management, technician, entrepreneur, blue-collar, unknown, ~\r## $ marital \u0026lt;fct\u0026gt; married, single, married, married, single, married, single, ~\r## $ education \u0026lt;fct\u0026gt; tertiary, secondary, secondary, unknown, unknown, tertiary, ~\r## $ default \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, yes, no, no, no, no, no, no, no,~\r## $ balance \u0026lt;int\u0026gt; 2143, 29, 2, 1506, 1, 231, 447, 2, 121, 593, 270, 390, 6, 71~\r## $ housing \u0026lt;fct\u0026gt; yes, yes, yes, yes, no, yes, yes, yes, yes, yes, yes, yes, y~\r## $ loan \u0026lt;fct\u0026gt; no, no, yes, no, no, no, yes, no, no, no, no, no, no, no, no~\r## $ contact \u0026lt;fct\u0026gt; unknown, unknown, unknown, unknown, unknown, unknown, unknow~\r## $ day \u0026lt;chr\u0026gt; \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;5\u0026quot;, ~\r## $ month \u0026lt;fct\u0026gt; may, may, may, may, may, may, may, may, may, may, may, may, ~\r## $ duration \u0026lt;int\u0026gt; 261, 151, 76, 92, 198, 139, 217, 380, 50, 55, 222, 137, 517,~\r## $ campaign \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\r## $ pdays \u0026lt;int\u0026gt; -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, ~\r## $ previous \u0026lt;int\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r## $ poutcome \u0026lt;fct\u0026gt; unknown, unknown, unknown, unknown, unknown, unknown, unknow~\r## $ y \u0026lt;fct\u0026gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, ~\rThe Variables description:\r\rbank client data:\n1 - age (numeric)\r2 - job : type of job (categorical: “admin.”,“unknown”,“unemployed”,“management”,“housemaid”,“entrepreneur”,“student”,\r“blue-collar”,“self-employed”,“retired”,“technician”,“services”)\r3 - marital : marital status (categorical: “married”,“divorced”,“single”; note: “divorced” means divorced or widowed)\r4 - education (categorical: “unknown”,“secondary”,“primary”,“tertiary”)\r5 - default: has credit in default? (binary: “yes”,“no”)\r6 - balance: average yearly balance, in euros (numeric)\r7 - housing: has housing loan? (binary: “yes”,“no”)\r8 - loan: has personal loan? (binary: “yes”,“no”)\r*related with the last contact of the current campaign:\r9 - contact: contact communication type (categorical: “unknown”,“telephone”,“cellular”)\r10 - day: last contact day of the month (numeric)\r11 - month: last contact month of year (categorical: “jan”, “feb”, “mar”, …, “nov”, “dec”)\r12 - duration: last contact duration, in seconds (numeric)\n\r\r*other attributes:\r13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\r14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\r15 - previous: number of contacts performed before this campaign and for this client (numeric)\r16 - poutcome: outcome of the previous marketing campaign (categorical: “unknown”,“other”,“failure”,“success”)\nOutput variable (desired target):\r17 - y - has the client subscribed a term deposit? (binary: “yes”,“no”)\n\r\rPartioning the data set to train and test parts\r## 75% of the sample size\rsmp_size \u0026lt;- floor(0.75 * nrow(whole_data))\r## set the seed to make your partition reproducible\rset.seed(123)\rtrain_ind \u0026lt;- sample(seq_len(nrow(whole_data)), size = smp_size)\rdata \u0026lt;- whole_data[train_ind, ]\rtest \u0026lt;- whole_data[-train_ind, ]\r\rThe Univriate Analysis\rwe will start our analysis with the metric data which are the age, balance, duration, campaign, pdays, and previous then we will analysis the non metric variables which are all the remaining variables.\nFor the metric variables the shown plots, provided summary and the simple calculations, we can get valuable information about the each metric variable distribution, like the range, min, max, mean , media, mode, 1st and 3rd quartile, IQR, variance and standard deviation.\nFor the non metric data the histogram would be used.\nThe Age\rThe data distribution of the age is right skewed and the peak occurs at age 32.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 18.00 33.00 39.00 40.91 48.00 95.00\r## [1] 112.7199\r## [1] 10.61696\r## age n\r## 1 32 1554\r## 75% ## 15\r\rThe Balance\rThe data distribution for the balance variable ranges from -8019 to 102127, so it is better to use log x scale in this case.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -6847 71 448 1353 1427 102127\r## [1] 8945813\r## [1] 2990.955\r## balance n\r## 1 0 2641\r## 75% ## 1356\r\rThe duration\rThe data distribution of the duration variable has a wide range as it ranges from 0 to 4918, so it is better to use log x scale in this case also.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 103.0 180.0 258.1 318.0 3785.0\r## [1] 65821.23\r## [1] 256.5565\r## duration n\r## 1 104 142\r## 75% ## 215\r\rThe Campaign\rThe data distribution of the campaign variable has ranges from 1 contact to 63 contacts where the mode is 1 contact that results in the distribution is right skewed.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.000 2.000 2.777 3.000 58.000\r## [1] 9.867114\r## [1] 3.141196\r## campaign n\r## 1 1 13125\r## 75% ## 2\r\rThe Pdays\rThe data distribution of the pdays variable ranges from -1 that means the client never contacted before to 871 days.\rthe distribution is left skewed with mode of -1 day.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.00 -1.00 -1.00 40.57 -1.00 871.00\r## [1] 10155.46\r## [1] 100.7743\r## pdays n\r## 1 -1 27657\r## 75% ## 0\r\rThe Previous\rThe data distribution of the previous variable ranges from 0 to 275 contacts,the distribution is right skewed with the most dominant number of contact of 1.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0000 0.0000 0.5882 0.0000 275.0000\r## [1] 5.91227\r## [1] 2.431516\r## previous n\r## 1 0 27657\r## 75% ## 0\r\rThe Job\rFrom the histogram plot of the job variable, we can see that the most dominant job titles are Blue collar and Management then Technician.\n\rThe Marital\rFrom the histogram plot of the Marital variable, we can see that the most dominant marital status is married.\r\rThe Education\rFrom the histogram plot of the education variable, we can see that the most dominant education is the secondary education.\n\rThe default\rFrom the histogram plot of the default variable, we can see that the most of the clients has no credit in default.\r\rThe Housing\rFrom the histogram plot of the Housing variable, we can see that clients with housing loan are greater than clients with no housing loan.\n\rThe Loan\rFrom the histogram plot of the loan variable, we can see that most of the clients don’t have loan.\n\rThe Contact\rFrom the histogram plot of the contact variable, we can see that most communication type was cellular.\n\rThe Month\rFrom the histogram plot of the month variable, we can see that the most dominant month is May.\n\rThe Poutcome\rFrom the histogram plot of the poutcome variable, we can see that the most dominant outcome is “unknown”.\n\rThe Y\rFrom the histogram plot of the Y variable, we can see that the most of the client haven’t subscribed to term deposit.\n\r\rThe Bivariate Analysis\rIn the Bivaraite analysis we will see the relationship between the variable y and all other variables\nThe relationship between variable Y and the Age variable\r### The relationship between variable Y and the balance variable\r\rThe relationship between variable Y and the duration variable\r### The relationship between variable Y and the Campaign variable\r\rThe relationship between variable Y and the pdays variable\r\rThe relationship between variable Y and the previous variable\r\rThe relationship between variable Y and the Job variable\rWe can see that the Management has the most subscription among other jobs.\n\rThe relationship between variable Y and the marital variable\rMarried people has more subscription that single and divorced.\n\rThe relationship between variable Y and the education variable\rSecondary education has the more subscription than the remaining education types.\r\rThe relationship between variable Y and the default variable\rThe clients that have default status are not subscribed to term deposit\r\rThe relationship between variable Y and the housing variable\rClients have not housing loan are subscribed to term deposit more than clients having housing loan.\r\rThe relationship between variable Y and the loan variable\rClients haven’t loan are subscribed to term deposit more than clients having loan.\r\rThe relationship between variable Y and the contact variable\rWe can see that cellular connection type results in more subscribtion to a term deposit\r\rThe relationship between variable Y and the day variable\r\rThe relationship between variable Y and the month variable\rThe Contact that happend in May results in more subscription to term deposit than other months.\r\rThe relationship between variable Y and the poutcome variable\rThe unknown outcome has the most subscription to term deposit.\n\r\rThe simple logestic regression models\rThe logistic regression model of y using the predictor variable age only\rFrom the model we can see that age variable is a significant variable for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ age, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5813 -0.5081 -0.4939 -0.4855 2.1290 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.265534 0.066949 -33.840 \u0026lt; 2e-16 ***\r## age 0.006031 0.001564 3.857 0.000115 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24516 on 33906 degrees of freedom\r## AIC: 24520\r## ## Number of Fisher Scoring iterations: 4\r\rThe logistic regression model of y using the predictor variable job only\rFrom the model we can see that out of 11 job types blue-collar, entrepreneur, housemaid, retired, services and student are the most significant job types in the prediction of the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ job, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.8267 -0.5484 -0.4823 -0.3898 2.2871 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.96033 0.04865 -40.294 \u0026lt; 2e-16 ***\r## jobblue-collar -0.57904 0.06635 -8.727 \u0026lt; 2e-16 ***\r## jobentrepreneur -0.42491 0.11826 -3.593 0.000327 ***\r## jobhousemaid -0.37660 0.12482 -3.017 0.002551 ** ## jobmanagement 0.14183 0.05950 2.383 0.017152 * ## jobretired 0.73374 0.07620 9.629 \u0026lt; 2e-16 ***\r## jobself-employed -0.04850 0.10148 -0.478 0.632730 ## jobservices -0.36818 0.07946 -4.633 3.6e-06 ***\r## jobstudent 1.06239 0.09734 10.914 \u0026lt; 2e-16 ***\r## jobtechnician -0.13271 0.06442 -2.060 0.039389 * ## jobunemployed 0.28798 0.10040 2.868 0.004126 ** ## jobunknown -0.15645 0.22601 -0.692 0.488794 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 23973 on 33896 degrees of freedom\r## AIC: 23997\r## ## Number of Fisher Scoring iterations: 5\r\rThe logistic regression model of y using the predictor variable marital only\rFrom the model we can see that all the marital status are considered significant variables for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ marital, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5701 -0.5109 -0.4619 -0.4619 2.1404 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.97041 0.04882 -40.359 \u0026lt; 2e-16 ***\r## maritalmarried -0.21361 0.05405 -3.952 7.76e-05 ***\r## maritalsingle 0.23561 0.05659 4.163 3.14e-05 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24385 on 33905 degrees of freedom\r## AIC: 24391\r## ## Number of Fisher Scoring iterations: 4\r\rThe logistic regression model of y using the predictor variable education only\rFrom the model we can see that secondary, tertiary and unknown education types are significant variables for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ education, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5695 -0.5695 -0.4742 -0.4267 2.2099 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.35073 0.04954 -47.450 \u0026lt; 2e-16 ***\r## educationsecondary 0.22198 0.05530 4.014 5.98e-05 ***\r## educationtertiary 0.61370 0.05693 10.781 \u0026lt; 2e-16 ***\r## educationunknown 0.50519 0.09299 5.433 5.54e-08 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24362 on 33904 degrees of freedom\r## AIC: 24370\r## ## Number of Fisher Scoring iterations: 4\r\rThe logistic regression model of y using the predictor variable default only\rFrom the model we can see that both default cases are significant for the prediction of the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ default, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5023 -0.5023 -0.5023 -0.5023 2.3732 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.00667 0.01696 -118.304 \u0026lt; 2e-16 ***\r## defaultyes -0.74766 0.16816 -4.446 8.74e-06 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24506 on 33906 degrees of freedom\r## AIC: 24510\r## ## Number of Fisher Scoring iterations: 5\r\rThe logistic regression model of y using the predictor variable balance only\rFrom the model we can see that both balance cases are considered significant for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ balance, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2789 -0.4959 -0.4871 -0.4840 2.1568 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.085e+00 1.847e-02 -112.895 \u0026lt;2e-16 ***\r## balance 4.509e-05 4.518e-06 9.979 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24437 on 33906 degrees of freedom\r## AIC: 24441\r## ## Number of Fisher Scoring iterations: 4\r\rThe logistic regression model of y using the predictor variable housing only\rFrom the model we can see that wether owning a house loan or not, both cases are considered significant for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ housing, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.6025 -0.6025 -0.4042 -0.4042 2.2563 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.61438 0.02188 -73.79 \u0026lt;2e-16 ***\r## housingyes -0.84930 0.03484 -24.38 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 23913 on 33906 degrees of freedom\r## AIC: 23917\r## ## Number of Fisher Scoring iterations: 5\r\rThe logistic regression model of y using the predictor variable loan only\rFrom the model we can see that both cases of the presence and absence of loan are considered significant for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ loan, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5207 -0.5207 -0.5207 -0.3768 2.3155 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.92995 0.01782 -108.33 \u0026lt;2e-16 ***\r## loanyes -0.67977 0.05653 -12.03 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24362 on 33906 degrees of freedom\r## AIC: 24366\r## ## Number of Fisher Scoring iterations: 5\r\rThe logistic regression model of y using the predictor variable contact only\rFrom the model we can see that the cellular and unknown contact types are considered significant for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ contact, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5705 -0.5705 -0.5705 -0.2863 2.5357 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.73311 0.01885 -91.946 \u0026lt;2e-16 ***\r## contacttelephone -0.15788 0.06705 -2.355 0.0185 * ## contactunknown -1.44078 0.05494 -26.224 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 23585 on 33905 degrees of freedom\r## AIC: 23591\r## ## Number of Fisher Scoring iterations: 5\r\rThe logistic regression model of y using the predictor variable day only\rFrom the model we can see that day variable is a significant variable for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ day, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.8618 -0.5489 -0.4645 -0.3888 2.3639 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -0.7992 0.1381 -5.786 7.19e-09 ***\r## day10 -0.5139 0.1844 -2.787 0.00532 ** ## day11 -1.1994 0.1664 -7.210 5.61e-13 ***\r## day12 -0.9022 0.1594 -5.661 1.51e-08 ***\r## day13 -0.9307 0.1606 -5.794 6.87e-09 ***\r## day14 -1.2945 0.1633 -7.928 2.23e-15 ***\r## day15 -0.9899 0.1591 -6.222 4.92e-10 ***\r## day16 -1.0057 0.1635 -6.150 7.73e-10 ***\r## day17 -1.4617 0.1647 -8.873 \u0026lt; 2e-16 ***\r## day18 -1.3807 0.1588 -8.693 \u0026lt; 2e-16 ***\r## day19 -1.9000 0.1787 -10.632 \u0026lt; 2e-16 ***\r## day2 -0.9752 0.1665 -5.855 4.77e-09 ***\r## day20 -1.8062 0.1635 -11.044 \u0026lt; 2e-16 ***\r## day21 -1.3730 0.1618 -8.487 \u0026lt; 2e-16 ***\r## day22 -0.8387 0.1727 -4.856 1.20e-06 ***\r## day23 -1.0173 0.1752 -5.806 6.40e-09 ***\r## day24 -1.0206 0.2093 -4.876 1.08e-06 ***\r## day25 -1.0311 0.1809 -5.699 1.20e-08 ***\r## day26 -1.2114 0.1772 -6.836 8.13e-12 ***\r## day27 -1.0265 0.1705 -6.019 1.75e-09 ***\r## day28 -1.6408 0.1697 -9.672 \u0026lt; 2e-16 ***\r## day29 -1.7452 0.1734 -10.062 \u0026lt; 2e-16 ***\r## day3 -0.8357 0.1677 -4.983 6.26e-07 ***\r## day30 -0.7284 0.1581 -4.608 4.06e-06 ***\r## day31 -1.9316 0.2362 -8.177 2.91e-16 ***\r## day4 -0.8272 0.1609 -5.142 2.71e-07 ***\r## day5 -1.2481 0.1614 -7.734 1.04e-14 ***\r## day6 -1.4638 0.1644 -8.905 \u0026lt; 2e-16 ***\r## day7 -1.6730 0.1715 -9.756 \u0026lt; 2e-16 ***\r## day8 -1.2551 0.1622 -7.738 1.01e-14 ***\r## day9 -1.1965 0.1649 -7.256 3.98e-13 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24083 on 33877 degrees of freedom\r## AIC: 24145\r## ## Number of Fisher Scoring iterations: 5\r\rThe logistic regression model of y using the predictor variable month only\rFrom the model we can see that all the months are significant for predicting the possibility of subscription to a term deposit except for February.\n## ## Call:\r## glm(formula = y ~ month, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.1699 -0.4672 -0.4436 -0.3748 2.3198 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.35179 0.05248 -25.757 \u0026lt; 2e-16 ***\r## monthaug -0.77239 0.07066 -10.930 \u0026lt; 2e-16 ***\r## monthdec 1.24125 0.16544 7.503 6.24e-14 ***\r## monthfeb -0.23880 0.07988 -2.990 0.00279 ** ## monthjan -0.84753 0.11504 -7.367 1.74e-13 ***\r## monthjul -0.91732 0.07079 -12.959 \u0026lt; 2e-16 ***\r## monthjun -0.80843 0.07373 -10.965 \u0026lt; 2e-16 ***\r## monthmar 1.33409 0.12064 11.058 \u0026lt; 2e-16 ***\r## monthmay -1.26873 0.06554 -19.357 \u0026lt; 2e-16 ***\r## monthnov -0.84432 0.08037 -10.506 \u0026lt; 2e-16 ***\r## monthoct 1.07262 0.10098 10.622 \u0026lt; 2e-16 ***\r## monthsep 1.25381 0.11000 11.398 \u0026lt; 2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 22913 on 33896 degrees of freedom\r## AIC: 22937\r## ## Number of Fisher Scoring iterations: 5\r\rThe logistic regression model of y using the predictor variable duration only\rFrom the model we can see that duartion variable is a significant variable for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ duration, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.5727 -0.4383 -0.3609 -0.3169 2.5213 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -3.219e+00 3.041e-02 -105.87 \u0026lt;2e-16 ***\r## duration 3.613e-03 6.395e-05 56.49 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 20523 on 33906 degrees of freedom\r## AIC: 20527\r## ## Number of Fisher Scoring iterations: 5\r\rThe logistic regression model of y using the predictor variable campaign only\rFrom the model we can see that the number of contacts performed during this campaign and for the client variable is a significant variable for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ campaign, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.5430 -0.5430 -0.5127 -0.4565 3.3628 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.716973 0.026529 -64.72 \u0026lt;2e-16 ***\r## campaign -0.122932 0.009274 -13.26 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24286 on 33906 degrees of freedom\r## AIC: 24290\r## ## Number of Fisher Scoring iterations: 5\r\rThe logistic regression model of y using the predictor variable pdays only\rFrom the model we can see that number of days that passed by after the client was last contacted from a previous campaign is significant variable for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ pdays, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.2040 -0.4695 -0.4695 -0.4695 2.1260 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.1471962 0.0189762 -113.15 \u0026lt;2e-16 ***\r## pdays 0.0025369 0.0001365 18.59 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24221 on 33906 degrees of freedom\r## AIC: 24225\r## ## Number of Fisher Scoring iterations: 4\r\rThe logistic regression model of y using the predictor variable previous only\rFrom the model we can see that the number of contacts performed before this campaign and for this client significant variable for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ previous, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -7.5518 -0.4809 -0.4809 -0.4809 2.1045 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.098905 0.017963 -116.85 \u0026lt;2e-16 ***\r## previous 0.111322 0.006967 15.98 \u0026lt;2e-16 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 24275 on 33906 degrees of freedom\r## AIC: 24279\r## ## Number of Fisher Scoring iterations: 4\r\rThe logistic regression model of y using the predictor variable poutcome only\rFrom the model we can see that outcome of the previous marketing campaign is significant variable for predicting the possibility of subscription to a term deposit.\n## ## Call:\r## glm(formula = y ~ poutcome, family = \u0026quot;binomial\u0026quot;, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.4439 -0.4394 -0.4394 -0.4394 2.1844 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -1.94806 0.04962 -39.258 \u0026lt; 2e-16 ***\r## poutcomeother 0.34468 0.08741 3.943 8.04e-05 ***\r## poutcomesuccess 2.55578 0.07934 32.212 \u0026lt; 2e-16 ***\r## poutcomeunknown -0.34131 0.05381 -6.343 2.25e-10 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 22527 on 33904 degrees of freedom\r## AIC: 22535\r## ## Number of Fisher Scoring iterations: 5\r\r\rThe multiple Logistic regression Model:\rFrom the model we can see that the significance of the some of the variables have been changed to less significant or not even significant at all where the significant variables have three stars beside them (P\u0026lt;0.05) and the less significant variables have two or one star only while the not significant variables have no stars at all.\n## ## Call:\r## glm(formula = y ~ ., family = binomial, data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -5.0852 -0.3691 -0.2436 -0.1456 3.2933 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.304e+00 2.720e-01 -8.469 \u0026lt; 2e-16 ***\r## age -1.211e-03 2.563e-03 -0.472 0.636697 ## jobblue-collar -2.650e-01 8.448e-02 -3.137 0.001706 ** ## jobentrepreneur -2.381e-01 1.450e-01 -1.642 0.100498 ## jobhousemaid -5.144e-01 1.576e-01 -3.263 0.001102 ** ## jobmanagement -1.135e-01 8.532e-02 -1.331 0.183270 ## jobretired 2.829e-01 1.145e-01 2.472 0.013442 * ## jobself-employed -2.433e-01 1.289e-01 -1.887 0.059228 . ## jobservices -2.076e-01 9.769e-02 -2.125 0.033551 * ## jobstudent 3.205e-01 1.281e-01 2.502 0.012336 * ## jobtechnician -1.736e-01 8.012e-02 -2.166 0.030285 * ## jobunemployed -1.602e-01 1.286e-01 -1.245 0.212992 ## jobunknown -4.341e-01 2.910e-01 -1.492 0.135709 ## maritalmarried -2.019e-01 6.818e-02 -2.961 0.003065 ** ## maritalsingle 5.891e-02 7.802e-02 0.755 0.450211 ## educationsecondary 2.114e-01 7.560e-02 2.796 0.005168 ** ## educationtertiary 3.631e-01 8.787e-02 4.132 3.60e-05 ***\r## educationunknown 2.336e-01 1.219e-01 1.917 0.055238 . ## defaultyes -1.291e-01 1.905e-01 -0.678 0.498028 ## balance 1.795e-05 6.039e-06 2.972 0.002957 ** ## housingyes -6.490e-01 5.115e-02 -12.687 \u0026lt; 2e-16 ***\r## loanyes -4.321e-01 6.948e-02 -6.220 4.97e-10 ***\r## contacttelephone -2.149e-01 8.922e-02 -2.408 0.016021 * ## contactunknown -1.711e+00 8.665e-02 -19.743 \u0026lt; 2e-16 ***\r## day10 2.605e-01 2.375e-01 1.097 0.272642 ## day11 -2.541e-01 2.116e-01 -1.200 0.229957 ## day12 1.451e-01 2.063e-01 0.703 0.481974 ## day13 2.497e-01 2.085e-01 1.197 0.231157 ## day14 -8.085e-02 2.098e-01 -0.385 0.699887 ## day15 2.320e-02 2.059e-01 0.113 0.910287 ## day16 -6.925e-02 2.097e-01 -0.330 0.741199 ## day17 -7.200e-01 2.107e-01 -3.417 0.000634 ***\r## day18 -3.113e-01 2.051e-01 -1.518 0.129039 ## day19 -8.877e-01 2.303e-01 -3.855 0.000116 ***\r## day2 -3.155e-01 2.127e-01 -1.483 0.138033 ## day20 -6.166e-01 2.101e-01 -2.935 0.003340 ** ## day21 -2.021e-01 2.124e-01 -0.952 0.341316 ## day22 -9.651e-02 2.223e-01 -0.434 0.664198 ## day23 3.233e-01 2.288e-01 1.413 0.157706 ## day24 -1.901e-01 2.624e-01 -0.724 0.468894 ## day25 -5.974e-02 2.292e-01 -0.261 0.794402 ## day26 1.647e-01 2.279e-01 0.723 0.469790 ## day27 5.321e-01 2.205e-01 2.413 0.015805 * ## day28 -1.186e-01 2.230e-01 -0.532 0.594785 ## day29 -3.486e-01 2.244e-01 -1.553 0.120368 ## day3 -2.349e-01 2.146e-01 -1.095 0.273676 ## day30 2.731e-01 2.073e-01 1.317 0.187693 ## day31 -4.008e-01 3.084e-01 -1.300 0.193692 ## day4 -1.461e-01 2.065e-01 -0.708 0.479104 ## day5 -3.837e-01 2.069e-01 -1.855 0.063578 . ## day6 -3.904e-01 2.110e-01 -1.850 0.064293 . ## day7 -6.767e-01 2.180e-01 -3.103 0.001913 ** ## day8 -1.466e-01 2.074e-01 -0.707 0.479685 ## day9 -8.388e-02 2.137e-01 -0.392 0.694708 ## monthaug -8.271e-01 9.897e-02 -8.357 \u0026lt; 2e-16 ***\r## monthdec 6.842e-01 2.095e-01 3.265 0.001093 ** ## monthfeb -2.137e-01 1.120e-01 -1.908 0.056345 . ## monthjan -1.284e+00 1.537e-01 -8.354 \u0026lt; 2e-16 ***\r## monthjul -9.017e-01 9.562e-02 -9.430 \u0026lt; 2e-16 ***\r## monthjun 4.794e-01 1.127e-01 4.255 2.09e-05 ***\r## monthmar 1.375e+00 1.467e-01 9.373 \u0026lt; 2e-16 ***\r## monthmay -5.297e-01 9.272e-02 -5.713 1.11e-08 ***\r## monthnov -6.742e-01 1.085e-01 -6.213 5.20e-10 ***\r## monthoct 7.727e-01 1.296e-01 5.961 2.51e-09 ***\r## monthsep 8.835e-01 1.423e-01 6.210 5.31e-10 ***\r## duration 4.315e-03 7.605e-05 56.743 \u0026lt; 2e-16 ***\r## campaign -7.668e-02 1.149e-02 -6.673 2.51e-11 ***\r## pdays 2.190e-04 3.497e-04 0.626 0.531150 ## previous 9.059e-03 6.566e-03 1.380 0.167650 ## poutcomeother 2.173e-01 1.048e-01 2.072 0.038242 * ## poutcomesuccess 2.269e+00 9.590e-02 23.666 \u0026lt; 2e-16 ***\r## poutcomeunknown 5.218e-02 1.079e-01 0.484 0.628580 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 15898 on 33836 degrees of freedom\r## AIC: 16042\r## ## Number of Fisher Scoring iterations: 6\r\rEvaluating our multiple Logistic regression Model\rIn this part we will use the test data to pass it to the multiple Logistic regression Model in order to classify whether clients associated with the given information in the test data set will subscribe to a term deposit. Also we will evaluate the performance of the model through using the confusion matrix\nAfter passing the test data set that represent 25% of the original data set to the developed multiple Logistic regression Model and evaluating the classification output with reference to the original output we got accuracy of 90.2g%.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000337 0.0181261 0.0413998 0.1176430 0.1112072 1.0000000\r## # A tibble: 13 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.902 ## 2 kap binary 0.404 ## 3 sens binary 0.350 ## 4 spec binary 0.974 ## 5 ppv binary 0.640 ## 6 npv binary 0.920 ## 7 mcc binary 0.426 ## 8 j_index binary 0.324 ## 9 bal_accuracy binary 0.662 ## 10 detection_prevalence binary 0.0633\r## 11 precision binary 0.640 ## 12 recall binary 0.350 ## 13 f_meas binary 0.453\r\rUpdating the model with the significant parameters only\rIn this part we will update our model to include only the significant predictors in the process of classification.\nBased on the obtained p-values from the previous model we can see that out of 17 predictors, 9 are considered the significant predictors. Accordingly our model would be updated to include the 9 significant predictors only.\nupdated_mdl\u0026lt;- update (full_mdl , ~ job+ education +housing+ loan+contact +month+duration+campaign+poutcome)\rsummary(updated_mdl)\r## ## Call:\r## glm(formula = y ~ job + education + housing + loan + contact + ## month + duration + campaign + poutcome, family = binomial, ## data = data)\r## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -4.9645 -0.3750 -0.2534 -0.1489 3.2598 ## ## Coefficients:\r## Estimate Std. Error z value Pr(\u0026gt;|z|) ## (Intercept) -2.517e+00 1.290e-01 -19.508 \u0026lt; 2e-16 ***\r## jobblue-collar -3.312e-01 8.352e-02 -3.966 7.32e-05 ***\r## jobentrepreneur -3.390e-01 1.433e-01 -2.366 0.017980 * ## jobhousemaid -5.679e-01 1.562e-01 -3.635 0.000278 ***\r## jobmanagement -1.595e-01 8.434e-02 -1.892 0.058548 . ## jobretired 2.015e-01 1.007e-01 2.002 0.045247 * ## jobself-employed -2.792e-01 1.275e-01 -2.190 0.028496 * ## jobservices -2.385e-01 9.671e-02 -2.467 0.013640 * ## jobstudent 4.857e-01 1.218e-01 3.988 6.67e-05 ***\r## jobtechnician -1.873e-01 7.943e-02 -2.358 0.018376 * ## jobunemployed -1.650e-01 1.274e-01 -1.295 0.195302 ## jobunknown -5.284e-01 2.871e-01 -1.841 0.065681 . ## educationsecondary 2.271e-01 7.440e-02 3.053 0.002267 ** ## educationtertiary 4.319e-01 8.568e-02 5.041 4.63e-07 ***\r## educationunknown 2.634e-01 1.203e-01 2.190 0.028551 * ## housingyes -7.095e-01 4.994e-02 -14.207 \u0026lt; 2e-16 ***\r## loanyes -4.660e-01 6.869e-02 -6.783 1.18e-11 ***\r## contacttelephone -2.023e-01 8.762e-02 -2.309 0.020922 * ## contactunknown -1.711e+00 8.430e-02 -20.291 \u0026lt; 2e-16 ***\r## monthaug -8.428e-01 8.971e-02 -9.395 \u0026lt; 2e-16 ***\r## monthdec 6.012e-01 2.038e-01 2.950 0.003178 ** ## monthfeb -2.390e-01 9.700e-02 -2.464 0.013729 * ## monthjan -1.179e+00 1.389e-01 -8.487 \u0026lt; 2e-16 ***\r## monthjul -8.314e-01 8.806e-02 -9.441 \u0026lt; 2e-16 ***\r## monthjun 4.221e-01 1.039e-01 4.061 4.88e-05 ***\r## monthmar 1.447e+00 1.408e-01 10.278 \u0026lt; 2e-16 ***\r## monthmay -3.879e-01 8.153e-02 -4.758 1.96e-06 ***\r## monthnov -8.996e-01 9.671e-02 -9.302 \u0026lt; 2e-16 ***\r## monthoct 8.534e-01 1.251e-01 6.822 8.97e-12 ***\r## monthsep 9.127e-01 1.366e-01 6.682 2.35e-11 ***\r## duration 4.278e-03 7.526e-05 56.841 \u0026lt; 2e-16 ***\r## campaign -7.553e-02 1.130e-02 -6.682 2.35e-11 ***\r## poutcomeother 2.521e-01 1.034e-01 2.438 0.014763 * ## poutcomesuccess 2.346e+00 9.203e-02 25.491 \u0026lt; 2e-16 ***\r## poutcomeunknown -1.179e-02 6.651e-02 -0.177 0.859258 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## (Dispersion parameter for binomial family taken to be 1)\r## ## Null deviance: 24531 on 33907 degrees of freedom\r## Residual deviance: 16143 on 33873 degrees of freedom\r## AIC: 16213\r## ## Number of Fisher Scoring iterations: 6\r\rEvaluating the updated Model\rAfter passing the test data set to the updated model and evaluating the classification output with respect to the observed values we got accuracy of 90.3% which is higher than the accuracy that we got by the model which includes all the predictors.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000337 0.0181261 0.0413998 0.1176430 0.1112072 1.0000000\r## # A tibble: 13 x 3\r## .metric .estimator .estimate\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 accuracy binary 0.903 ## 2 kap binary 0.405 ## 3 sens binary 0.349 ## 4 spec binary 0.975 ## 5 ppv binary 0.645 ## 6 npv binary 0.920 ## 7 mcc binary 0.428 ## 8 j_index binary 0.324 ## 9 bal_accuracy binary 0.662 ## 10 detection_prevalence binary 0.0626\r## 11 precision binary 0.645 ## 12 recall binary 0.349 ## 13 f_meas binary 0.453\r\rConclusion\rBy comparing the accuracy associated with the first model that used all the predictors to the second model that used only the significant predictors, The accuracy associated with the second model is considered higher than the first one and the second model is considered less computationally expensive than the first model.\n\r","date":1626480000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626533484,"objectID":"2957ab45bf02b03cc959e3bae17578c9","permalink":"/post/a02-classification-model/","publishdate":"2021-07-17T00:00:00Z","relpermalink":"/post/a02-classification-model/","section":"post","summary":"The Goal\rThe goal behind this study is to use the provided data set to identify whether a customer will subscribe to a term deposit or not.\n\rThe Bank data\rThe data used in this study was obtained from UCI  website and we specifically used the Bank-full.","tags":[],"title":"A02 Classification Model","type":"post"},{"authors":[],"categories":["ggplot2","Basics"],"content":"\r\rThe diamond ring data\rIn this blog we used the diamond ring data. Getting a glimpse into the data we can see that it is composed of 440 rows where each row represents a diamond ring and for each ring the diamond characteristics are provided that are Color, Cut, Carat Weight, Clarity , Polish, Symmetry, and certification in addition to the price.\n## Rows: 440\r## Columns: 9\r## $ carat \u0026lt;dbl\u0026gt; 0.92, 0.92, 0.82, 0.81, 0.90, 0.87, 0.80, 0.84, 0.80, 0.~\r## $ colour \u0026lt;chr\u0026gt; \u0026quot;I\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;J\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;F\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;D\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;~\r## $ clarity \u0026lt;chr\u0026gt; \u0026quot;SI2\u0026quot;, \u0026quot;SI2\u0026quot;, \u0026quot;SI2\u0026quot;, \u0026quot;SI1\u0026quot;, \u0026quot;VS2\u0026quot;, \u0026quot;SI2\u0026quot;, \u0026quot;SI2\u0026quot;, \u0026quot;SI1\u0026quot;, ~\r## $ cut \u0026lt;chr\u0026gt; \u0026quot;G\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;I\u0026quot;, \u0026quot;~\r## $ certification \u0026lt;chr\u0026gt; \u0026quot;AGS\u0026quot;, \u0026quot;AGS\u0026quot;, \u0026quot;GIA\u0026quot;, \u0026quot;GIA\u0026quot;, \u0026quot;GIA\u0026quot;, \u0026quot;AGS\u0026quot;, \u0026quot;GIA\u0026quot;, \u0026quot;GIA\u0026quot;, ~\r## $ polish \u0026lt;chr\u0026gt; \u0026quot;V\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;~\r## $ symmetry \u0026lt;chr\u0026gt; \u0026quot;V\u0026quot;, \u0026quot;G\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;X\u0026quot;, \u0026quot;V\u0026quot;, \u0026quot;~\r## $ price \u0026lt;dbl\u0026gt; 3000, 3000, 3004, 3004, 3006, 3007, 3008, 3010, 3012, 30~\r## $ wholesaler \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~\r\rThe Univriate Analysis\rwe will start our analysis with the metric data which are the price and the carat mass of the diamond ring then we will analyse the non metric data which are the colour, clarity, cut, certification, polish, symmetry, and wholesaler.\nThe Price\rFrom the following plots and provided summary, we can see that the diamond ring price ranges from minimum value of $160 to maximum value of $3145, the mean of the price is $1717, where in most of the diamond ring purchases (mode) the price was $520 and we can also see that the first quartile is $520, the third quartile is $3012 and the IQR is $2429 with median price of $2169.\nThe price dispersion can be evaluated through a variance of $1382245 and standard deviation of $1175.689.\nThe data distribution has no specific shape but we can notice that there is no diamond ring was sold in the price range from $700 to $1700 similarly at price of $2900.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 160 520 2169 1717 3012 3145\r## [1] 1382245\r## [1] 1175.689\r\rThe Carat mass\rFrom the following plots and provided summary, we can see that the diamond ring carat mass ranges from minimum value of 0.09 gram to maximum value of 1.58 gram, the mean of the carat mass is 0.6693 gram, where in most of the diamond ring purchases (mode) the carat mass was 0.3 gram and we can also see that the first quartile is 0.3 gram, the third quartile is 1.01 gram and the IQR is 0.71 gram with median carat mass of 0.81 gram.\nThe carat mass dispersion can be evaluated through a variance of 0.144248 grams and standard deviation of 0.3797999 grams.\nThe data distribution has no specific shape but we can notice that there is no carat mass was sold in the carat mass range from 0.4 gram to 0.7 gram similarly at carat mass of 1.5 grams.\n## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0900 0.3000 0.8100 0.6693 1.0100 1.5800\r## [1] 0.144248\r## [1] 0.3797999\r\rThe Colour\rFrom the histogram plot of the color, we can see that the most purchased color for the diamond rings is the color I and after that are color J and color H and the least purchased are color L and color D.\n\rThe Clarity\rFrom the histogram plot of the clarity, we can see that the most purchased clarity type for the diamond rings is the clarity Sl1 and after that is clarity Sl2 and the least purchased are WS1 and Ws2.\n\rThe Cut\rFrom the histogram plot of the diamond ring cut, we can see that the most purchased cut type for the diamond rings is X and after that is cut V then cut I while the least purchased cuts are cut G and cut F.\n\rThe Certification\rFrom the histogram plot of the diamond ring certification, we can see that the most purchased certification type for the diamond rings is GIA and after that is EGL then IGI while the least purchased certifications are DOW and AGS.\n\rThe Polish\rFrom the histogram plot of the diamond ring polish, we can see that the most purchased polish type for the diamond rings is V and after that is G then X while the least purchased polish types are V,I and F.\n\rThe Symmetry\rFrom the histogram plot of the diamond ring symmetry, we can see that the most purchased symmetry type for the diamond rings is V and after that is G then X while the least purchased symmetry types are I and F.\n\rWholesaler\rFrom the histogram plot of the diamond ring wholesaler, we can see that the most of the purchased diamond rings were purchased from wholesaler number 3 then wholesaler number 2, while the fewer purchases done at the wholesaler number 1.\n\r\rThe Bivariate Analysis and regressision models\rThe simple regression model for the price Vs. the carat mass\rThe correlation between the price and the carat mass of the diamond ring is 0.925436 that means that the two variables have a perfect positive relationship so that when the carat mass increases the price increase and vice versa.\nThe covariance between the price and the carat mass of the diamond ring is 413.2318 which shows the extent to which the two variables change in tandem.\n## [1] 0.925436\r## [1] 413.2318\rIn the following plot, a simple linear regression model has been fitted that can help in the prediction of the price of the diamond ring based on the carat mass.\nThe simple linear regression model of the price Vs. the carat mass can be represented by:\n\\[\\widehat{Y}_{i} = \\beta_0 + \\beta_1 \\times CaratMass_{i}\\]\r\\[\\widehat{Price} =-200.5 + 2864.7 \\times Carat Mass\\]\n## [1] 0.925436\r## [1] 413.2318\r## ## Call:\r## lm(formula = price ~ carat, data = data)\r## ## Coefficients:\r## (Intercept) carat ## -200.5 2864.7\r\rThe simple regression model for the price Vs. the colour\rThe correlation between the price and the color of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the colour of the diamond ring is very weak.\n## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## colour 8 51620143 6452518 5.009 5.87e-06 ***\r## Residuals 431 555185378 1288133 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe simple linear regression model of the price Vs. the color can be represented by:\n\\[\\widehat{Y} = \\beta_0 + \\beta_1 \\times colorE+ \\beta_2 \\times colorF+ \\beta_3 \\times colorG+ \\beta_4\\times colorH+ \\beta_5 \\times colorI+ \\beta_6 \\times colorJ+ \\beta_7 \\times colorK+ \\beta_8 \\times colorL\\]\r\\[\\widehat{Price} =2316.20 -764.29 \\times colorE -982.36 \\times colorF -147.73 \\times colorG -873.92\\times colorH -765.88 \\times colorI -535.28 \\times colorJ+ 42.06 \\times colorK+ 52.30\\times colorL\\]\n## ## Call:\r## lm(formula = price ~ colour, data = data)\r## ## Coefficients:\r## (Intercept) colourE colourF colourG colourH colourI ## 2316.20 -764.29 -982.36 -147.73 -873.92 -765.88 ## colourJ colourK colourL ## -535.28 42.06 52.30\r\rThe simple regression model for the price Vs. the clarity\rThe correlation between the price and the clarity of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the clarity of the diamond ring is very weak.\n## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## clarity 8 194938677 24367335 25.5 \u0026lt;2e-16 ***\r## Residuals 431 411866844 955608 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe simple linear regression model of the price Vs. the clarity can be represented by:\n\\[\\widehat{Y} = \\beta_0 + \\beta_1 \\times clarityI2 + \\beta_2 \\times claritySI1+ \\beta_3 \\times claritySI2+ \\beta_4\\times claritySI3 + \\beta_5 \\times clarityVS1+ \\beta_6 \\times clarityVS2+ \\beta_7 \\times clarityVVS1 + \\beta_8 \\times clarityVVS2\\]\n\\[\\widehat{Price} = 2543.15 -201.22 \\times clarityI2 -1495.53 \\times claritySI1-568.95 \\times claritySI2+ 76.24\\times claritySI3 -1405.38 \\times clarityVS1-1654.95 \\times clarityVS2-1996.15 \\times clarityVVS1 -1978.95 \\times clarityVVS2\\]\nlm(price ~ clarity, data = data)\r## ## Call:\r## lm(formula = price ~ clarity, data = data)\r## ## Coefficients:\r## (Intercept) clarityI2 claritySI1 claritySI2 claritySI3 clarityVS1 ## 2543.15 -201.22 -1495.53 -568.95 76.24 -1405.38 ## clarityVS2 clarityVVS1 clarityVVS2 ## -1654.95 -1996.15 -1978.95\r\rThe simple regression model for the price Vs. cut\rThe correlation between the price and the cut of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the cut of the diamond ring is very weak.\n## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## cut 4 66164570 16541142 13.31 3.07e-10 ***\r## Residuals 435 540640951 1242853 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe simple linear regression model of the price Vs. the cut can be represented by:\n\\[\\widehat{Y} = \\beta_0 + \\beta_1 \\times Cut G+ \\beta_2 \\times Cut I+ \\beta_3 \\times Cut V+ \\beta_4\\times Cut X\\]\n\\[\\widehat{Price} =2455.2-409.2 \\times Cut G -723.5 \\times Cut I-1277.1 \\times Cut V -797.2\\times Cut X \\]\n## ## Call:\r## lm(formula = price ~ cut, data = data)\r## ## Coefficients:\r## (Intercept) cutG cutI cutV cutX ## 2455.2 -409.2 -723.5 -1277.1 -797.2\r\rThe simple regression model for the price Vs. the certification\rThe correlation between the price and the certification of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the certification of the diamond ring is very weak.\n## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## certification 4 238869617 59717404 70.6 \u0026lt;2e-16 ***\r## Residuals 435 367935903 845830 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe simple linear regression model of the price Vs. the certification can be represented by:\n\\[\\widehat{Y} = \\beta_0 + \\beta_1 \\times certificationDOW + \\beta_2 \\times certificationEGL + \\beta_3 \\times certificationGIA + \\beta_4\\times certificationIGI \\]\n\\[\\widehat{Price} = 3033.4 -1002.4 \\times certificationDOW -355.6 \\times certificationEGL -1573.6 \\times certificationGIA -2767.9 \\times certificationIGI \\]\n## ## Call:\r## lm(formula = price ~ certification, data = data)\r## ## Coefficients:\r## (Intercept) certificationDOW certificationEGL certificationGIA ## 3033.4 -1002.4 -355.6 -1573.6 ## certificationIGI ## -2767.9\r\rThe simple regression model for the price Vs. the polish\rThe correlation between the price and the polish of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the certification of the diamond ring is very weak.\n## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## polish 5 28574143 5714829 4.289 0.000808 ***\r## Residuals 434 578231378 1332330 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe simple linear regression model of the price Vs. the polish can be represented by:\n\\[\\widehat{Y} = \\beta_0 + \\beta_1 \\times polishG + \\beta_2 \\times polishI + \\beta_3 \\times polishV + \\beta_4\\times polishV + \\beta_5\\times polishX \\]\r\\[\\widehat{Price} = 2318.6 -404.0 \\times polishG + 728.8 \\times polishI + 762.4 \\times polishV -715.5\\times polishV -939.8 \\times polishX \\]\nlm(price ~ polish, data = data)\r## ## Call:\r## lm(formula = price ~ polish, data = data)\r## ## Coefficients:\r## (Intercept) polishG polishI polishv polishV polishX ## 2318.6 -404.0 728.8 762.4 -715.5 -939.8\r\rThe simple regression model for the price Vs. the symmetry\rThe correlation between the price and the symmetry of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the symmetry of the diamond ring is very weak.\n## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## symmetry 4 37655698 9413925 7.195 1.29e-05 ***\r## Residuals 435 569149823 1308390 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe simple linear regression model of the price Vs. the symmetry can be represented by:\n\\[\\widehat{Y} = \\beta_0 + \\beta_1 \\times symmetryG + \\beta_2 \\times symmetryI + \\beta_3 \\times symmetryV + \\beta_4\\times symmetryX\\]\n\\[\\widehat{Price} = 2432.3 -537.9 \\times symmetryG+615.1 \\times symmetryI -966.8 \\times symmetryV -673.0\\times symmetryX\\]\n## ## Call:\r## lm(formula = price ~ symmetry, data = data)\r## ## Coefficients:\r## (Intercept) symmetryG symmetryI symmetryV symmetryX ## 2432.3 -537.9 615.1 -966.8 -673.0\r\rThe simple regression model for the price Vs. the wholesaler\rThe correlation between the price and the wholesaler of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the certification of the diamond ring is very weak.\n## Df Sum Sq Mean Sq F value Pr(\u0026gt;F) ## factor(wholesaler) 2 578213494 289106747 4419 \u0026lt;2e-16 ***\r## Residuals 437 28592027 65428 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe simple linear regression model of the price Vs. the wholesaler can be represented by:\n\\[\\widehat{Y} = \\beta_0 + \\beta_1 \\times wholesaler1 + \\beta_2 \\times wholesaler2 \\]\r\\[\\widehat{Price} = 3043.2 -381.2 \\times wholesaler1 -2575.1 \\times wholesaler2 \\]\n## ## Call:\r## lm(formula = price ~ factor(wholesaler), data = data)\r## ## Coefficients:\r## (Intercept) factor(wholesaler)2 factor(wholesaler)3 ## 3043.2 -381.2 -2575.1\r\rThe multiple linear regression:\rThe multiple linear regression model can be achieved by using the following betas, so that we can predict the price of the diamond ring taking in consideration the all the variables either metric or non metric.\n## ## Call:\r## lm(formula = price ~ carat + colour + clarity + cut + certification + ## polish + symmetry + factor(wholesaler), data = data)\r## ## Coefficients:\r## (Intercept) carat colourE ## 788.88 1885.30 -166.61 ## colourF colourG colourH ## -247.15 -229.60 -299.84 ## colourI colourJ colourK ## -337.63 -434.39 -696.43 ## colourL clarityI2 claritySI1 ## -895.44 -590.10 652.28 ## claritySI2 claritySI3 clarityVS1 ## 560.51 290.46 743.08 ## clarityVS2 clarityVVS1 clarityVVS2 ## 689.50 1015.21 760.50 ## cutG cutI cutV ## 48.82 84.51 78.45 ## cutX certificationDOW certificationEGL ## 93.37 -271.24 -307.29 ## certificationGIA certificationIGI polishG ## 12.16 -118.27 66.57 ## polishI polishv polishV ## 247.70 136.45 78.40 ## polishX symmetryG symmetryI ## 84.85 133.37 NA ## symmetryV symmetryX factor(wholesaler)2 ## 151.49 137.40 112.07 ## factor(wholesaler)3 ## -1460.67\rFrom the above analysis, we can see that the price of the diamond ring depends mainly on the carat mass and the relationship between these two variables are strong and positive so that as the carat mass increase as the price of the diamond ring increase and vice versa.\nTo evaluate the professor’s ring that he is interested on buying it we can use the multiple regression model mentioned above that result in estimated price of :\n\\[\\widehat{Price} = 788.88 + 1885.30\\times 0.9(carat) -434.39 (colorJ)+560.51 (ClaritySL2)+78.45 (Cut V)+ 12.16(Certification GIA)+ 66.57(Polish G) + 151.49(symmetry V)\\]\r\\[\\widehat{Price} = $ 2920.44 \\]\nThe estimated price based on our model is less than the price of the ring with around 180 dollars.\n\r\r","date":1625875200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625932461,"objectID":"0fa394e422558b28fb2389f0ea15b3ad","permalink":"/post/assignment-a01/","publishdate":"2021-07-10T00:00:00Z","relpermalink":"/post/assignment-a01/","section":"post","summary":"The diamond ring data\rIn this blog we used the diamond ring data. Getting a glimpse into the data we can see that it is composed of 440 rows where each row represents a diamond ring and for each ring the diamond characteristics are provided that are Color, Cut, Carat Weight, Clarity , Polish, Symmetry, and certification in addition to the price.","tags":[],"title":"Assignment A01  ","type":"post"},{"authors":null,"categories":["Basics","ggplot2","shiny","Tideverse","tidymodels"],"content":"\r\rThe dataset\rThe used data set in this project composed of a record of 7 common different fish species in fish market sales. For each fish participated on this record, a certain measurements were taken, which are the fish species, the weight in Gram g, vertical length in cm, diagonal length in cm, cross length in cm, height in cm and diagonal width in cm.\nThe data was obtained from Kaggle  website, to access the source of the data use the link below\nLink to the dataset\n\rExplanatory data analysis\rIn this part we are going to create a data profiling report, in this report we can get an overview of the shape and structure of our dataset by summarizing their main characters and use statistical graphics and other data visualization methods.\n\rBased on the data profiling report, specifically in the Univariate Distribution Histogram part , We can see that there are some fishes in our data set with zero weights, we need to remove the rows where the weight equal to zero.\nTo access the Data Profiling Report in a new window please press the link below\rLink to open the data profiling report in a new window\nIn this part we can interactively perform further exploration to our data set.\n\rTo open the Shiny app in a new window please press the link below\rLink to open the Shiny app in a new window\n\rFitting a model to predict the fish height based on it’s cross length\rLinear Regression Model\r\rTo create a liner regression model to predict the height of a fish given its cross length, we need to determine the value of \\(\\beta_0\\) that is the population parameter for the intercept and the value of \\(\\beta_1\\) that is the population parameter for the slope as shown in the following model:\r\r\\[\\widehat{Height}_{i} = \\beta_0 + \\beta_1 \\times Cross Length_{i}\\]\nAfter determining the values of both \\(\\beta_0\\) and \\(\\beta_1\\), the updated linear regression model with the values of \\(\\beta_0\\) and \\(\\beta_1\\) is shown as follow:\n\\[\\widehat{Height}_{i} = 0.87 + 0.26 \\times Cross Length_{i}\\]\nFrom the above model we can see that :\n\rSlope: For each additional cm the fish is longer, the height is expected to be higher, on average, by 0.26 cm.\rIntercept: fish that is 8.8 cm tall (minimum cross length value among the dataset) is expected to be 3.2 cm high, on average.\r\r\rThe data is shown in the following plot:\r\rThe data and the least square line are shown in the following plot:\r\rThe data, the least square line and the residuales are shown in the following plot:\r\r\r","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624233600,"objectID":"62487835d8b2a044e1fad80bf2440cea","permalink":"/post/the-project/","publishdate":"2021-06-21T00:00:00Z","relpermalink":"/post/the-project/","section":"post","summary":"The dataset\rThe used data set in this project composed of a record of 7 common different fish species in fish market sales. For each fish participated on this record, a certain measurements were taken, which are the fish species, the weight in Gram g, vertical length in cm, diagonal length in cm, cross length in cm, height in cm and diagonal width in cm.","tags":[],"title":"Project ","type":"post"},{"authors":null,"categories":["shiny","ggplot2","Tideverse","Basics"],"content":"\r\rThe dataset\rThe data used in this blog was obtained from The World Bank Data, to access the source of the data use the link below\nLink to the data set\n\rInitial Analysis Questions\r1- which country in North America with the highest population in 2019?\n2- which country in North America has the highest population of elderly in 2019?\n3- which country in North America has the highest birth rate in 2019?\n4- which country in North America has the highest mortality rate in 2019?\n5- How does the population in Canada change over time?\n6- The difference between age distribution in Canada in 2000 and that in 2019?\n7- Is the modern technology that is present in all life aspects including healthcare affect the human lifespan in Canada?\n8- Is the modern lifestyle in Canada affect the adolescent mind regarding giving birth in this phase of their life?\n\rExplanatory data analysis\rData profiling report, in this report we can get an overview of the shape and structure of our dataset by summarizing their main characters and use statistical graphics and other data visualization methods.\r\rTo access the Data Profiling Report in a new window please press the link below\rLink to open the data profiling report in a new window\n\rin this part we can analyze the dataset by visually explore each indicator in each country\r\rTo open the Shiny app in a new window please press the link below\rLink to open the Shiny app in a new window\n\r\rQuestion 1: what’s the country with the highest population in North America in 2019?\rFrom Figure(1), we can see that in 2019 the United States of America has the highest population of 328239523 among the countries of North America, then Mexico that has a population of 127575529 then Canada that has population of 37593384.\n\rQuestion 2: which country in North America has the highest population of elderly in 2019?\rfrom Figure(2) we can see that the elderly population among the North America countries follows the same ranking of the total population we did in question 1 , so that in 2019 the United States of America has the highest elderly population of 53206334, then Mexico that has elderly population of 9461844 then Canada that has elderly population of 6634442.\nAlso from Figure (2), we can see the portion of the elderly population among the whole population in each country.\n\rQuestion 3: which country in North America has the highest birth rate in 2019?\rTo answer this question we will use the indicator of fertility rate that is defined as the total births per woman in the country as we can see in Figure(3), using the fertility rate as an indicator to the birth rate, we can see that Haiti has the highest fertility rate in 2019 among the North America countries of value 2.89. Also we can see that the fertility rate in Canada was 1.47 in 2019.\n\rQuestion 4: which country in North America has the highest mortality rate in 2019?\rTo answer this question we will use the indicator of Death Rate Crude Per 1000 People in the country, as we can see in Figure(4), using the Death Rate Crude Per 1000 People as an indicator to the mortality rate, we can see that Grenada has the highest mortality rate in 2019 among the North America countries of value 9.58. Also we can see that in 2019 the death rate in Canada was 7.60 and the lowest death rate was in Honduras of value 4.45.\n\rQuestion 5: How does the population in Canada change over time?\rFrom Figure (5) shown above , we can see that the population in Canada increases every year so that starting from year 1960 the population was 17909009 and it keeps increasing linearly reaching to population of 30685730 in 2000 and population of 37593384 in 2019.\n\rQuestion 6: the difference between age distribution in Canada population in 2000 and that in 2019?\rFrom Figure (6) shown above , we can see that the age distribution in Canada in 1960 that the zero to 9 age group was contributed more in the population than in 2019 and also in 2019 the plus eighty age group has a larger contribution in the population compared to 1960 where the contribution is almost zero and from 1960 to 2019 the contribution of the two age groups forty to forty nine and fifty to fifty nine increases over time.\n\rQuestion 7: Is the modern technology that is present in all life aspects including healthcare affect the human lifespan in Canada?\rfrom Figure(7), we can see that the modern technology present in all life aspects including healthcare affect the human lifespan in Canada, so that human tends to live longer as the population of the age group of eighty and above increases with time and technology.\nWe can have a closer look by plotting the population of group age 80 and above from 1960 to 2019, as we can see in Figure (8) that there is an increase of this age group in Canada’s population since 1960 and since 2000 this increase became a little higher.\n\rQuestion 8: Is the modern lifestyle in Canada affect the adolescent mind regarding giving birth in this phase of their life?\rFrom Figure (9), we can see that the Adolescent fertility rate in Canada has tremendously decreases over time starting at 1960 the adolescent fertility rate was 55.63 reaching to adolescent fertility rate of 7.68 in 2019. it seems that the modern life style in Canada has the effect to decrease the adolescent women fertility rate.\n\rSummary\rThe modern technology has helped to make human live longer life, although in the other side the modern life affected the adolescent fertility rate in Canada.in 2019 United states of America has the highest population in the North America and comparing it Canada the difference is huge. In 2019 among countries of North America the mortality rate was the least in Honduras while the birth rate was the highest in Haiti.\n\r","date":1623801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623801600,"objectID":"ba76cd365217aa39f56ad818240afe74","permalink":"/post/a05-exploratory-data-analysis/","publishdate":"2021-06-16T00:00:00Z","relpermalink":"/post/a05-exploratory-data-analysis/","section":"post","summary":"The dataset\rThe data used in this blog was obtained from The World Bank Data, to access the source of the data use the link below\nLink to the data set","tags":[],"title":"A05  Exploratory Data Analysis","type":"post"},{"authors":null,"categories":["tidymodels"],"content":"\r\r\rImport Libraries\r\rImport Data: Paris Paintings\rpp \u0026lt;- read_csv(\u0026quot;paris-paintings.csv\u0026quot;, na = c(\u0026quot;n/a\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;))\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## .default = col_double(),\r## name = col_character(),\r## sale = col_character(),\r## lot = col_character(),\r## dealer = col_character(),\r## origin_author = col_character(),\r## origin_cat = col_character(),\r## school_pntg = col_character(),\r## price = col_number(),\r## subject = col_character(),\r## authorstandard = col_character(),\r## authorstyle = col_character(),\r## author = col_character(),\r## winningbidder = col_character(),\r## winningbiddertype = col_character(),\r## endbuyer = col_character(),\r## type_intermed = col_character(),\r## Shape = col_character(),\r## material = col_character(),\r## mat = col_character(),\r## materialCat = col_character()\r## )\r## i Use `spec()` for the full column specifications.\r\rGoal: Predict height from width\r\\[\\widehat{height}_{i} = \\beta_0 + \\beta_1 \\times width_{i}\\]\n## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\rStep 1: Specify model\rlinear_reg()\r## Linear Regression Model Specification (regression)\r\rStep 2: Set model fitting engine\rlinear_reg() %\u0026gt;%\rset_engine(\u0026quot;lm\u0026quot;) # lm: linear model\r## Linear Regression Model Specification (regression)\r## ## Computational engine: lm\r\rStep 3: Fit model \u0026amp; estimate parameters\r… using formula syntax\nlinear_reg() %\u0026gt;%\rset_engine(\u0026quot;lm\u0026quot;) %\u0026gt;%\rfit(Height_in ~ Width_in, data = pp)\r## parsnip model object\r## ## Fit time: 0ms ## ## Call:\r## stats::lm(formula = Height_in ~ Width_in, data = data)\r## ## Coefficients:\r## (Intercept) Width_in ## 3.6214 0.7808\r\rA closer look at model output\r## parsnip model object\r## ## Fit time: 0ms ## ## Call:\r## stats::lm(formula = Height_in ~ Width_in, data = data)\r## ## Coefficients:\r## (Intercept) Width_in ## 3.6214 0.7808\r.large[\r\\[\\widehat{height}_{i} = 3.6214 + 0.7808 \\times width_{i}\\]]\n\rA tidy look at model output\rlinear_reg() %\u0026gt;%\rset_engine(\u0026quot;lm\u0026quot;) %\u0026gt;%\rfit(Height_in ~ Width_in, data = pp) %\u0026gt;%\rtidy()\r## # A tibble: 2 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 3.62 0.254 14.3 8.82e-45\r## 2 Width_in 0.781 0.00950 82.1 0\r.large[\r\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]]\n\rSlope and intercept\r.large[\r\\[\\widehat{height}_{i} = 3.62 + 0.781 \\times width_{i}\\]]\n–\n\rSlope: For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.\r\r\r\r\r- Intercept: Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)\r\r\r\r\rCorrelation does not imply causation\rRemember this when interpreting model coefficients\n.footnote[\rSource: XKCD, Cell phones]\nclass: middle\n\rParameter estimation\rLinear model with a single predictor\r\rWe’re interested in \\(\\beta_0\\) (population parameter for the intercept) and \\(\\beta_1\\) (population parameter for the slope) in the following model:\r\r\\[\\hat{y}_{i} = \\beta_0 + \\beta_1~x_{i}\\]\n\r\r\r- Tough luck, you can’t have them…\r\r\r\r- So we use sample statistics to estimate them:\r\r\\[\\hat{y}_{i} = b_0 + b_1~x_{i}\\]\r\r\r\r\rLeast squares regression\r\rThe regression line minimizes the sum of squared residuals.\r\r\r\r\r- If \\(e_i = y_i - \\hat{y}_i\\), then, the regression line minimizes\r\\(\\sum_{i = 1}^n e_i^2\\).\r\r\r\r\rVisualizing residuals\r\rVisualizing residuals (cont.)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\rVisualizing residuals (cont.)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\rProperties of least squares regression\r\rThe regression line goes through the center of mass point, the coordinates corresponding to average \\(x\\) and average \\(y\\), \\((\\bar{x}, \\bar{y})\\):\r\r\\[\\bar{y} = b_0 + b_1 \\bar{x} ~ \\rightarrow ~ b_0 = \\bar{y} - b_1 \\bar{x}\\]\n\r\r\r- The slope has the same sign as the correlation coefficient: \\(b_1 = r \\frac{s_y}{s_x}\\)\r\r\r\r- The sum of the residuals is zero: \\(\\sum_{i = 1}^n e_i = 0\\)\r\r\r\r\rThe residuals and \\(x\\) values are uncorrelated\r\rclass: middle\n\r\rModels with categorical explanatory variables\rCategorical predictor with 2 levels\r.pull-left-narrow[\r.small[\n## # A tibble: 3,393 x 3\r## name Height_in landsALL\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 L1764-2 37 0\r## 2 L1764-3 18 0\r## 3 L1764-4 13 1\r## 4 L1764-5a 14 1\r## 5 L1764-5b 14 1\r## 6 L1764-6 7 0\r## 7 L1764-7a 6 0\r## 8 L1764-7b 6 0\r## 9 L1764-8 15 0\r## 10 L1764-9a 9 0\r## 11 L1764-9b 9 0\r## 12 L1764-10a 16 1\r## 13 L1764-10b 16 1\r## 14 L1764-10c 16 1\r## 15 L1764-11 20 0\r## 16 L1764-12a 14 1\r## 17 L1764-12b 14 1\r## 18 L1764-13a 15 1\r## 19 L1764-13b 15 1\r## 20 L1764-14 37 0\r## # ... with 3,373 more rows\r]\r]\r.pull-right-wide[\r- landsALL = 0: No landscape features\r- landsALL = 1: Some landscape features]\n\rHeight \u0026amp; landscape features\rlinear_reg() %\u0026gt;%\rset_engine(\u0026quot;lm\u0026quot;) %\u0026gt;%\rfit(Height_in ~ factor(landsALL), data = pp) %\u0026gt;%\rtidy()\r## # A tibble: 2 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 22.7 0.328 69.1 0 ## 2 factor(landsALL)1 -5.65 0.532 -10.6 7.97e-26\r\rHeight \u0026amp; landscape features\r\\[\\widehat{Height_{in}} = 22.7 - 5.645~landsALL\\]\n\rSlope: Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features\r\rCompares baseline level (landsALL = 0) to the other level (landsALL = 1)\r\rIntercept: Paintings that don’t have landscape features are expected, on average, to be 22.7 inches tall\r\r\rRelationship between height and school\rlinear_reg() %\u0026gt;%\rset_engine(\u0026quot;lm\u0026quot;) %\u0026gt;%\rfit(Height_in ~ school_pntg, data = pp) %\u0026gt;%\rtidy()\r## # A tibble: 7 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 14.0 10.0 1.40 0.162 ## 2 school_pntgD/FL 2.33 10.0 0.232 0.816 ## 3 school_pntgF 10.2 10.0 1.02 0.309 ## 4 school_pntgG 1.65 11.9 0.139 0.889 ## 5 school_pntgI 10.3 10.0 1.02 0.306 ## 6 school_pntgS 30.4 11.4 2.68 0.00744\r## 7 school_pntgX 2.87 10.3 0.279 0.780\r\rDummy variables\r## # A tibble: 7 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 14.0 10.0 1.40 0.162 ## 2 school_pntgD/FL 2.33 10.0 0.232 0.816 ## 3 school_pntgF 10.2 10.0 1.02 0.309 ## 4 school_pntgG 1.65 11.9 0.139 0.889 ## 5 school_pntgI 10.3 10.0 1.02 0.306 ## 6 school_pntgS 30.4 11.4 2.68 0.00744\r## 7 school_pntgX 2.87 10.3 0.279 0.780\r\rWhen the categorical explanatory variable has many levels, they’re encoded to dummy variables\rEach coefficient describes the expected difference between heights in that particular school compared to the baseline level\r\r\rCategorical predictor with 3+ levels\r.pull-left-wide[\r\r\rschool_pntg\r\rD_FL\r\rF\r\rG\r\rI\r\rS\r\rX\r\r\r\r\r\rA\r\r0\r\r0\r\r0\r\r0\r\r0\r\r0\r\r\r\rD/FL\r\r1\r\r0\r\r0\r\r0\r\r0\r\r0\r\r\r\rF\r\r0\r\r1\r\r0\r\r0\r\r0\r\r0\r\r\r\rG\r\r0\r\r0\r\r1\r\r0\r\r0\r\r0\r\r\r\rI\r\r0\r\r0\r\r0\r\r1\r\r0\r\r0\r\r\r\rS\r\r0\r\r0\r\r0\r\r0\r\r1\r\r0\r\r\r\rX\r\r0\r\r0\r\r0\r\r0\r\r0\r\r1\r\r\r\r\r]\r.pull-right-narrow[\r.small[\n## # A tibble: 3,393 x 3\r## name Height_in school_pntg\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 L1764-2 37 F ## 2 L1764-3 18 I ## 3 L1764-4 13 D/FL ## 4 L1764-5a 14 F ## 5 L1764-5b 14 F ## 6 L1764-6 7 I ## 7 L1764-7a 6 F ## 8 L1764-7b 6 F ## 9 L1764-8 15 I ## 10 L1764-9a 9 D/FL ## 11 L1764-9b 9 D/FL ## 12 L1764-10a 16 X ## 13 L1764-10b 16 X ## 14 L1764-10c 16 X ## 15 L1764-11 20 D/FL ## 16 L1764-12a 14 D/FL ## 17 L1764-12b 14 D/FL ## 18 L1764-13a 15 D/FL ## 19 L1764-13b 15 D/FL ## 20 L1764-14 37 F ## # ... with 3,373 more rows\r]\r]\n\rRelationship between height and school\r.small[\n## # A tibble: 7 x 5\r## term estimate std.error statistic p.value\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 14.0 10.0 1.40 0.162 ## 2 school_pntgD/FL 2.33 10.0 0.232 0.816 ## 3 school_pntgF 10.2 10.0 1.02 0.309 ## 4 school_pntgG 1.65 11.9 0.139 0.889 ## 5 school_pntgI 10.3 10.0 1.02 0.306 ## 6 school_pntgS 30.4 11.4 2.68 0.00744\r## 7 school_pntgX 2.87 10.3 0.279 0.780\r\rAustrian school (A) paintings are expected, on average, to be 14 inches tall.\rDutch/Flemish school (D/FL) paintings are expected, on average, to be 2.33 inches taller than Austrian school paintings.\rFrench school (F) paintings are expected, on average, to be 10.2 inches taller than Austrian school paintings.\rGerman school (G) paintings are expected, on average, to be 1.65 inches taller than Austrian school paintings.\rItalian school (I) paintings are expected, on average, to be 10.3 inches taller than Austrian school paintings.\rSpanish school (S) paintings are expected, on average, to be 30.4 inches taller than Austrian school paintings.\rPaintings whose school is unknown (X) are expected, on average, to be 2.87 inches taller than Austrian school paintings.\r]\r\r\r\r","date":1623715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623715200,"objectID":"40b1b830c2a42f8fe6eb996f85fded68","permalink":"/post/fitting-and-interpreting-models/","publishdate":"2021-06-15T00:00:00Z","relpermalink":"/post/fitting-and-interpreting-models/","section":"post","summary":"Import Libraries\r\rImport Data: Paris Paintings\rpp \u0026lt;- read_csv(\u0026quot;paris-paintings.csv\u0026quot;, na = c(\u0026quot;n/a\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;))\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## .default = col_double(),\r## name = col_character(),\r## sale = col_character(),\r## lot = col_character(),\r## dealer = col_character(),\r## origin_author = col_character(),\r## origin_cat = col_character(),\r## school_pntg = col_character(),\r## price = col_number(),\r## subject = col_character(),\r## authorstandard = col_character(),\r## authorstyle = col_character(),\r## author = col_character(),\r## winningbidder = col_character(),\r## winningbiddertype = col_character(),\r## endbuyer = col_character(),\r## type_intermed = col_character(),\r## Shape = col_character(),\r## material = col_character(),\r## mat = col_character(),\r## materialCat = col_character()\r## )\r## i Use `spec()` for the full column specifications.","tags":[],"title":"Fitting and Interpreting models ","type":"post"},{"authors":null,"categories":["shiny","ggplot2","Basics"],"content":"\r\rProvisional count of deaths involving coronavirus disease 2019 (COVID-19) by United States county\rThe data used in this post is obtained from Data.Gov and it shows the provisional count of deaths involving coronavirus disease 2019 (COVID-19) by United States county.\nThe dataset is intended for public access and use.\n\r\r","date":1623369600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623369600,"objectID":"5b772d4142312e32e610d4234cf914d0","permalink":"/post/a04-shiny-app/","publishdate":"2021-06-11T00:00:00Z","relpermalink":"/post/a04-shiny-app/","section":"post","summary":"Provisional count of deaths involving coronavirus disease 2019 (COVID-19) by United States county\rThe data used in this post is obtained from Data.Gov and it shows the provisional count of deaths involving coronavirus disease 2019 (COVID-19) by United States county.","tags":[],"title":"A04 Shiny App","type":"post"},{"authors":null,"categories":["Tideverse"],"content":"\r\rReading the lego_sales.csv file\rlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v ggplot2 3.3.3 v purrr 0.3.4\r## v tibble 3.1.1 v dplyr 1.0.6\r## v tidyr 1.1.3 v stringr 1.4.0\r## v readr 1.4.0 v forcats 0.5.1\r## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rlego_sales \u0026lt;- read_csv(\u0026quot;lego_sales.csv\u0026quot;)\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## first_name = col_character(),\r## last_name = col_character(),\r## age = col_double(),\r## phone_number = col_character(),\r## set_id = col_double(),\r## number = col_character(),\r## theme = col_character(),\r## subtheme = col_character(),\r## year = col_double(),\r## name = col_character(),\r## pieces = col_double(),\r## us_price = col_double(),\r## image_url = col_character(),\r## quantity = col_double()\r## )\rglimpse(lego_sales)\r## Rows: 620\r## Columns: 14\r## $ first_name \u0026lt;chr\u0026gt; \u0026quot;Kimberly\u0026quot;, \u0026quot;Neel\u0026quot;, \u0026quot;Neel\u0026quot;, \u0026quot;Chelsea\u0026quot;, \u0026quot;Chelsea\u0026quot;, \u0026quot;Chelse~\r## $ last_name \u0026lt;chr\u0026gt; \u0026quot;Beckstead\u0026quot;, \u0026quot;Garvin\u0026quot;, \u0026quot;Garvin\u0026quot;, \u0026quot;Bouchard\u0026quot;, \u0026quot;Bouchard\u0026quot;, ~\r## $ age \u0026lt;dbl\u0026gt; 24, 35, 35, 41, 41, 41, 19, 19, 37, 37, 19, 19, 20, 36, 3~\r## $ phone_number \u0026lt;chr\u0026gt; \u0026quot;216-555-2549\u0026quot;, \u0026quot;819-555-3189\u0026quot;, \u0026quot;819-555-3189\u0026quot;, NA, NA, N~\r## $ set_id \u0026lt;dbl\u0026gt; 24701, 25626, 24665, 24695, 25626, 24721, 24797, 24701, 2~\r## $ number \u0026lt;chr\u0026gt; \u0026quot;76062\u0026quot;, \u0026quot;70595\u0026quot;, \u0026quot;21031\u0026quot;, \u0026quot;31048\u0026quot;, \u0026quot;70595\u0026quot;, \u0026quot;10831\u0026quot;, \u0026quot;75~\r## $ theme \u0026lt;chr\u0026gt; \u0026quot;DC Comics Super Heroes\u0026quot;, \u0026quot;Ninjago\u0026quot;, \u0026quot;Architecture\u0026quot;, \u0026quot;Cre~\r## $ subtheme \u0026lt;chr\u0026gt; \u0026quot;Mighty Micros\u0026quot;, \u0026quot;Rise of the Villains\u0026quot;, NA, NA, \u0026quot;Rise of~\r## $ year \u0026lt;dbl\u0026gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 201~\r## $ name \u0026lt;chr\u0026gt; \u0026quot;Robin vs. Bane\u0026quot;, \u0026quot;Ultra Stealth Raider\u0026quot;, \u0026quot;Burj Khalifa\u0026quot;,~\r## $ pieces \u0026lt;dbl\u0026gt; 77, 1093, 333, 368, 1093, 19, 233, 77, 108, NA, 13, 15, 6~\r## $ us_price \u0026lt;dbl\u0026gt; 9.99, 119.99, 39.99, 29.99, 119.99, 9.99, 24.99, 9.99, 9.~\r## $ image_url \u0026lt;chr\u0026gt; \u0026quot;http://images.brickset.com/sets/images/76062-1.jpg\u0026quot;, \u0026quot;ht~\r## $ quantity \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, ~\rlego_sales\r## # A tibble: 620 x 14\r## first_name last_name age phone_number set_id number theme subtheme year\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Kimberly Beckstead 24 216-555-2549 24701 76062 DC Co~ Mighty M~ 2018\r## 2 Neel Garvin 35 819-555-3189 25626 70595 Ninja~ Rise of ~ 2018\r## 3 Neel Garvin 35 819-555-3189 24665 21031 Archi~ \u0026lt;NA\u0026gt; 2018\r## 4 Chelsea Bouchard 41 \u0026lt;NA\u0026gt; 24695 31048 Creat~ \u0026lt;NA\u0026gt; 2018\r## 5 Chelsea Bouchard 41 \u0026lt;NA\u0026gt; 25626 70595 Ninja~ Rise of ~ 2018\r## 6 Chelsea Bouchard 41 \u0026lt;NA\u0026gt; 24721 10831 Duplo \u0026lt;NA\u0026gt; 2018\r## 7 Bryanna Welsh 19 \u0026lt;NA\u0026gt; 24797 75138 Star ~ Episode V 2018\r## 8 Bryanna Welsh 19 \u0026lt;NA\u0026gt; 24701 76062 DC Co~ Mighty M~ 2018\r## 9 Caleb Garcia-Wi~ 37 907-555-9236 24730 41115 Frien~ \u0026lt;NA\u0026gt; 2018\r## 10 Caleb Garcia-Wi~ 37 907-555-9236 25611 21127 Minec~ Minifig-~ 2018\r## # ... with 610 more rows, and 5 more variables: name \u0026lt;chr\u0026gt;, pieces \u0026lt;dbl\u0026gt;,\r## # us_price \u0026lt;dbl\u0026gt;, image_url \u0026lt;chr\u0026gt;, quantity \u0026lt;dbl\u0026gt;\r\rQ1.What are the three most common first names of customers?\rThe answer:\rcommon_first_names\u0026lt;- lego_sales %\u0026gt;%\rcount(first_name,sort = TRUE) %\u0026gt;%\rhead(3)\rcat(\u0026quot;The three most common first names of customers are \u0026quot;,common_first_names$first_name,sep = \u0026quot;\\n \u0026quot;)\r## The three most common first names of customers are ## Jackson\r## Jacob\r## Joseph\r\rQ2.What are the three most common themes of lego sets purchased?\r\rThe answer:\rcommon_themes \u0026lt;-lego_sales %\u0026gt;%\rcount(theme ,sort = TRUE)%\u0026gt;%\rhead(3)\rcat(\u0026quot;The three most common themes of lego sets purchased are \u0026quot;,common_themes$theme ,sep = \u0026quot;\\n \u0026quot;)\r## The three most common themes of lego sets purchased are ## Star Wars\r## Nexo Knights\r## Gear\r\rQ3.Among the most common theme of lego sets purchased, what is the most common subtheme?\r\rThe answer:\rcommon_subtheme\u0026lt;- lego_sales %\u0026gt;%\rfilter (theme==\u0026quot;Star Wars\u0026quot;) %\u0026gt;%\rcount(subtheme,sort = TRUE )%\u0026gt;%\rhead(1)\rcat(\u0026quot;The most common subtheme among the most common theme of lego sets purchased is \u0026quot;,common_subtheme$subtheme ,sep = \u0026quot;\\n \u0026quot;)\r## The most common subtheme among the most common theme of lego sets purchased is ## The Force Awakens\r\rQ4.Create a new variable called age_group and group the ages into the following categories: “18 and under”, “19 - 25”, “26 - 35”, “36 - 50”, “51 and over”. Be sure to save the updated data set so you can use the new variable in other questions.\r\rThe answer:\r lego_sales_age_group \u0026lt;- lego_sales %\u0026gt;%\rmutate (age_group=case_when(\rbetween(age,min(age),18)~ \u0026quot;18 and under\u0026quot;,\rbetween(age,19,25)~ \u0026quot;19 - 25\u0026quot;,\rbetween(age,26,35)~ \u0026quot;26 - 35\u0026quot;,\rbetween(age,36,50)~ \u0026quot;36 - 50\u0026quot;,\rbetween(age,51,max(age))~ \u0026quot;51 and over\u0026quot;\r))\r## Displaying the new data frame with the new variable:\rlego_sales_age_group\r## # A tibble: 620 x 15\r## first_name last_name age phone_number set_id number theme subtheme year\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Kimberly Beckstead 24 216-555-2549 24701 76062 DC Co~ Mighty M~ 2018\r## 2 Neel Garvin 35 819-555-3189 25626 70595 Ninja~ Rise of ~ 2018\r## 3 Neel Garvin 35 819-555-3189 24665 21031 Archi~ \u0026lt;NA\u0026gt; 2018\r## 4 Chelsea Bouchard 41 \u0026lt;NA\u0026gt; 24695 31048 Creat~ \u0026lt;NA\u0026gt; 2018\r## 5 Chelsea Bouchard 41 \u0026lt;NA\u0026gt; 25626 70595 Ninja~ Rise of ~ 2018\r## 6 Chelsea Bouchard 41 \u0026lt;NA\u0026gt; 24721 10831 Duplo \u0026lt;NA\u0026gt; 2018\r## 7 Bryanna Welsh 19 \u0026lt;NA\u0026gt; 24797 75138 Star ~ Episode V 2018\r## 8 Bryanna Welsh 19 \u0026lt;NA\u0026gt; 24701 76062 DC Co~ Mighty M~ 2018\r## 9 Caleb Garcia-Wi~ 37 907-555-9236 24730 41115 Frien~ \u0026lt;NA\u0026gt; 2018\r## 10 Caleb Garcia-Wi~ 37 907-555-9236 25611 21127 Minec~ Minifig-~ 2018\r## # ... with 610 more rows, and 6 more variables: name \u0026lt;chr\u0026gt;, pieces \u0026lt;dbl\u0026gt;,\r## # us_price \u0026lt;dbl\u0026gt;, image_url \u0026lt;chr\u0026gt;, quantity \u0026lt;dbl\u0026gt;, age_group \u0026lt;chr\u0026gt;\r## Displaying selected variables from the new generated data frame so that the new variable age_group can be clearly seen:\rlego_sales_age_group %\u0026gt;% select(first_name,age,age_group)\r## # A tibble: 620 x 3\r## first_name age age_group\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 Kimberly 24 19 - 25 ## 2 Neel 35 26 - 35 ## 3 Neel 35 26 - 35 ## 4 Chelsea 41 36 - 50 ## 5 Chelsea 41 36 - 50 ## 6 Chelsea 41 36 - 50 ## 7 Bryanna 19 19 - 25 ## 8 Bryanna 19 19 - 25 ## 9 Caleb 37 36 - 50 ## 10 Caleb 37 36 - 50 ## # ... with 610 more rows\r\rQ5.What is the probability a randomly selected customer\r\ris in the 19 - 25 age group?\ris in the 19 - 25 age group and purchased a Duplo theme set?\ris in the 19 - 25 age group given they purchased a Duplo theme set?\r\r\rThe answer:\r# 1st part Calculating the probability a randomly selected customer is in the 19 - 25 age group\rfrom19_to25_age_group \u0026lt;- lego_sales_age_group %\u0026gt;%\rcount(age_group) %\u0026gt;%\rfilter( age_group==\u0026quot;19 - 25\u0026quot;)\rprob_of_19_25 \u0026lt;- (from19_to25_age_group$n/nrow(lego_sales))\r# 2nd part Calculating the probability a randomly selected customer is in the 19 - 25 age group and purchased a Duplo theme set\rfrom19_25_duplo_theme \u0026lt;- lego_sales_age_group %\u0026gt;%\rfilter( age_group==\u0026quot;19 - 25\u0026quot; \u0026amp; theme==\u0026quot;Duplo\u0026quot;)\rprob_from19_25_duplo_theme \u0026lt;- (nrow(from19_25_duplo_theme)/nrow(lego_sales))\r# 3rd part Calculating the probability a randomly selected customer is in the 19 - 25 age group given they purchased a Duplo theme set\rduplo_theme_customers \u0026lt;- lego_sales_age_group %\u0026gt;%\rfilter(theme==\u0026quot;Duplo\u0026quot;)\rprob_of_19_given_duplo_theme \u0026lt;- (nrow(from19_25_duplo_theme)/nrow(duplo_theme_customers))\rcat(\u0026quot;The probability of a randomly selected customer is in the 19 - 25 age groupis \u0026quot;,prob_of_19_25 ,sep = \u0026quot;\\n \u0026quot;)\r## The probability of a randomly selected customer is in the 19 - 25 age groupis ## 0.2080645\rcat(\u0026quot;The probability of a randomly selected customer is in the 19 - 25 age group and purchased a Duplo theme is \u0026quot;,prob_from19_25_duplo_theme ,sep = \u0026quot;\\n \u0026quot;)\r## The probability of a randomly selected customer is in the 19 - 25 age group and purchased a Duplo theme is ## 0.01451613\rcat(\u0026quot;The probability of a randomly selected customer is in the 19 - 25 age group given they purchased a Duplo theme is \u0026quot;,prob_of_19_given_duplo_theme ,sep = \u0026quot;\\n \u0026quot;)\r## The probability of a randomly selected customer is in the 19 - 25 age group given they purchased a Duplo theme is ## 0.2571429\r\rQ6.Which age group has purchased the largest number of lego sets? How many did they purchase?\r\rThe answer:\rlego_sales_by_age_group \u0026lt;- lego_sales_age_group %\u0026gt;% group_by(age_group) %\u0026gt;% summarise(total_quantity=sum(quantity)) %\u0026gt;% arrange(desc(total_quantity))%\u0026gt;%\rhead(1)\rcat(\u0026quot;The age group that has purchased the largest number of lego sets is\u0026quot;,lego_sales_by_age_group$age_group,sep=\u0026quot;\\n\u0026quot;)\r## The age group that has purchased the largest number of lego sets is\r## 36 - 50\r cat(\u0026quot;They purchased a total number of lego sets equal to \u0026quot;,lego_sales_by_age_group$total_quantity,sep=\u0026quot;\\n\u0026quot;)\r## They purchased a total number of lego sets equal to ## 313\r\rQ7.Which age group has spent the most money on legos? How much did they spend?\r\rThe answer:\rmost_spending_by_age_group \u0026lt;- lego_sales_age_group %\u0026gt;% mutate(total_price=us_price*quantity) %\u0026gt;%\rgroup_by(age_group) %\u0026gt;% summarise(total_spending=sum(total_price)) %\u0026gt;% arrange(desc(total_spending))%\u0026gt;%\rhead(1)\rcat(\u0026quot;The age group that has spent the most money on Lego is \u0026quot;,most_spending_by_age_group$age_group ,sep = \u0026quot;\\n \u0026quot;)\r## The age group that has spent the most money on Lego is ## 36 - 50\rcat(\u0026quot;They spend amount of US dollars equal to \u0026quot;,most_spending_by_age_group$total_spending ,sep = \u0026quot;\\n \u0026quot;)\r## They spend amount of US dollars equal to ## 9532.87\r\rQ8.Come up with a question you want to answer using this data, and write it down. Then, create a data visualization that answers the question, and briefly explain how your visualization answers the question.\r\rThe answer:\r\rThe question could be how much revenue Lego makes from each theme and the contribution of each age group in each theme revenue?\r\rThe answer of this question can be obtained by first multiplying the quantity of sets purchased by every customer by the price per each set, then group the data frame by the theme so that the revenue from each theme can be obtained, plotting the theme on the x axis and the revenue on the y axis of the column plot would allow us easily to determine the revenue of each theme and the age groups contributed in each theme revenue.\nFrom the plot we can see that Star Wars theme makes the most revenue of more than 4000 Us dollars where 26-35 and 36-50 age groups have more contributions on this revenue than the other age groups, next is the Ninjago theme of revenue more than 2000 US dollar where we can see that 18 and under age group doesn’t contribute in this revenue much and city theme of revenue more than 2000 US dollar where we can see that this theme doesn’t attract the 51 and over age group and the themes that makes the little revenues are Classic, Collectable Minifigures and seasonal themes where each theme attract only a certain age group that is 36-50, 51 and over and 26-35 respectively.\nThe Duplo, Elvis, Friends, Gear and Minicraft themes make revenue of a little more than 1000 us dollars where the contribution of the 18 and under age group is very small, the revenue from the remaining themes is less than 1000 us dollars and the age groups contribution vary from one theme to another.\nlego_sales_by_theme \u0026lt;- lego_sales_age_group %\u0026gt;% mutate(total_price=us_price*quantity) %\u0026gt;%\rgroup_by(theme) ggplot(lego_sales_by_theme,aes(x=theme,y=total_price,fill=age_group))+\rgeom_col()+\rtheme(axis.text.x=element_text(color = \u0026quot;black\u0026quot;, size=9, angle=90, vjust=.5, hjust=0.7))\r\rQ9.Add one element to the plot from the previous exercise to change the look of the plot without changing the underlying data. For example, you can change the theme, background color, add annotations, etc. State the change you’re making and display the updated visualization. We encourage you to be creative!\r\rThe answer:\r\rTitle for the plot has been added\rTitles for the 2 axes of the plot have been added\rBlack and white theme is used\n\rThe background color has been changed\rviridis used to update the color map\r\rggplot(lego_sales_by_theme,aes(x=theme,y=total_price,fill=age_group))+\rgeom_col() +\rscale_fill_viridis_d() +\rtheme_bw()+\rtheme(axis.text.x=element_text(color = \u0026quot;red\u0026quot;, size=9, angle=90, vjust=.5, hjust=0.7)) +\rtheme(plot.background = element_rect(fill = \u0026quot;#BFD5E3\u0026quot;))+\rlabs(\rx = \u0026quot;Theme\u0026quot;, y = \u0026quot;The revenue in US Dollars \u0026quot;, title = \u0026quot;The revenue associated with each theme and the contribution \\n of each age group in this revenue\u0026quot;, fill = \u0026quot;Age Groups\u0026quot; )\r\r\r","date":1622678400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622678400,"objectID":"39a865e59a560dbb27dd85b3dc3c7cbe","permalink":"/post/a03/","publishdate":"2021-06-03T00:00:00Z","relpermalink":"/post/a03/","section":"post","summary":"Reading the lego_sales.csv file\rlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v ggplot2 3.3.3 v purrr 0.3.4\r## v tibble 3.1.1 v dplyr 1.0.6\r## v tidyr 1.","tags":[],"title":"A03","type":"post"},{"authors":null,"categories":["Tideverse"],"content":"\r\rImport Nobel.csv file\rlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v ggplot2 3.3.3 v purrr 0.3.4\r## v tibble 3.1.1 v dplyr 1.0.6\r## v tidyr 1.1.3 v stringr 1.4.0\r## v readr 1.4.0 v forcats 0.5.1\r## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rnobel \u0026lt;- read_csv(\u0026quot;nobel.csv\u0026quot;)\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## .default = col_character(),\r## id = col_double(),\r## year = col_double(),\r## born_date = col_date(format = \u0026quot;\u0026quot;),\r## died_date = col_date(format = \u0026quot;\u0026quot;),\r## share = col_double()\r## )\r## i Use `spec()` for the full column specifications.\rhead(nobel)\r## # A tibble: 6 x 26\r## id firstname surname year category affiliation city country born_date ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; ## 1 1 Wilhelm C~ Röntgen 1901 Physics Munich Unive~ Muni~ Germany 1845-03-27\r## 2 2 Hendrik A. Lorentz 1902 Physics Leiden Unive~ Leid~ Nether~ 1853-07-18\r## 3 3 Pieter Zeeman 1902 Physics Amsterdam Un~ Amst~ Nether~ 1865-05-25\r## 4 4 Henri Becque~ 1903 Physics École Polyte~ Paris France 1852-12-15\r## 5 5 Pierre Curie 1903 Physics École munici~ Paris France 1859-05-15\r## 6 6 Marie Curie 1903 Physics \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; 1867-11-07\r## # ... with 17 more variables: died_date \u0026lt;date\u0026gt;, gender \u0026lt;chr\u0026gt;, born_city \u0026lt;chr\u0026gt;,\r## # born_country \u0026lt;chr\u0026gt;, born_country_code \u0026lt;chr\u0026gt;, died_city \u0026lt;chr\u0026gt;,\r## # died_country \u0026lt;chr\u0026gt;, died_country_code \u0026lt;chr\u0026gt;, overall_motivation \u0026lt;chr\u0026gt;,\r## # share \u0026lt;dbl\u0026gt;, motivation \u0026lt;chr\u0026gt;, born_country_original \u0026lt;chr\u0026gt;,\r## # born_city_original \u0026lt;chr\u0026gt;, died_country_original \u0026lt;chr\u0026gt;,\r## # died_city_original \u0026lt;chr\u0026gt;, city_original \u0026lt;chr\u0026gt;, country_original \u0026lt;chr\u0026gt;\r\rWrite a csv file\rdf \u0026lt;- tribble(\r~x, ~y,\r1, \u0026quot;a\u0026quot;,\r2, \u0026quot;b\u0026quot;,\r3, \u0026quot;c\u0026quot;\r)\rwrite_csv(df, file = \u0026quot;df.csv\u0026quot;)\r\rDealing with bad variable names\r#edibnb_badnames \u0026lt;- read_csv(\u0026quot;edibnb-badnames.csv\u0026quot;)\r#names(edibnb_badnames)\redibnb_col_names \u0026lt;- read_csv(\u0026quot;edibnb-badnames.csv\u0026quot;,\rcol_names = c(\u0026quot;id\u0026quot;, \u0026quot;price\u0026quot;, \u0026quot;neighbourhood\u0026quot;, \u0026quot;accommodates\u0026quot;,\r\u0026quot;bathroom\u0026quot;, \u0026quot;bedroom\u0026quot;, \u0026quot;bed\u0026quot;, \u0026quot;review_scores_rating\u0026quot;, \u0026quot;n_reviews\u0026quot;, \u0026quot;url\u0026quot;))\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## id = col_character(),\r## price = col_character(),\r## neighbourhood = col_character(),\r## accommodates = col_character(),\r## bathroom = col_character(),\r## bedroom = col_character(),\r## bed = col_character(),\r## review_scores_rating = col_character(),\r## n_reviews = col_character(),\r## url = col_character()\r## )\rnames(edibnb_col_names)\r## [1] \u0026quot;id\u0026quot; \u0026quot;price\u0026quot; \u0026quot;neighbourhood\u0026quot; ## [4] \u0026quot;accommodates\u0026quot; \u0026quot;bathroom\u0026quot; \u0026quot;bedroom\u0026quot; ## [7] \u0026quot;bed\u0026quot; \u0026quot;review_scores_rating\u0026quot; \u0026quot;n_reviews\u0026quot; ## [10] \u0026quot;url\u0026quot;\r\rImporting data with snake case variables\redibnb_clean_names \u0026lt;- read_csv(\u0026quot;edibnb-badnames.csv\u0026quot;) %\u0026gt;%\rjanitor::clean_names()\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## ID = col_double(),\r## Price = col_double(),\r## neighbourhood = col_character(),\r## accommodates = col_double(),\r## `Number of bathrooms` = col_double(),\r## `Number of Bedrooms` = col_double(),\r## `n beds` = col_double(),\r## `Review Scores Rating` = col_double(),\r## `Number of reviews` = col_double(),\r## listing_url = col_character()\r## )\rnames(edibnb_clean_names)\r## [1] \u0026quot;id\u0026quot; \u0026quot;price\u0026quot; \u0026quot;neighbourhood\u0026quot; ## [4] \u0026quot;accommodates\u0026quot; \u0026quot;number_of_bathrooms\u0026quot; \u0026quot;number_of_bedrooms\u0026quot; ## [7] \u0026quot;n_beds\u0026quot; \u0026quot;review_scores_rating\u0026quot; \u0026quot;number_of_reviews\u0026quot; ## [10] \u0026quot;listing_url\u0026quot;\r\rRead df-na.csv\r#read_csv(\u0026quot;df-na.csv\u0026quot;) read_csv(\u0026quot;df-na.csv\u0026quot;, na = c(\u0026quot;\u0026quot;, \u0026quot;NA\u0026quot;, \u0026quot;.\u0026quot;, \u0026quot;9999\u0026quot;, \u0026quot;Not applicable\u0026quot;))\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## x = col_double(),\r## y = col_character(),\r## z = col_character()\r## )\r## # A tibble: 9 x 3\r## x y z ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1 a hi ## 2 NA b hello ## 3 3 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 4 4 d ola ## 5 5 e hola ## 6 NA f whatup\r## 7 7 g wassup\r## 8 8 h sup ## 9 9 i \u0026lt;NA\u0026gt;\r\rReading an xlsx file\rlibrary(readxl)\rfav_food \u0026lt;- read_excel(\u0026quot;favourite-food.xlsx\u0026quot;,\rna = c(\u0026quot;N/A\u0026quot;, \u0026quot;99999\u0026quot;)) %\u0026gt;%\rjanitor::clean_names()\rfav_food\r## # A tibble: 5 x 6\r## student_id full_name favourite_food meal_plan age ses ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt;\r## 1 1 Sunil Huffmann Strawberry yoghurt Lunch only 4 High ## 2 2 Barclay Lynn French fries Lunch only 5 Midd~\r## 3 3 Jayendra Lyne \u0026lt;NA\u0026gt; Breakfast and lunch 7 Low ## 4 4 Leon Rossini Anchovies Lunch only \u0026lt;NA\u0026gt; Midd~\r## 5 5 Chidiegwu Dunkel Pizza Breakfast and lunch five High\rfav_food \u0026lt;- fav_food %\u0026gt;%\rmutate(\rage = if_else(age == \u0026quot;five\u0026quot;, \u0026quot;5\u0026quot;, age),\rage = as.numeric(age)\r)\rfav_food\r## # A tibble: 5 x 6\r## student_id full_name favourite_food meal_plan age ses ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;\r## 1 1 Sunil Huffmann Strawberry yoghurt Lunch only 4 High ## 2 2 Barclay Lynn French fries Lunch only 5 Midd~\r## 3 3 Jayendra Lyne \u0026lt;NA\u0026gt; Breakfast and lunch 7 Low ## 4 4 Leon Rossini Anchovies Lunch only NA Midd~\r## 5 5 Chidiegwu Dunkel Pizza Breakfast and lunch 5 High\rglimpse(fav_food)\r## Rows: 5\r## Columns: 6\r## $ student_id \u0026lt;dbl\u0026gt; 1, 2, 3, 4, 5\r## $ full_name \u0026lt;chr\u0026gt; \u0026quot;Sunil Huffmann\u0026quot;, \u0026quot;Barclay Lynn\u0026quot;, \u0026quot;Jayendra Lyne\u0026quot;, \u0026quot;Leo~\r## $ favourite_food \u0026lt;chr\u0026gt; \u0026quot;Strawberry yoghurt\u0026quot;, \u0026quot;French fries\u0026quot;, NA, \u0026quot;Anchovies\u0026quot;, ~\r## $ meal_plan \u0026lt;chr\u0026gt; \u0026quot;Lunch only\u0026quot;, \u0026quot;Lunch only\u0026quot;, \u0026quot;Breakfast and lunch\u0026quot;, \u0026quot;Lun~\r## $ age \u0026lt;dbl\u0026gt; 4, 5, 7, NA, 5\r## $ ses \u0026lt;chr\u0026gt; \u0026quot;High\u0026quot;, \u0026quot;Middle\u0026quot;, \u0026quot;Low\u0026quot;, \u0026quot;Middle\u0026quot;, \u0026quot;High\u0026quot;\r\rETL of SES varaible\rfav_food %\u0026gt;%\rcount(ses)\r## # A tibble: 3 x 2\r## ses n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 High 2\r## 2 Low 1\r## 3 Middle 2\rfav_food \u0026lt;- fav_food %\u0026gt;%\rmutate(ses = fct_relevel(ses, \u0026quot;Low\u0026quot;, \u0026quot;Middle\u0026quot;, \u0026quot;High\u0026quot;))\rfav_food %\u0026gt;%\rcount(ses)\r## # A tibble: 3 x 2\r## ses n\r## \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt;\r## 1 Low 1\r## 2 Middle 2\r## 3 High 2\rfav_food\r## # A tibble: 5 x 6\r## student_id full_name favourite_food meal_plan age ses ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 Sunil Huffmann Strawberry yoghurt Lunch only 4 High ## 2 2 Barclay Lynn French fries Lunch only 5 Midd~\r## 3 3 Jayendra Lyne \u0026lt;NA\u0026gt; Breakfast and lunch 7 Low ## 4 4 Leon Rossini Anchovies Lunch only NA Midd~\r## 5 5 Chidiegwu Dunkel Pizza Breakfast and lunch 5 High\r\rPutting it altogether\rfav_food \u0026lt;- read_excel(\u0026quot;favourite-food.xlsx\u0026quot;, na = c(\u0026quot;N/A\u0026quot;, \u0026quot;99999\u0026quot;)) %\u0026gt;%\rjanitor::clean_names() %\u0026gt;%\rmutate(\rage = if_else(age == \u0026quot;five\u0026quot;, \u0026quot;5\u0026quot;, age), age = as.numeric(age),\rses = fct_relevel(ses, \u0026quot;Low\u0026quot;, \u0026quot;Middle\u0026quot;, \u0026quot;High\u0026quot;)\r)\rfav_food\r## # A tibble: 5 x 6\r## student_id full_name favourite_food meal_plan age ses ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;fct\u0026gt;\r## 1 1 Sunil Huffmann Strawberry yoghurt Lunch only 4 High ## 2 2 Barclay Lynn French fries Lunch only 5 Midd~\r## 3 3 Jayendra Lyne \u0026lt;NA\u0026gt; Breakfast and lunch 7 Low ## 4 4 Leon Rossini Anchovies Lunch only NA Midd~\r## 5 5 Chidiegwu Dunkel Pizza Breakfast and lunch 5 High\r\r","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"4291ddd31c71d676466b323b17c16e6a","permalink":"/post/data-i-o/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/post/data-i-o/","section":"post","summary":"Import Nobel.csv file\rlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v ggplot2 3.3.3 v purrr 0.3.4\r## v tibble 3.1.1 v dplyr 1.0.6\r## v tidyr 1.","tags":[],"title":"Data I/O","type":"post"},{"authors":null,"categories":[],"content":"\r\rImport and Transform Relig-income.csv file\rlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v ggplot2 3.3.3 v purrr 0.3.4\r## v tibble 3.1.1 v dplyr 1.0.6\r## v tidyr 1.1.3 v stringr 1.4.0\r## v readr 1.4.0 v forcats 0.5.1\r## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rlibrary(readxl)\rrel_inc \u0026lt;- read_excel(\u0026quot;relig-income.xlsx\u0026quot;)\rrel_inc_long \u0026lt;- rel_inc %\u0026gt;%\rrename( religion = `Religious tradition`, n = `Sample Size` ) %\u0026gt;%\rpivot_longer( cols = -c(religion, n), # all but religion and n names_to = \u0026quot;income\u0026quot;, values_to = \u0026quot;proportion\u0026quot; ) %\u0026gt;%\rmutate(frequency = round(proportion * n))\r\rVisualize using Barplot\rrel_inc_long \u0026lt;- rel_inc_long %\u0026gt;%\rmutate(religion = case_when(\rreligion == \u0026quot;Evangelical Protestant\u0026quot; ~ \u0026quot;Ev. Protestant\u0026quot;,\rreligion == \u0026quot;Historically Black Protestant\u0026quot; ~ \u0026quot;Hist. Black Protestant\u0026quot;,\rreligion == \u0026#39;Unaffiliated (religious \u0026quot;nones\u0026quot;)\u0026#39; ~ \u0026quot;Unaffiliated\u0026quot;,\rTRUE ~ religion\r))\rrel_inc_long \u0026lt;- rel_inc_long %\u0026gt;%\rmutate(religion = fct_rev(religion))\rggplot(rel_inc_long, aes(y = religion, x = frequency)) +\rgeom_col()\r\rFill Barplot with Income\rggplot(rel_inc_long, aes(y = religion, x = frequency, fill=income )) +\rgeom_col()\rggplot(rel_inc_long, aes(y = religion, x = frequency, fill=income )) +\rgeom_col(position = \u0026quot;fill\u0026quot;)\rggplot(rel_inc_long, aes(y = religion, x = frequency, fill=income )) +\rgeom_col(position = \u0026quot;fill\u0026quot;)+\rscale_fill_viridis_d()\r\rChange the Theme of the plot\rggplot(rel_inc_long, aes(y = religion, x = frequency, fill = income)) +\rgeom_col(position = \u0026quot;fill\u0026quot;) +\rscale_fill_viridis_d() +\rtheme_minimal()\rggplot(rel_inc_long, aes(y = religion, x = frequency, fill = income)) +\rgeom_col(position = \u0026quot;fill\u0026quot;) +\rscale_fill_viridis_d() +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;bottom\u0026quot;)\rggplot(rel_inc_long, aes(y = religion, x = frequency, fill = income)) +\rgeom_col(position = \u0026quot;fill\u0026quot;) +\rscale_fill_viridis_d() +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;bottom\u0026quot;) +\rguides(fill = guide_legend(nrow = 2, byrow = TRUE))\rggplot(rel_inc_long, aes(y = religion, x = frequency, fill = income)) +\rgeom_col(position = \u0026quot;fill\u0026quot;) +\rscale_fill_viridis_d() +\rtheme_minimal() +\rtheme(legend.position = \u0026quot;bottom\u0026quot;) +\rguides(fill = guide_legend(nrow = 2, byrow = TRUE)) +\rlabs(\rx = \u0026quot;Proportion\u0026quot;, y = \u0026quot;\u0026quot;, title = \u0026quot;Income distribution by religious group\u0026quot;, subtitle = \u0026quot;Source: Pew Research Center, Religious Landscape Study\u0026quot;, fill = \u0026quot;Income\u0026quot; )\r\r","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"5df92dfd30069a845ea59bb05cdde5bd","permalink":"/post/data-record/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/post/data-record/","section":"post","summary":"Import and Transform Relig-income.csv file\rlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v ggplot2 3.3.3 v purrr 0.3.4\r## v tibble 3.1.1 v dplyr 1.0.6\r## v tidyr 1.","tags":[],"title":"Data Record","type":"post"},{"authors":null,"categories":["Tideverse"],"content":"\r\rCOVID-19 World Vaccination Progress\rDaily and Total Vaccination for COVID-19 in the World from Our World in Data, Data is collected daily from Our World in Data GitHub repository for covid-19, merged and uploaded.\rthe used data set in this blog is updated version (119) and it was released in may 26th,2021.\nlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v ggplot2 3.3.3 v purrr 0.3.4\r## v tibble 3.1.1 v dplyr 1.0.6\r## v tidyr 1.1.3 v stringr 1.4.0\r## v readr 1.4.0 v forcats 0.5.1\r## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rlibrary(dplyr)\r# reading the CSV file after placing it in the same directory where getdw() was used to determine the current directory\rcountry_vacc \u0026lt;- read_csv(\u0026quot;country_vaccinations.csv\u0026quot;) \r## ## -- Column specification --------------------------------------------------------\r## cols(\r## country = col_character(),\r## iso_code = col_character(),\r## date = col_date(format = \u0026quot;\u0026quot;),\r## total_vaccinations = col_double(),\r## people_vaccinated = col_double(),\r## people_fully_vaccinated = col_double(),\r## daily_vaccinations_raw = col_double(),\r## daily_vaccinations = col_double(),\r## total_vaccinations_per_hundred = col_double(),\r## people_vaccinated_per_hundred = col_double(),\r## people_fully_vaccinated_per_hundred = col_double(),\r## daily_vaccinations_per_million = col_double(),\r## vaccines = col_character(),\r## source_name = col_character(),\r## source_website = col_character()\r## )\r\rCreating two tables from the country_vaccinations dataset\rtable_1 includes the following variables:\n\rcountry: this is the country for which the vaccination information is provided\riso_code: ISO code for the country\rdate: Date of observation\rpeople_vaccinated: Total number of people who received at least one vaccine dose\rdaily_vaccinations:for a certain data entry, the number of vaccination for that date/country\r\rtable_2 includes the following variables:\n\rcountry: this is the country for which the vaccination information is provided\riso_code: ISO code for the country\rdate: Date of observation\rpeople_fully_vaccinated: this is the number of people that received the entire set of immunization according to the immunization scheme (typically 2)\r\r# Creating the first table (table_1)\rtable_1 \u0026lt;- country_vacc %\u0026gt;%\rselect (country,iso_code,date,people_vaccinated,daily_vaccinations)\rtable_1\r## # A tibble: 20,390 x 5\r## country iso_code date people_vaccinated daily_vaccinations\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan AFG 2021-02-22 0 NA\r## 2 Afghanistan AFG 2021-02-23 NA 1367\r## 3 Afghanistan AFG 2021-02-24 NA 1367\r## 4 Afghanistan AFG 2021-02-25 NA 1367\r## 5 Afghanistan AFG 2021-02-26 NA 1367\r## 6 Afghanistan AFG 2021-02-27 NA 1367\r## 7 Afghanistan AFG 2021-02-28 8200 1367\r## 8 Afghanistan AFG 2021-03-01 NA 1580\r## 9 Afghanistan AFG 2021-03-02 NA 1794\r## 10 Afghanistan AFG 2021-03-03 NA 2008\r## # ... with 20,380 more rows\r# Creating the second table (table_2)\rtable_2 \u0026lt;- country_vacc %\u0026gt;%\rselect (country,iso_code,date, people_fully_vaccinated) table_2\r## # A tibble: 20,390 x 4\r## country iso_code date people_fully_vaccinated\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Afghanistan AFG 2021-02-22 NA\r## 2 Afghanistan AFG 2021-02-23 NA\r## 3 Afghanistan AFG 2021-02-24 NA\r## 4 Afghanistan AFG 2021-02-25 NA\r## 5 Afghanistan AFG 2021-02-26 NA\r## 6 Afghanistan AFG 2021-02-27 NA\r## 7 Afghanistan AFG 2021-02-28 NA\r## 8 Afghanistan AFG 2021-03-01 NA\r## 9 Afghanistan AFG 2021-03-02 NA\r## 10 Afghanistan AFG 2021-03-03 NA\r## # ... with 20,380 more rows\r\rCalculating the percentage of people who are fully vaccinated in Canada\rto calculate the percentage of people who are fully vaccinated we need first to join table_1 and table_2, then applying filter to pick only the Canada data set thenWe can calculate the percentage of people who are fully vaccinated by dividing the number of fully vaccinated people by the total number of vaccinations per day and multiply the result by 100.\npercent_of_fully_vacc \u0026lt;- table_1 %\u0026gt;%\rleft_join(table_2)%\u0026gt;%\rfilter(country==\u0026quot;Canada\u0026quot;) %\u0026gt;%\rmutate(percent_of_fully_vaccinated_people=people_fully_vaccinated/daily_vaccinations*100)\r## Joining, by = c(\u0026quot;country\u0026quot;, \u0026quot;iso_code\u0026quot;, \u0026quot;date\u0026quot;)\rggplot(percent_of_fully_vacc,aes(date,percent_of_fully_vaccinated_people))+\rgeom_col()+\rlabs(title=\u0026quot; Percetage of fully vaccinated people from the daily vaccination\u0026quot;,x=\u0026quot;Date\u0026quot;,y=\u0026quot;percentage of fully vaccinated\u0026quot;)\r## Warning: Removed 23 rows containing missing values (position_stack).\rpercent_of_fully_vacc\r## # A tibble: 163 x 7\r## country iso_code date people_vaccinated daily_vaccinations\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Canada CAN 2020-12-14 NA NA\r## 2 Canada CAN 2020-12-15 NA 718\r## 3 Canada CAN 2020-12-16 NA 1509\r## 4 Canada CAN 2020-12-17 NA 2399\r## 5 Canada CAN 2020-12-18 NA 2792\r## 6 Canada CAN 2020-12-19 NA 2378\r## 7 Canada CAN 2020-12-20 NA 2122\r## 8 Canada CAN 2020-12-21 NA 2980\r## 9 Canada CAN 2020-12-22 NA 3697\r## 10 Canada CAN 2020-12-23 NA 4581\r## # ... with 153 more rows, and 2 more variables: people_fully_vaccinated \u0026lt;dbl\u0026gt;,\r## # percent_of_fully_vaccinated_people \u0026lt;dbl\u0026gt;\r\rCreating a summary table for Canada vaccination records\rThe Can_vacc table will be created after fully joining the two tables which are table country_vacc and percent_of_fully_vacc, then filter Canada data and from the filtered data select and rearrange the date, daily_vaccinations, vaccines, percent_of_fully_vaccinated_people variables\nCan_vacc \u0026lt;- country_vacc %\u0026gt;%\rfull_join(percent_of_fully_vacc) %\u0026gt;%\rfilter(country==\u0026quot;Canada\u0026quot;) %\u0026gt;%\rselect(date,daily_vaccinations,vaccines,percent_of_fully_vaccinated_people)%\u0026gt;%\rrelocate(percent_of_fully_vaccinated_people, .after =daily_vaccinations )\r## Joining, by = c(\u0026quot;country\u0026quot;, \u0026quot;iso_code\u0026quot;, \u0026quot;date\u0026quot;, \u0026quot;people_vaccinated\u0026quot;, \u0026quot;people_fully_vaccinated\u0026quot;, \u0026quot;daily_vaccinations\u0026quot;)\r Can_vacc\r## # A tibble: 163 x 4\r## date daily_vaccinatio~ percent_of_fully_vaccin~ vaccines ## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 2020-12-14 NA NA Moderna, Oxford/AstraZ~\r## 2 2020-12-15 718 NA Moderna, Oxford/AstraZ~\r## 3 2020-12-16 1509 NA Moderna, Oxford/AstraZ~\r## 4 2020-12-17 2399 NA Moderna, Oxford/AstraZ~\r## 5 2020-12-18 2792 NA Moderna, Oxford/AstraZ~\r## 6 2020-12-19 2378 NA Moderna, Oxford/AstraZ~\r## 7 2020-12-20 2122 NA Moderna, Oxford/AstraZ~\r## 8 2020-12-21 2980 NA Moderna, Oxford/AstraZ~\r## 9 2020-12-22 3697 NA Moderna, Oxford/AstraZ~\r## 10 2020-12-23 4581 NA Moderna, Oxford/AstraZ~\r## # ... with 153 more rows\r\rgrouping the data by the vaccine scheme\rwe will group our data based on the Vaccines scheme and calculate the mean of the daily_vaccinations and arrange the result in descending order to know the most used vaccine scheme\nVaccine_scheme\u0026lt;- country_vacc %\u0026gt;% group_by(vaccines) %\u0026gt;% drop_na() %\u0026gt;% summarise(mean_daily_vacc=mean(daily_vaccinations))%\u0026gt;%\rarrange(desc(mean_daily_vacc))\rVaccine_scheme\r## # A tibble: 35 x 2\r## vaccines mean_daily_vacc\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Johnson\u0026amp;Johnson, Moderna, Pfizer/BioNTech 2129707.\r## 2 Covaxin, Oxford/AstraZeneca 1825083.\r## 3 EpiVacCorona, Sputnik V 260009.\r## 4 CanSino, Oxford/AstraZeneca, Pfizer/BioNTech, Sinovac, Sputn~ 200752.\r## 5 Pfizer/BioNTech, Sinovac 144037.\r## 6 CanSino, Oxford/AstraZeneca, Sinopharm/Beijing, Sinovac, Spu~ 141494 ## 7 Oxford/AstraZeneca, Pfizer/BioNTech, Sinovac 140997.\r## 8 Moderna, Oxford/AstraZeneca, Pfizer/BioNTech 134150.\r## 9 Oxford/AstraZeneca, Sinovac 129134.\r## 10 Johnson\u0026amp;Johnson, Moderna, Oxford/AstraZeneca, Pfizer/BioNTech 80371.\r## # ... with 25 more rows\r\rgrouping the data by the source of that data\rwe will group our data based on the source of the data and showing the number of data obtained from each source and then ranking the sources to determine the most source used to collect the data\nSource\u0026lt;-country_vacc %\u0026gt;% group_by(source_name) %\u0026gt;% summarize(n=n()) %\u0026gt;% arrange(desc(n))\rSource\r## # A tibble: 98 x 2\r## source_name n\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 Ministry of Health 5984\r## 2 World Health Organization 2248\r## 3 Government of the United Kingdom 725\r## 4 SPC Public Health Division 510\r## 5 Federal Office of Public Health 273\r## 6 Ministry of Public Health 255\r## 7 Norwegian Institute of Public Health 175\r## 8 Official data from provinces via covid19tracker.ca 163\r## 9 National Health Commission 162\r## 10 Official data from local governments via gogov.ru 162\r## # ... with 88 more rows\r\rCreating a monthly vaccination records\rmonthly_vacc table includes the vaccination records at the end of Jan., Feb., Mach , April and May for each of Australia, Bahamas, Canada and United Kingdom.\nIn monthly_vacc_wider table we dropped any NA data and rearranged the variables to have a wider table\nmonthly_vacc \u0026lt;- country_vacc %\u0026gt;%\rfilter(date==\u0026quot;2021-01-31\u0026quot;| date==\u0026quot;2021-02-28\u0026quot;|date==\u0026quot;2021-03-31\u0026quot;|date==\u0026quot;2021-04-30\u0026quot;|date==\u0026quot;2021-05-20\u0026quot;) %\u0026gt;%\rselect(country,date,total_vaccinations)%\u0026gt;%\rfilter(country==\u0026quot;Canada\u0026quot;|country==\u0026quot;Australia\u0026quot; |country==\u0026quot;United Kingdom\u0026quot;|country==\u0026quot;Bahamas\u0026quot;)\rmonthly_vacc_wider \u0026lt;- monthly_vacc %\u0026gt;% drop_na() %\u0026gt;% pivot_wider(names_from = date,values_from = total_vaccinations)\rmonthly_vacc\r## # A tibble: 17 x 3\r## country date total_vaccinations\r## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Australia 2021-02-28 31894\r## 2 Australia 2021-03-31 670349\r## 3 Australia 2021-04-30 2179544\r## 4 Australia 2021-05-20 3371728\r## 5 Bahamas 2021-03-31 NA\r## 6 Bahamas 2021-04-30 NA\r## 7 Bahamas 2021-05-20 NA\r## 8 Canada 2021-01-31 957229\r## 9 Canada 2021-02-28 1887059\r## 10 Canada 2021-03-31 5690380\r## 11 Canada 2021-04-30 13420198\r## 12 Canada 2021-05-20 19841562\r## 13 United Kingdom 2021-01-31 9790576\r## 14 United Kingdom 2021-02-28 21091267\r## 15 United Kingdom 2021-03-31 35660902\r## 16 United Kingdom 2021-04-30 49319518\r## 17 United Kingdom 2021-05-20 59178397\rmonthly_vacc_wider\r## # A tibble: 3 x 6\r## country `2021-02-28` `2021-03-31` `2021-04-30` `2021-05-20` `2021-01-31`\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Australia 31894 670349 2179544 3371728 NA\r## 2 Canada 1887059 5690380 13420198 19841562 957229\r## 3 United Kingd~ 21091267 35660902 49319518 59178397 9790576\r\rCreating April vaccination records\rApril_vacc table includes the vaccination records of April for each of Australia, Bahamas, Canada, United Kingdom, Hong Kong, Luxembourg, Russia, Scotland, Switzerland, United States and Turkey\nIn April_vacc_longer table the variables were rearranged to have a longer table and any NA values have been replaced by 0.\nApril_vacc \u0026lt;- country_vacc %\u0026gt;%\rfilter(date==\u0026quot;2021-04-30\u0026quot;) %\u0026gt;%\rselect(country,date,total_vaccinations)%\u0026gt;%\rfilter(country==\u0026quot;Canada\u0026quot;|country==\u0026quot;Australia\u0026quot; |country==\u0026quot;United Kingdom\u0026quot;|country==\u0026quot;Bahamas\u0026quot;|country==\u0026quot;Hong Kong\u0026quot;|country==\u0026quot;Luxembourg\u0026quot;|country==\u0026quot;Russia\u0026quot;|country==\u0026quot;Scotland\u0026quot;|country==\u0026quot;Switzerland\u0026quot;|country==\u0026quot;United States\u0026quot;|country==\u0026quot;Turkey\u0026quot;) %\u0026gt;%\rpivot_wider(names_from = country,values_from = total_vaccinations)\rApril_vacc_longer \u0026lt;- April_vacc %\u0026gt;%\rpivot_longer(cols=\u0026quot;Australia\u0026quot;:\u0026quot;United States\u0026quot;,names_to = \u0026quot;Country\u0026quot;,values_to = \u0026quot;Total_vaccinations\u0026quot;) %\u0026gt;%\rreplace_na(list(Total_vaccinations=0))\rApril_vacc\r## # A tibble: 1 x 12\r## date Australia Bahamas Canada `Hong Kong` Luxembourg Russia Scotland\r## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2021-04-30 2179544 NA 13420198 1413788 NA 19834392 4075205\r## # ... with 4 more variables: Switzerland \u0026lt;dbl\u0026gt;, Turkey \u0026lt;dbl\u0026gt;,\r## # United Kingdom \u0026lt;dbl\u0026gt;, United States \u0026lt;dbl\u0026gt;\rApril_vacc_longer\r## # A tibble: 11 x 3\r## date Country Total_vaccinations\r## \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2021-04-30 Australia 2179544\r## 2 2021-04-30 Bahamas 0\r## 3 2021-04-30 Canada 13420198\r## 4 2021-04-30 Hong Kong 1413788\r## 5 2021-04-30 Luxembourg 0\r## 6 2021-04-30 Russia 19834392\r## 7 2021-04-30 Scotland 4075205\r## 8 2021-04-30 Switzerland 2769526\r## 9 2021-04-30 Turkey 22816891\r## 10 2021-04-30 United Kingdom 49319518\r## 11 2021-04-30 United States 240159677\r\r","date":1622073600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622073600,"objectID":"da1d530443e560b7c2e8cb0f0bf51e1b","permalink":"/post/assignment-a02-tidyverse/","publishdate":"2021-05-27T00:00:00Z","relpermalink":"/post/assignment-a02-tidyverse/","section":"post","summary":"COVID-19 World Vaccination Progress\rDaily and Total Vaccination for COVID-19 in the World from Our World in Data, Data is collected daily from Our World in Data GitHub repository for covid-19, merged and uploaded.","tags":[],"title":"Assignment A02: TIDYVERSE","type":"post"},{"authors":null,"categories":["Tideverse"],"content":"\r\rImport the Hotels.cvs file\rlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v ggplot2 3.3.3 v purrr 0.3.4\r## v tibble 3.1.1 v dplyr 1.0.6\r## v tidyr 1.1.3 v stringr 1.4.0\r## v readr 1.4.0 v forcats 0.5.1\r## -- Conflicts ------------------------------------------ tidyverse_conflicts() --\r## x dplyr::filter() masks stats::filter()\r## x dplyr::lag() masks stats::lag()\rhotels \u0026lt;- read_csv(\u0026quot;hotels.csv\u0026quot;)\r## ## -- Column specification --------------------------------------------------------\r## cols(\r## .default = col_double(),\r## hotel = col_character(),\r## arrival_date_month = col_character(),\r## meal = col_character(),\r## country = col_character(),\r## market_segment = col_character(),\r## distribution_channel = col_character(),\r## reserved_room_type = col_character(),\r## assigned_room_type = col_character(),\r## deposit_type = col_character(),\r## agent = col_character(),\r## company = col_character(),\r## customer_type = col_character(),\r## reservation_status = col_character(),\r## reservation_status_date = col_date(format = \u0026quot;\u0026quot;)\r## )\r## i Use `spec()` for the full column specifications.\rhead(hotels)\r## # A tibble: 6 x 32\r## hotel is_canceled lead_time arrival_date_ye~ arrival_date_mo~ arrival_date_we~\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Reso~ 0 342 2015 July 27\r## 2 Reso~ 0 737 2015 July 27\r## 3 Reso~ 0 7 2015 July 27\r## 4 Reso~ 0 13 2015 July 27\r## 5 Reso~ 0 14 2015 July 27\r## 6 Reso~ 0 14 2015 July 27\r## # ... with 26 more variables: arrival_date_day_of_month \u0026lt;dbl\u0026gt;,\r## # stays_in_weekend_nights \u0026lt;dbl\u0026gt;, stays_in_week_nights \u0026lt;dbl\u0026gt;, adults \u0026lt;dbl\u0026gt;,\r## # children \u0026lt;dbl\u0026gt;, babies \u0026lt;dbl\u0026gt;, meal \u0026lt;chr\u0026gt;, country \u0026lt;chr\u0026gt;,\r## # market_segment \u0026lt;chr\u0026gt;, distribution_channel \u0026lt;chr\u0026gt;, is_repeated_guest \u0026lt;dbl\u0026gt;,\r## # previous_cancellations \u0026lt;dbl\u0026gt;, previous_bookings_not_canceled \u0026lt;dbl\u0026gt;,\r## # reserved_room_type \u0026lt;chr\u0026gt;, assigned_room_type \u0026lt;chr\u0026gt;, booking_changes \u0026lt;dbl\u0026gt;,\r## # deposit_type \u0026lt;chr\u0026gt;, agent \u0026lt;chr\u0026gt;, company \u0026lt;chr\u0026gt;, days_in_waiting_list \u0026lt;dbl\u0026gt;,\r## # customer_type \u0026lt;chr\u0026gt;, adr \u0026lt;dbl\u0026gt;, required_car_parking_spaces \u0026lt;dbl\u0026gt;,\r## # total_of_special_requests \u0026lt;dbl\u0026gt;, reservation_status \u0026lt;chr\u0026gt;,\r## # reservation_status_date \u0026lt;date\u0026gt;\rnames(hotels)\r## [1] \u0026quot;hotel\u0026quot; \u0026quot;is_canceled\u0026quot; ## [3] \u0026quot;lead_time\u0026quot; \u0026quot;arrival_date_year\u0026quot; ## [5] \u0026quot;arrival_date_month\u0026quot; \u0026quot;arrival_date_week_number\u0026quot; ## [7] \u0026quot;arrival_date_day_of_month\u0026quot; \u0026quot;stays_in_weekend_nights\u0026quot; ## [9] \u0026quot;stays_in_week_nights\u0026quot; \u0026quot;adults\u0026quot; ## [11] \u0026quot;children\u0026quot; \u0026quot;babies\u0026quot; ## [13] \u0026quot;meal\u0026quot; \u0026quot;country\u0026quot; ## [15] \u0026quot;market_segment\u0026quot; \u0026quot;distribution_channel\u0026quot; ## [17] \u0026quot;is_repeated_guest\u0026quot; \u0026quot;previous_cancellations\u0026quot; ## [19] \u0026quot;previous_bookings_not_canceled\u0026quot; \u0026quot;reserved_room_type\u0026quot; ## [21] \u0026quot;assigned_room_type\u0026quot; \u0026quot;booking_changes\u0026quot; ## [23] \u0026quot;deposit_type\u0026quot; \u0026quot;agent\u0026quot; ## [25] \u0026quot;company\u0026quot; \u0026quot;days_in_waiting_list\u0026quot; ## [27] \u0026quot;customer_type\u0026quot; \u0026quot;adr\u0026quot; ## [29] \u0026quot;required_car_parking_spaces\u0026quot; \u0026quot;total_of_special_requests\u0026quot; ## [31] \u0026quot;reservation_status\u0026quot; \u0026quot;reservation_status_date\u0026quot;\r\rSelect a variable\r# select(hotels, lead_time)\r# select(hotels, hotel,lead_time)\rhotels %\u0026gt;%\rselect(hotel, lead_time) %\u0026gt;%\rarrange(desc(lead_time))\r## # A tibble: 119,390 x 2\r## hotel lead_time\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Resort Hotel 737\r## 2 Resort Hotel 709\r## 3 City Hotel 629\r## 4 City Hotel 629\r## 5 City Hotel 629\r## 6 City Hotel 629\r## 7 City Hotel 629\r## 8 City Hotel 629\r## 9 City Hotel 629\r## 10 City Hotel 629\r## # ... with 119,380 more rows\r\rSelect a range of variable\rhotels %\u0026gt;%\rselect(hotel, lead_time)\r## # A tibble: 119,390 x 2\r## hotel lead_time\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Resort Hotel 342\r## 2 Resort Hotel 737\r## 3 Resort Hotel 7\r## 4 Resort Hotel 13\r## 5 Resort Hotel 14\r## 6 Resort Hotel 14\r## 7 Resort Hotel 0\r## 8 Resort Hotel 9\r## 9 Resort Hotel 85\r## 10 Resort Hotel 75\r## # ... with 119,380 more rows\rhotels %\u0026gt;%\rselect(starts_with(\u0026quot;arrival\u0026quot;))\r## # A tibble: 119,390 x 4\r## arrival_date_year arrival_date_mon~ arrival_date_week_n~ arrival_date_day_of~\r## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 2015 July 27 1\r## 2 2015 July 27 1\r## 3 2015 July 27 1\r## 4 2015 July 27 1\r## 5 2015 July 27 1\r## 6 2015 July 27 1\r## 7 2015 July 27 1\r## 8 2015 July 27 1\r## 9 2015 July 27 1\r## 10 2015 July 27 1\r## # ... with 119,380 more rows\r\rselect a range of rows /cases\rhotels %\u0026gt;%\r# we will select 1st, 5th, 8th and 10th rows\rslice(c(1,5,8, 10))\r## # A tibble: 4 x 32\r## hotel is_canceled lead_time arrival_date_ye~ arrival_date_mo~ arrival_date_we~\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 Reso~ 0 342 2015 July 27\r## 2 Reso~ 0 14 2015 July 27\r## 3 Reso~ 0 9 2015 July 27\r## 4 Reso~ 1 75 2015 July 27\r## # ... with 26 more variables: arrival_date_day_of_month \u0026lt;dbl\u0026gt;,\r## # stays_in_weekend_nights \u0026lt;dbl\u0026gt;, stays_in_week_nights \u0026lt;dbl\u0026gt;, adults \u0026lt;dbl\u0026gt;,\r## # children \u0026lt;dbl\u0026gt;, babies \u0026lt;dbl\u0026gt;, meal \u0026lt;chr\u0026gt;, country \u0026lt;chr\u0026gt;,\r## # market_segment \u0026lt;chr\u0026gt;, distribution_channel \u0026lt;chr\u0026gt;, is_repeated_guest \u0026lt;dbl\u0026gt;,\r## # previous_cancellations \u0026lt;dbl\u0026gt;, previous_bookings_not_canceled \u0026lt;dbl\u0026gt;,\r## # reserved_room_type \u0026lt;chr\u0026gt;, assigned_room_type \u0026lt;chr\u0026gt;, booking_changes \u0026lt;dbl\u0026gt;,\r## # deposit_type \u0026lt;chr\u0026gt;, agent \u0026lt;chr\u0026gt;, company \u0026lt;chr\u0026gt;, days_in_waiting_list \u0026lt;dbl\u0026gt;,\r## # customer_type \u0026lt;chr\u0026gt;, adr \u0026lt;dbl\u0026gt;, required_car_parking_spaces \u0026lt;dbl\u0026gt;,\r## # total_of_special_requests \u0026lt;dbl\u0026gt;, reservation_status \u0026lt;chr\u0026gt;,\r## # reservation_status_date \u0026lt;date\u0026gt;\rhotels %\u0026gt;%\rfilter(hotel == \u0026quot;City Hotel\u0026quot;)\r## # A tibble: 79,330 x 32\r## hotel is_canceled lead_time arrival_date_year arrival_date_month\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 City Hotel 0 6 2015 July ## 2 City Hotel 1 88 2015 July ## 3 City Hotel 1 65 2015 July ## 4 City Hotel 1 92 2015 July ## 5 City Hotel 1 100 2015 July ## 6 City Hotel 1 79 2015 July ## 7 City Hotel 0 3 2015 July ## 8 City Hotel 1 63 2015 July ## 9 City Hotel 1 62 2015 July ## 10 City Hotel 1 62 2015 July ## # ... with 79,320 more rows, and 27 more variables:\r## # arrival_date_week_number \u0026lt;dbl\u0026gt;, arrival_date_day_of_month \u0026lt;dbl\u0026gt;,\r## # stays_in_weekend_nights \u0026lt;dbl\u0026gt;, stays_in_week_nights \u0026lt;dbl\u0026gt;, adults \u0026lt;dbl\u0026gt;,\r## # children \u0026lt;dbl\u0026gt;, babies \u0026lt;dbl\u0026gt;, meal \u0026lt;chr\u0026gt;, country \u0026lt;chr\u0026gt;,\r## # market_segment \u0026lt;chr\u0026gt;, distribution_channel \u0026lt;chr\u0026gt;, is_repeated_guest \u0026lt;dbl\u0026gt;,\r## # previous_cancellations \u0026lt;dbl\u0026gt;, previous_bookings_not_canceled \u0026lt;dbl\u0026gt;,\r## # reserved_room_type \u0026lt;chr\u0026gt;, assigned_room_type \u0026lt;chr\u0026gt;, booking_changes \u0026lt;dbl\u0026gt;,\r## # deposit_type \u0026lt;chr\u0026gt;, agent \u0026lt;chr\u0026gt;, company \u0026lt;chr\u0026gt;, days_in_waiting_list \u0026lt;dbl\u0026gt;,\r## # customer_type \u0026lt;chr\u0026gt;, adr \u0026lt;dbl\u0026gt;, required_car_parking_spaces \u0026lt;dbl\u0026gt;,\r## # total_of_special_requests \u0026lt;dbl\u0026gt;, reservation_status \u0026lt;chr\u0026gt;,\r## # reservation_status_date \u0026lt;date\u0026gt;\rhotels %\u0026gt;%\rfilter( adults == 0,\rchildren \u0026gt;= 1\r) %\u0026gt;% select(adults, babies, children)\r## # A tibble: 223 x 3\r## adults babies children\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 0 3\r## 2 0 0 2\r## 3 0 0 2\r## 4 0 0 2\r## 5 0 0 2\r## 6 0 0 3\r## 7 0 1 2\r## 8 0 0 2\r## 9 0 0 2\r## 10 0 0 2\r## # ... with 213 more rows\rhotels %\u0026gt;%\rfilter( adults == 0, children \u0026gt;= 1 \u0026amp; babies \u0026gt;= 1 # \u0026amp; means and\r) %\u0026gt;%\rselect(adults, babies, children)\r## # A tibble: 3 x 3\r## adults babies children\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 0 1 2\r## 2 0 1 2\r## 3 0 1 2\r\rsummarizing\rhotels %\u0026gt;%\rcount(hotel,market_segment)\r## # A tibble: 14 x 3\r## hotel market_segment n\r## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt;\r## 1 City Hotel Aviation 237\r## 2 City Hotel Complementary 542\r## 3 City Hotel Corporate 2986\r## 4 City Hotel Direct 6093\r## 5 City Hotel Groups 13975\r## 6 City Hotel Offline TA/TO 16747\r## 7 City Hotel Online TA 38748\r## 8 City Hotel Undefined 2\r## 9 Resort Hotel Complementary 201\r## 10 Resort Hotel Corporate 2309\r## 11 Resort Hotel Direct 6513\r## 12 Resort Hotel Groups 5836\r## 13 Resort Hotel Offline TA/TO 7472\r## 14 Resort Hotel Online TA 17729\r\rmutation\rlittle_ones\u0026lt;- hotels\rlittle_ones %\u0026lt;\u0026gt;%\rmutate(little_ones = children + babies) %\u0026gt;%\rselect(children, babies, little_ones) %\u0026gt;%\rarrange(desc(little_ones))\r\rSummary\rsummary\u0026lt;- hotels %\u0026gt;%\rgroup_by(hotel) %\u0026gt;%\rsummarise(mean_adr = mean(adr))\rsummary\r## # A tibble: 2 x 2\r## hotel mean_adr\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 City Hotel 105. ## 2 Resort Hotel 95.0\r\r","date":1621900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621900800,"objectID":"4038a826fcbfb7daf3d08c1f197b3b5a","permalink":"/post/data-wrangling-using-tidyverse/","publishdate":"2021-05-25T00:00:00Z","relpermalink":"/post/data-wrangling-using-tidyverse/","section":"post","summary":"Import the Hotels.cvs file\rlibrary(tidyverse)\r## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --\r## v ggplot2 3.3.3 v purrr 0.3.4\r## v tibble 3.1.1 v dplyr 1.0.6\r## v tidyr 1.","tags":[],"title":"Data Wrangling using Tidyverse","type":"post"},{"authors":null,"categories":["Basics"],"content":"\r\rThe Variables and Numbers\rx\u0026lt;- 1\ry=2\r3 -\u0026gt; z\rx+y\r## [1] 3\ry*z\r## [1] 6\ry/x\r## [1] 2\r\rVectors\ra\u0026lt;- 0:10\rprint(a)\r## [1] 0 1 2 3 4 5 6 7 8 9 10\rb\u0026lt;- 10:-4\rprint(b)\r## [1] 10 9 8 7 6 5 4 3 2 1 0 -1 -2 -3 -4\rclass(a)\r## [1] \u0026quot;integer\u0026quot;\rstr(a)\r## int [1:11] 0 1 2 3 4 5 6 7 8 9 ...\ra[ c(1, 5, 10) ]\r## [1] 0 4 9\rhead(a)\r## [1] 0 1 2 3 4 5\rhead(a,4)\r## [1] 0 1 2 3\rtail(b,5)\r## [1] 0 -1 -2 -3 -4\r\rCompine Operators for Creatinmg Vectors\rWhen we create a vector with multiple data types, R coerces the vector to most compatible data type\nC\u0026lt;-c(1:5,10.5,\u0026#39;red\u0026#39;,\u0026#39;green\u0026#39;,\u0026quot;yellow\u0026quot;)\rprint(C)\r## [1] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5\u0026quot; \u0026quot;10.5\u0026quot; \u0026quot;red\u0026quot; \u0026quot;green\u0026quot; ## [9] \u0026quot;yellow\u0026quot;\rclass(C)\r## [1] \u0026quot;character\u0026quot;\rstr(C)\r## chr [1:9] \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;4\u0026quot; \u0026quot;5\u0026quot; \u0026quot;10.5\u0026quot; \u0026quot;red\u0026quot; \u0026quot;green\u0026quot; \u0026quot;yellow\u0026quot;\rd\u0026lt;- c(1:5,10.5)\rprint(d)\r## [1] 1.0 2.0 3.0 4.0 5.0 10.5\rclass(d)\r## [1] \u0026quot;numeric\u0026quot;\rstr(d)\r## num [1:6] 1 2 3 4 5 10.5\r\rSequence Operator Application\rx\u0026lt;-seq(0,8*pi,length.out = 200);\ry \u0026lt;- sin(x);\rplot(x,y)\r\rDropping Missing Values\ra\u0026lt;-c(3,-2,4,NA,-1,8,-4,9,NA, 11,3,8,NA)\ra[!is.na(a)]\r## [1] 3 -2 4 -1 8 -4 9 11 3 8\ra \u0026lt;-c(2,3,-1,3,5,2,-3,1)\rcat(\u0026quot;The sum of positive integers in a =\u0026quot;,sum(a[a\u0026gt;0]))\r## The sum of positive integers in a = 16\r\rMatrix\rM1\u0026lt;-matrix(1:12, ncol=4,byrow=TRUE)\rM1\r## [,1] [,2] [,3] [,4]\r## [1,] 1 2 3 4\r## [2,] 5 6 7 8\r## [3,] 9 10 11 12\rM1[,3]\r## [1] 3 7 11\rM2\u0026lt;-matrix(1:12, nrow=4)\rM2\r## [,1] [,2] [,3]\r## [1,] 1 5 9\r## [2,] 2 6 10\r## [3,] 3 7 11\r## [4,] 4 8 12\r\rChallange: Matrix element\rM1 \u0026lt;-matrix(1:20,ncol=4) M1[c(3,5),c(2,4)]\r## [,1] [,2]\r## [1,] 8 18\r## [2,] 10 20\r\rData Frames\rDF \u0026lt;- data.frame(\rgender = c(\u0026quot;Male\u0026quot;, \u0026quot;Male\u0026quot;,\u0026quot;Female\u0026quot;),\rheight = c(152, 171.5, 165),\rweight = c(81,93, 78),\rage =c(42,38,26),\rrow.names=c(\u0026#39;Ally\u0026#39;,\u0026#39;Belinda\u0026#39;,\u0026#39;Alfred\u0026#39;)\r)\rDF$age\r## [1] 42 38 26\rDF[DF$gender==\u0026quot;Male\u0026quot;,]\r## gender height weight age\r## Ally Male 152.0 81 42\r## Belinda Male 171.5 93 38\r\r","date":1621900800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621900800,"objectID":"45c364a94785a15e35af29e8883707d6","permalink":"/post/fun-with-r/","publishdate":"2021-05-25T00:00:00Z","relpermalink":"/post/fun-with-r/","section":"post","summary":"The Variables and Numbers\rx\u0026lt;- 1\ry=2\r3 -\u0026gt; z\rx+y\r## [1] 3\ry*z\r## [1] 6\ry/x\r## [1] 2\r\rVectors\ra\u0026lt;- 0:10\rprint(a)\r## [1] 0 1 2 3 4 5 6 7 8 9 10\rb\u0026lt;- 10:-4\rprint(b)\r## [1] 10 9 8 7 6 5 4 3 2 1 0 -1 -2 -3 -4\rclass(a)\r## [1] \u0026quot;integer\u0026quot;\rstr(a)\r## int [1:11] 0 1 2 3 4 5 6 7 8 9 .","tags":[],"title":"Fun with R","type":"post"},{"authors":null,"categories":["ggplot2"],"content":"\r\rFirst, The Effect of Vitamin C on Tooth Growth in Guinea Pigs\rIn this blog we are going to show the effect of providing vitamin C on the growth of the odontoblasts of 60 guinea pigs.\nKeywords:\r\rOdontoblasts: cells responsible for tooth growth\rAscorbic acid: a form of vitamin C and coded as VC.\r\r\rEach pig received one of three dose levels of vitamin C which are:\r0.5 mg/day\r1 mg/day\r2 mg/day\r\r\rThe Vitamin C dose is provided by one of the two following delivery methods:\rThrough orange juice.\rThrough ascorbic acid.\r\rThe collected data is composed of 60 observations one for each pig where the tooth length, taken dose and the delivery method are recorded.\n\rThe number of participant in each dose level providing the dose delivery method\rfrom the above Fig (1), we can see that for every dose level there are 20 participants and from which 10 participants received the dose through orange juice (OJ) and the other 10 participants received the dose through ascorbic acid (VC).\n\rThe relationship between the tooth growth and the dose level\rFrom Fig(2) we can see that the tooth growth that is determined by measuring the tooth length vs the dose level given to each pig providing the delivery method of the dose.\n\rGrouping the data based on both the dose and the delivery method\rFrom Fig(3), we can have a more detailed information about each group, where the grouping is done based on the dose level and the dose delivery method. in fig (3), we can see that the group with dose level equal 2 mg/day and deliver the dose through ascorbic acid (VC/2) achieved the largest tooth growth but when the dose was 0.5 mg/day and the delivery method was VC, it achieved the lowest tooth growth.\n\rDistribution of Tooth growth at each dose level for each delivery method\rFrom Fig (4), we can see that at dose level 0.5 and 1 mg/day the tooth growth is better when the delivery method is the orange juice and at dose level of 2mg/day the tooth growth is better when the dose is delivered by ascorbic acid.\n\rTooth length histogram\rggplot(ToothGrowth,aes(x=len))+\rgeom_histogram(binwidth = 1)+\rlabs(title=\u0026quot; Counts for Tooth length measurements \u0026quot;,x=\u0026quot;Tooth Length\u0026quot;,\rcaption = \u0026quot;Fig(5) \u0026quot;)\rFrom Fig(5), we can see that the peak of the histogram occurs at length around 26 with 5 teeth occurred at the bin.\n\rDensity of the tooth growth based on the dose delivery method\rFig (6), shows the distribution of tooth length for each dose delivery method.\n\rBoxplot for the tooth length distribution based on the delivery method\rFig(7) shows the distribution of the tooth growth data based on each dose delivery method. This boxplot shows the minimum, maximum, median, first quartile and third quartile in the data set.\n\rDensity of the tooth growth based on the dose level\rFig (8), shows the distribution of tooth length for each dose level.\n\rBoxplot for the tooth length distribution based on the dose level\rFig (9), shows the distribution of the tooth growth data based on each dose level. This boxplot shows the minimum, maximum, median, first quartile and third quartile in the data set.\n\r\rSecond, Average Heights and Weights for American Women\rThis data set provides the average heights and weights for American women of age ranges from 30 to 39\nggplot(women,aes(height,weight))+geom_point()+labs(title=\u0026quot; Height vs weight\u0026quot;,x=\u0026quot;Height in In \u0026quot;,y=\u0026quot;Weight in Ibs\u0026quot;, caption = \u0026quot;Fig(10) \u0026quot;)\rFig (10), shows us that the relationship between the height and the weight for the American women is linearly.\n\r","date":1621641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621641600,"objectID":"410f2206c6c8a48ca578ac63387cdac1","permalink":"/post/assignment-a01-ggplot2/","publishdate":"2021-05-22T00:00:00Z","relpermalink":"/post/assignment-a01-ggplot2/","section":"post","summary":"First, The Effect of Vitamin C on Tooth Growth in Guinea Pigs\rIn this blog we are going to show the effect of providing vitamin C on the growth of the odontoblasts of 60 guinea pigs.","tags":[],"title":"Assignment A01: GGPLOT2 ","type":"post"},{"authors":null,"categories":["ggplot2"],"content":"\r\rData visualization Exercise\rlibrary(dplyr)\rstarwars\r## # A tibble: 87 x 14\r## name height mass hair_color skin_color eye_color birth_year sex gender\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke S~ 172 77 blond fair blue 19 male mascu~\r## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; gold yellow 112 none mascu~\r## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; white, bl~ red 33 none mascu~\r## 4 Darth ~ 202 136 none white yellow 41.9 male mascu~\r## 5 Leia O~ 150 49 brown light brown 19 fema~ femin~\r## 6 Owen L~ 178 120 brown, grey light blue 52 male mascu~\r## 7 Beru W~ 165 75 brown light blue 47 fema~ femin~\r## 8 R5-D4 97 32 \u0026lt;NA\u0026gt; white, red red NA none mascu~\r## 9 Biggs ~ 183 84 black light brown 24 male mascu~\r## 10 Obi-Wa~ 182 77 auburn, wh~ fair blue-gray 57 male mascu~\r## # ... with 77 more rows, and 5 more variables: homeworld \u0026lt;chr\u0026gt;, species \u0026lt;chr\u0026gt;,\r## # films \u0026lt;list\u0026gt;, vehicles \u0026lt;list\u0026gt;, starships \u0026lt;list\u0026gt;\rMass Vs Weight\rWe will study the mass vs weight relationship through scatter plot\nlibrary(ggplot2)\rggplot(data = starwars, mapping = aes(x = height, y = mass)) +\rgeom_point() +\rlabs(title = \u0026quot; Mass vs. height of Starwars characters\u0026quot;,\rx = \u0026quot;Height (cm)\u0026quot;, y = \u0026quot;Weight (kg)\u0026quot;)\r\rAnscombe’s Quartet\rWe summarize the quartet information by each set of data\nlibrary(Tmisc)\rquartet %\u0026gt;%\rgroup_by(set) %\u0026gt;%\rsummarise(\rmean_x = mean(x), mean_y = mean(y),\rsd_x = sd(x),\rsd_y = sd(y),\rr = cor(x, y)\r)\r## # A tibble: 4 x 6\r## set mean_x mean_y sd_x sd_y r\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 I 9 7.50 3.32 2.03 0.816\r## 2 II 9 7.50 3.32 2.03 0.816\r## 3 III 9 7.5 3.32 2.03 0.816\r## 4 IV 9 7.50 3.32 2.03 0.817\rWe visualize all four sets\r\r\r","date":1621296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621296000,"objectID":"bbb85674cfb69c0bf5c3cc86dd4ffc5e","permalink":"/post/data-and-visualization/","publishdate":"2021-05-18T00:00:00Z","relpermalink":"/post/data-and-visualization/","section":"post","summary":"Data visualization Exercise\rlibrary(dplyr)\rstarwars\r## # A tibble: 87 x 14\r## name height mass hair_color skin_color eye_color birth_year sex gender\r## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Luke S~ 172 77 blond fair blue 19 male mascu~\r## 2 C-3PO 167 75 \u0026lt;NA\u0026gt; gold yellow 112 none mascu~\r## 3 R2-D2 96 32 \u0026lt;NA\u0026gt; white, bl~ red 33 none mascu~\r## 4 Darth ~ 202 136 none white yellow 41.","tags":[],"title":"Data and Visualization","type":"post"},{"authors":null,"categories":[],"content":"\r\rPalmer Penguins Data Visualization\r\r","date":1621296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621296000,"objectID":"bbc1f60018aacb6f953d2021701ffb0b","permalink":"/post/data-visualisation-using-ggplot2/","publishdate":"2021-05-18T00:00:00Z","relpermalink":"/post/data-visualisation-using-ggplot2/","section":"post","summary":"\r\rPalmer Penguins Data Visualization\r\r","tags":[],"title":"Data Visualisation using ggplot2 ","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]