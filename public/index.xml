<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HOME</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>HOME</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 22 Jul 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url>
      <title>HOME</title>
      <link>/</link>
    </image>
    
    <item>
      <title>A03 Resampling Methods</title>
      <link>/post/a03-resampling-methods/</link>
      <pubDate>Thu, 22 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/a03-resampling-methods/</guid>
      <description>
&lt;script src=&#34;/post/a03-resampling-methods/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-goal&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Goal&lt;/h3&gt;
&lt;p&gt;The goal behind this study is to discuss two of the most commonly used resampling methods, cross-validation and the bootstrap and we would apply both resampling methods on linear and logistic regression models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;first-for-logistic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First For Logistic Regression Model:&lt;/h2&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The data&lt;/h3&gt;
&lt;p&gt;The data used in this study was obtained from UCI website and we specifically used the Bank-full.csv dataset.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;splitting-the-data-to-training-and-testing-splits&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Splitting the data to training and testing splits&lt;/h3&gt;
&lt;p&gt;In this step we are going to split the data so that 3/4 of the data would be assigned for training and the remaining 1/4 of the data would be assigned for testing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-the-logistic-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create the logistic regression model&lt;/h3&gt;
&lt;p&gt;Creating the logistic regression model that will be used to identify whether a customer will subscribe to a term deposit or not.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-recipe&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating the recipe&lt;/h3&gt;
&lt;p&gt;Creating the suitable recipe to apply feature engineering to our data&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create workflow&lt;/h3&gt;
&lt;p&gt;Create a workflow that combine the model and the recipe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model&lt;/h3&gt;
&lt;p&gt;Train and fitting the model using the training split of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 35 x 5
##    term              estimate std.error statistic  p.value
##    &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 (Intercept)         -1.65     0.126     -13.1  3.88e-39
##  2 duration             1.06     0.0189     56.3  0       
##  3 campaign            -0.272    0.0358     -7.60 2.94e-14
##  4 job_blue.collar     -0.368    0.0833     -4.41 1.01e- 5
##  5 job_entrepreneur    -0.367    0.143      -2.57 1.01e- 2
##  6 job_housemaid       -0.441    0.150      -2.95 3.20e- 3
##  7 job_management      -0.243    0.0838     -2.90 3.72e- 3
##  8 job_retired          0.166    0.0997      1.67 9.57e- 2
##  9 job_self.employed   -0.330    0.128      -2.57 1.03e- 2
## 10 job_services        -0.242    0.0971     -2.49 1.27e- 2
## 11 job_student          0.521    0.119       4.37 1.23e- 5
## 12 job_technician      -0.183    0.0794     -2.31 2.11e- 2
## # ... with 23 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-predictions-for-training-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Make predictions for training data&lt;/h3&gt;
&lt;p&gt;Using the trained model for prediction on the training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 33,907 x 11
##    .pred_class y     job          education housing loan  contact month duration
##    &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;        &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;    &amp;lt;int&amp;gt;
##  1 no          no    management   tertiary  yes     no    unknown may        261
##  2 no          no    entrepreneur secondary yes     yes   unknown may         76
##  3 no          no    blue-collar  unknown   yes     no    unknown may         92
##  4 no          no    unknown      unknown   no      no    unknown may        198
##  5 no          no    management   tertiary  yes     no    unknown may        139
##  6 no          no    technician   secondary yes     no    unknown may         55
##  7 no          no    admin.       secondary yes     no    unknown may        137
##  8 no          no    technician   secondary yes     no    unknown may        517
##  9 no          no    technician   unknown   yes     no    unknown may         71
## 10 no          no    services     secondary yes     no    unknown may        174
## # ... with 33,897 more rows, and 2 more variables: campaign &amp;lt;int&amp;gt;,
## #   poutcome &amp;lt;fct&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-performance-on-tarining-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluate performance on tarining data&lt;/h3&gt;
&lt;p&gt;Evaluate the performance of the model on the training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##           Truth
## Prediction    no   yes
##        no  29193  2595
##        yes   748  1371&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.901
## 2 sens     binary         0.975
## 3 spec     binary         0.346&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-predictions-for-testing-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Make predictions for testing data&lt;/h3&gt;
&lt;p&gt;Using the trained model for prediction on the testing data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 11,304 x 11
##    .pred_class y     job          education housing loan  contact month duration
##    &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;        &amp;lt;fct&amp;gt;     &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;   &amp;lt;fct&amp;gt;    &amp;lt;int&amp;gt;
##  1 no          no    technician   secondary yes     no    unknown may        151
##  2 no          no    management   tertiary  yes     yes   unknown may        217
##  3 no          no    entrepreneur tertiary  yes     no    unknown may        380
##  4 no          no    retired      primary   yes     no    unknown may         50
##  5 no          no    admin.       secondary yes     no    unknown may        222
##  6 no          no    retired      primary   yes     no    unknown may        353
##  7 no          no    admin.       unknown   yes     no    unknown may         98
##  8 no          no    blue-collar  primary   yes     no    unknown may         38
##  9 no          no    retired      primary   yes     no    unknown may        219
## 10 no          no    technician   secondary yes     yes   unknown may        348
## # ... with 11,294 more rows, and 2 more variables: campaign &amp;lt;int&amp;gt;,
## #   poutcome &amp;lt;fct&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-performance-on-testing-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluate performance on testing data&lt;/h3&gt;
&lt;p&gt;Evaluate the performance of the model on the testing data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##           Truth
## Prediction   no  yes
##        no  9754  875
##        yes  227  448&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 accuracy binary         0.903
## 2 sens     binary         0.977
## 3 spec     binary         0.339&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;apply-cross-validation-method-for-model-evaluation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apply Cross Validation method for model evaluation&lt;/h2&gt;
&lt;p&gt;We will use the cross-validation resampling method to evaluate the performance of our logestic regression model.&lt;/p&gt;
&lt;div id=&#34;split-data-into-folds&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Split data into folds&lt;/h3&gt;
&lt;p&gt;The train data is split-ted into 5 folds&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-resamples&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit resamples&lt;/h3&gt;
&lt;p&gt;Fit the folds into the workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collect-cv-metrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collect CV metrics&lt;/h3&gt;
&lt;p&gt;Show the metrics resulted from Cross-Validation process&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric  .estimator  mean     n std_err .config             
##   &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 accuracy binary     0.902     5 0.00165 Preprocessor1_Model1
## 2 roc_auc  binary     0.904     5 0.00174 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The accuracy associated with the model using the training data is 90.1%, that associated with the model using the testing data is 90.3% and that associated with the model using cross validation resampling method is 90.1%. The accuracy in the testing data is quite good and it is slightly better than the accuracy in the training data which indicates that our model is not exposed to overfitting on the training data, also the accuracy resulted from the testing data is comparable with the accuracy we got from cross-validation resampling method that indicates that our model is working properly.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;applying-bootstrap-resampling-method&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applying Bootstrap resampling method&lt;/h2&gt;
&lt;p&gt;In this part we are estimating the accuracy of a Logestic Regression Model through the bootstrap approach which can be used to assess the variability of the coefficient estimates (Betas) and predictions from a statistical learning method.&lt;/p&gt;
&lt;p&gt;The standard error reflects the variability between the estimates we would obtain if we repeatedly took samples from the population. The standard error associated with the coefficients of both the model and that obtained through bootstrapping are shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##        (Intercept)     jobblue-collar    jobentrepreneur       jobhousemaid 
##       -2.416485382       -0.346072166       -0.413788618       -0.538843667 
##      jobmanagement         jobretired   jobself-employed        jobservices 
##       -0.189089703        0.196164320       -0.311713723       -0.236414425 
##         jobstudent      jobtechnician      jobunemployed         jobunknown 
##        0.511192792       -0.173019378       -0.177429381       -0.361506746 
## educationsecondary  educationtertiary   educationunknown         housingyes 
##        0.203955264        0.436149134        0.269835962       -0.700007468 
##            loanyes   contacttelephone     contactunknown           monthaug 
##       -0.444875217       -0.175980434       -1.605880094       -0.763099074 
##           monthdec           monthfeb           monthjan           monthjul 
##        0.646688881       -0.251511233       -1.163911847       -0.843156031 
##           monthjun           monthmar           monthmay           monthnov 
##        0.366545869        1.553360619       -0.425837991       -0.874011401 
##           monthoct           monthsep           duration           campaign 
##        0.888103673        0.811484702        0.004190172       -0.086589973 
##      poutcomeother    poutcomesuccess    poutcomeunknown 
##        0.233821843        2.303692088       -0.078536962&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                        Estimate   Std. Error    z value      Pr(&amp;gt;|z|)
## (Intercept)        -2.416485382 1.113856e-01 -21.694773 2.298578e-104
## jobblue-collar     -0.346072166 7.226905e-02  -4.788664  1.678955e-06
## jobentrepreneur    -0.413788618 1.249918e-01  -3.310525  9.312098e-04
## jobhousemaid       -0.538843667 1.354648e-01  -3.977740  6.957349e-05
## jobmanagement      -0.189089703 7.293692e-02  -2.592510  9.527837e-03
## jobretired          0.196164320 8.618864e-02   2.275988  2.284672e-02
## jobself-employed   -0.311713723 1.114374e-01  -2.797210  5.154603e-03
## jobservices        -0.236414425 8.390921e-02  -2.817503  4.839869e-03
## jobstudent          0.511192792 1.044029e-01   4.896346  9.763496e-07
## jobtechnician      -0.173019378 6.880062e-02  -2.514794  1.191020e-02
## jobunemployed      -0.177429381 1.114466e-01  -1.592058  1.113717e-01
## jobunknown         -0.361506746 2.329359e-01  -1.551958  1.206723e-01
## educationsecondary  0.203955264 6.411055e-02   3.181306  1.466128e-03
## educationtertiary   0.436149134 7.395298e-02   5.897654  3.687051e-09
## educationunknown    0.269835962 1.035801e-01   2.605095  9.184878e-03
## housingyes         -0.700007468 4.329460e-02 -16.168472  8.415943e-59
## loanyes            -0.444875217 5.967071e-02  -7.455504  8.952482e-14
## contacttelephone   -0.175980434 7.410680e-02  -2.374687  1.756384e-02
## contactunknown     -1.605880094 7.237608e-02 -22.187993 4.485657e-109
## monthaug           -0.763099074 7.747181e-02  -9.850022  6.852875e-23
## monthdec            0.646688881 1.757141e-01   3.680347  2.329164e-04
## monthfeb           -0.251511233 8.412492e-02  -2.989735  2.792195e-03
## monthjan           -1.163911847 1.196732e-01  -9.725755  2.341610e-22
## monthjul           -0.843156031 7.706815e-02 -10.940395  7.387702e-28
## monthjun            0.366545869 9.019358e-02   4.063991  4.824078e-05
## monthmar            1.553360619 1.187454e-01  13.081435  4.204289e-39
## monthmay           -0.425837991 7.108191e-02  -5.990807  2.088027e-09
## monthnov           -0.874011401 8.371180e-02 -10.440719  1.615814e-25
## monthoct            0.888103673 1.076217e-01   8.252090  1.556480e-16
## monthsep            0.811484702 1.180207e-01   6.875780  6.165138e-12
## duration            0.004190172 6.442787e-05  65.036632  0.000000e+00
## campaign           -0.086589973 1.003337e-02  -8.630197  6.124664e-18
## poutcomeother       0.233821843 8.916180e-02   2.622444  8.730153e-03
## poutcomesuccess     2.303692088 7.959003e-02  28.944480 3.293120e-184
## poutcomeunknown    -0.078536962 5.731654e-02  -1.370232  1.706144e-01&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = bank_data, statistic = boot_fn_bank, R = 100)
## 
## 
## Bootstrap Statistics :
##          original        bias     std. error
## t1*  -2.416485382 -8.028300e-03 0.1255282876
## t2*  -0.346072166  6.884443e-03 0.0656667243
## t3*  -0.413788618 -1.746870e-02 0.1250063163
## t4*  -0.538843667 -2.632170e-02 0.1506685985
## t5*  -0.189089703 -1.946738e-03 0.0703688067
## t6*   0.196164320  9.726636e-03 0.1007565978
## t7*  -0.311713723  6.681341e-03 0.1118123533
## t8*  -0.236414425  4.509866e-03 0.0777245698
## t9*   0.511192792  1.878887e-02 0.0999970178
## t10* -0.173019378 -2.964749e-03 0.0639570480
## t11* -0.177429381 -1.918871e-02 0.1279365248
## t12* -0.361506746  2.929020e-02 0.2202905524
## t13*  0.203955264  3.677783e-03 0.0710370345
## t14*  0.436149134  2.678866e-03 0.0783973333
## t15*  0.269835962 -7.098522e-03 0.1134082599
## t16* -0.700007468  2.614857e-03 0.0437524671
## t17* -0.444875217  2.541731e-03 0.0564787713
## t18* -0.175980434 -1.769945e-02 0.0788122518
## t19* -1.605880094  2.710617e-03 0.0776266569
## t20* -0.763099074  2.196291e-04 0.0861235310
## t21*  0.646688881 -1.281272e-03 0.2189127383
## t22* -0.251511233 -3.810631e-03 0.0928764444
## t23* -1.163911847 -6.144847e-03 0.1196075241
## t24* -0.843156031 -4.977336e-03 0.0803586902
## t25*  0.366545869 -1.096552e-02 0.0949528655
## t26*  1.553360619  4.760479e-03 0.1289172848
## t27* -0.425837991 -2.116673e-03 0.0731852404
## t28* -0.874011401 -5.277234e-03 0.0887677215
## t29*  0.888103673  1.226003e-02 0.1220745778
## t30*  0.811484702 -3.511322e-02 0.1457014049
## t31*  0.004190172 -5.900232e-07 0.0001029556
## t32* -0.086589973 -2.098398e-03 0.0115775812
## t33*  0.233821843  1.028080e-02 0.0957398188
## t34*  2.303692088  1.141992e-02 0.0869714862
## t35* -0.078536962  3.761925e-03 0.0625711164&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;second-for-linear-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Second For Linear Regression Model:&lt;/h2&gt;
&lt;div id=&#34;the-diamond-ring-data&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The diamond ring data&lt;/h3&gt;
&lt;p&gt;In this part we used the diamond ring data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 440
## Columns: 9
## $ price         &amp;lt;dbl&amp;gt; 3000, 3000, 3004, 3004, 3006, 3007, 3008, 3010, 3012, 30~
## $ carat         &amp;lt;dbl&amp;gt; 0.92, 0.92, 0.82, 0.81, 0.90, 0.87, 0.80, 0.84, 0.80, 0.~
## $ colour        &amp;lt;chr&amp;gt; &amp;quot;I&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;J&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;~
## $ clarity       &amp;lt;fct&amp;gt; SI2, SI2, SI2, SI1, VS2, SI2, SI2, SI1, SI2, SI2, SI2, S~
## $ cut           &amp;lt;chr&amp;gt; &amp;quot;G&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;~
## $ certification &amp;lt;chr&amp;gt; &amp;quot;AGS&amp;quot;, &amp;quot;AGS&amp;quot;, &amp;quot;GIA&amp;quot;, &amp;quot;GIA&amp;quot;, &amp;quot;GIA&amp;quot;, &amp;quot;AGS&amp;quot;, &amp;quot;GIA&amp;quot;, &amp;quot;GIA&amp;quot;, ~
## $ polish        &amp;lt;chr&amp;gt; &amp;quot;V&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;~
## $ symmetry      &amp;lt;chr&amp;gt; &amp;quot;V&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;~
## $ wholesaler    &amp;lt;chr&amp;gt; &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;1&amp;quot;, &amp;quot;~&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;splitting-the-data-to-training-and-testing-splits-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;splitting the data to training and testing splits&lt;/h3&gt;
&lt;p&gt;In this step we are going to split the data on hands so that 3/4 of the data would be assigned for training and 1/4 of the data would be assigned for testing.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-the-linear-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;create the linear regression model&lt;/h3&gt;
&lt;p&gt;Creating the linear regression model that will be used to predict the price of the diamond ring of interest.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-the-recipe-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;creating the recipe&lt;/h3&gt;
&lt;p&gt;creating the suitable recipe to apply feature engineering to our data&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflow-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create workflow&lt;/h3&gt;
&lt;p&gt;Create a workflow that combine the model and the recipe.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model&lt;/h3&gt;
&lt;p&gt;Train and fitting the model using the training split of the data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 36 x 5
##    term        estimate std.error statistic  p.value
##    &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 (Intercept)    2141.     128.      16.7  2.29e-44
##  2 carat           709.      58.5     12.1  1.14e-27
##  3 colour_E       -215.      45.1     -4.75 3.13e- 6
##  4 colour_F       -280.      45.2     -6.20 1.94e- 9
##  5 colour_G       -276.      46.7     -5.91 9.47e- 9
##  6 colour_H       -343.      46.0     -7.46 9.79e-13
##  7 colour_I       -380.      46.4     -8.19 8.25e-15
##  8 colour_J       -435.      49.6     -8.76 1.65e-16
##  9 colour_K       -666.      57.1    -11.7  4.62e-26
## 10 colour_L       -878.      68.3    -12.8  2.92e-30
## # ... with 26 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-predictions-for-training-data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Make predictions for training data&lt;/h3&gt;
&lt;p&gt;Using the trained model for prediction on the training data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 328 x 10
##    .pred price carat colour clarity cut   certification polish symmetry
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;fct&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   
##  1 350.    190  0.09 D      SI1     X     IGI           G      G       
##  2 134.    160  0.09 E      SI1     I     IGI           G      G       
##  3  85.6   160  0.09 F      SI1     I     IGI           V      G       
##  4 134.    180  0.09 E      SI1     I     IGI           G      G       
##  5 227.    190  0.09 E      VS1     I     IGI           V      G       
##  6 196.    330  0.1  E      VS2     V     IGI           V      G       
##  7 192.    180  0.1  E      VS2     X     IGI           G      G       
##  8  88.9   160  0.1  F      SI1     X     IGI           G      G       
##  9 142.    160  0.1  E      SI1     V     IGI           G      G       
## 10  60.7   190  0.1  E      SI2     G     IGI           V      G       
## # ... with 318 more rows, and 1 more variable: wholesaler &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-performance-on-tarining-data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluate performance on tarining data&lt;/h3&gt;
&lt;p&gt;Evaluate the performance of the model on the training data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;R-squared&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 rsq     standard       0.986&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/a03-resampling-methods/index_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;
&lt;em&gt;RMSE&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 rmse    standard        136.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;make-predictions-for-testing-data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Make predictions for testing data&lt;/h3&gt;
&lt;p&gt;Using the trained model for prediction on the testing data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 112 x 10
##    .pred price carat colour clarity cut   certification polish symmetry
##    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;fct&amp;gt;   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;   
##  1 3035.  3000  0.92 I      SI2     G     AGS           V      V       
##  2 3028.  3000  0.92 I      SI2     V     AGS           G      G       
##  3 3088.  3006  0.9  J      VS2     V     GIA           V      V       
##  4 3027.  3027  0.81 F      SI1     V     AGS           V      V       
##  5 2957.  3036  0.81 H      SI1     V     GIA           G      V       
##  6 3235.  3041  0.83 D      SI2     G     GIA           G      G       
##  7 3052.  3044  0.91 I      SI2     I     GIA           V      V       
##  8 3000.  3062  0.8  E      SI2     V     GIA           V      V       
##  9   NA   3081  0.9  F      SI2     F     GIA           v      G       
## 10 2956.  3089  0.91 H      SI2     F     GIA           V      V       
## # ... with 102 more rows, and 1 more variable: wholesaler &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-performance-on-testing-data-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Evaluate performance on testing data&lt;/h3&gt;
&lt;p&gt;Evaluate the performance of the model on the testing data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;R-squared&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 rsq     standard       0.983&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/a03-resampling-methods/index_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;RMSE&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 3
##   .metric .estimator .estimate
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
## 1 rmse    standard        157.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;apply-cross-validation-method-for-model-evaluation-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Apply Cross Validation method for model evaluation&lt;/h2&gt;
&lt;p&gt;We will use the cross-validation resampling method to evaluate the performance of our linear regression model.&lt;/p&gt;
&lt;div id=&#34;split-data-into-folds-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Split data into folds&lt;/h3&gt;
&lt;p&gt;The train data is split-ted into 5 folds&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-resamples-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit resamples&lt;/h3&gt;
&lt;p&gt;Fit the folds into the workflow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collect-cv-metrics-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collect CV metrics&lt;/h3&gt;
&lt;p&gt;Show the metrics resulted from Cross-Validation process&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 6
##   .metric .estimator    mean     n std_err .config             
##   &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt; &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;               
## 1 rmse    standard   153.        5 5.07    Preprocessor1_Model1
## 2 rsq     standard     0.983     5 0.00123 Preprocessor1_Model1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;The rsq associated with the model on the training data is 0.99, that associated with the model in the testing data is 0.98 and that associated with the model using cross validation is 0.98. From that we can see that our model is performing good.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;applying-bootstrap-resampling-method-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Applying Bootstrap resampling method&lt;/h2&gt;
&lt;p&gt;In this part we are estimating the accuracy of a Linear Regression Model through the bootstrap approach which can be used to assess the variability of the coefficient estimates (Betas) and predictions from a statistical learning method.&lt;/p&gt;
&lt;p&gt;The standard error reflects the variability between the estimates we would obtain if we repeatedly took samples from the population. The standard error associated with the coefficients of both the model and that obtained through bootstrapping are shown below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# boot_fn_rings= function (data ,index){
#   return(tidy( wkfl_rings %&amp;gt;%fit(data=data[index,]))) }

boot_fn_rings= function (data ,index ){
return (coef (lm(price ~ .,data =data , subset =index )))}

df2 &amp;lt;- model.matrix( ~ price + carat + colour + clarity+cut + certification + polish + symmetry + wholesaler-1, data = rings_data)

set.seed(103)

summary(lm(price ~ .,data =as.data.frame(df2)))$coef&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                     Estimate Std. Error     t value     Pr(&amp;gt;|t|)
## (Intercept)       -106.56155  175.79890  -0.6061560 5.447516e-01
## carat             1885.30336  133.35039  14.1379667 3.742257e-37
## colourD            895.44460   59.30734  15.0983785 3.695004e-41
## colourE            728.83798   52.92247  13.7718071 1.196791e-35
## colourF            648.29530   52.36663  12.3799311 4.580770e-30
## colourG            665.84244   51.58213  12.9083948 3.708078e-32
## colourH            595.60521   50.56202  11.7796946 9.764476e-28
## colourI            557.81920   49.66629  11.2313443 1.170879e-25
## colourJ            461.05137   49.26745   9.3581342 5.704793e-19
## colourK            199.01441   50.96007   3.9053008 1.102762e-04
## clarityI2         -590.10469   39.59374 -14.9039905 2.424038e-40
## claritySI1         652.27743   37.30949  17.4828817 2.280084e-51
## claritySI2         560.50704   30.84165  18.1737061 2.265758e-54
## claritySI3         290.45599   37.17977   7.8122046 4.902422e-14
## clarityVS1         743.08136   49.56030  14.9934784 1.020529e-40
## clarityVS2         689.49914   43.96568  15.6826658 1.246192e-43
## clarityVVS1       1015.21402  117.33428   8.6523228 1.208576e-16
## clarityVVS2        760.49537   77.07807   9.8665595 1.025130e-20
## cutG                48.81986   30.73652   1.5883339 1.129931e-01
## cutI                84.50708   30.00680   2.8162641 5.096725e-03
## cutV                78.44805   30.50328   2.5717904 1.047401e-02
## cutX                93.37420   26.51667   3.5213390 4.783374e-04
## certificationDOW  -271.24204  164.60408  -1.6478452 1.001619e-01
## certificationEGL  -307.28583   67.95169  -4.5221220 8.057030e-06
## certificationGIA    12.15763   60.44293   0.2011423 8.406885e-01
## certificationIGI  -118.26743   72.53254  -1.6305431 1.037661e-01
## polishG             66.57386   78.84945   0.8443161 3.989925e-01
## polishI            247.69579  115.61368   2.1424436 3.275488e-02
## polishv            136.45426  169.48598   0.8051065 4.212318e-01
## polishV             78.39863   81.67731   0.9598581 3.377007e-01
## polishX             84.85158   83.94039   1.0108553 3.126907e-01
## symmetryG          133.37429   42.42139   3.1440340 1.789425e-03
## symmetryV          151.49072   45.27573   3.3459583 8.967355e-04
## symmetryX          137.39985   50.44279   2.7238747 6.732040e-03
## wholesaler2        112.07125   49.94276   2.2439938 2.537367e-02
## wholesaler3      -1460.66895   78.09862 -18.7028787 1.115682e-56&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;boot(statistic = boot_fn_rings,
                  data = as.data.frame(df2), R = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = as.data.frame(df2), statistic = boot_fn_rings, R = 1000)
## 
## 
## Bootstrap Statistics :
##         original      bias    std. error
## t1*   -106.56155 -9.35456333   261.67043
## t2*   1885.30336 16.89859427   177.59923
## t3*    895.44460  2.78838482    99.78342
## t4*    728.83798  1.96797173    88.88759
## t5*    648.29530  0.44403917    88.81436
## t6*    665.84244 -0.87732839    86.79063
## t7*    595.60521  1.51940839    86.22062
## t8*    557.81920  1.08977842    85.60847
## t9*    461.05137 -1.31429699    83.33705
## t10*   199.01441 -5.71033660    81.01659
## t12*  -590.10469 -5.62400852    57.95193
## t13*   652.27743  4.13415624    53.01940
## t14*   560.50704  3.28170559    46.95164
## t15*   290.45599  5.96937782    58.04980
## t16*   743.08136  5.88777718    62.79755
## t17*   689.49914  3.99395348    58.81016
## t18*  1015.21402 10.65485966    80.97264
## t19*   760.49537  4.40900611   103.99886
## t20*    48.81986 -3.96590696    42.73852
## t21*    84.50708 -2.33151751    37.91071
## t22*    78.44805 -2.30491726    32.90870
## t23*    93.37420 -1.88620077    34.89901
## t24*  -271.24204 -1.08968639    78.50134
## t25*  -307.28583 -2.51905597    53.47293
## t26*    12.15763  0.16925914    30.04501
## t27*  -118.26743  2.50044800    49.52972
## t28*    66.57386 -7.36747324   136.36556
## t29*   247.69579 -8.64360605   133.21323
## t30*   136.45426 -7.91079733   139.81229
## t31*    78.39863 -7.36281044   140.63048
## t32*    84.85158 -6.72228927   141.11272
## t33*   133.37429  0.59236148    62.87303
## t35*   151.49072 -0.05086443    65.82882
## t36*   137.39985  1.01165019    67.30974
## t37*   112.07125  0.30607925    62.57052
## t38* -1460.66895  8.51131528    99.28839
## WARNING: All values of t11* are NA
## WARNING: All values of t34* are NA&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A02 Classification Model</title>
      <link>/post/a02-classification-model/</link>
      <pubDate>Sat, 17 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/a02-classification-model/</guid>
      <description>
&lt;script src=&#34;/post/a02-classification-model/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-goal&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Goal&lt;/h2&gt;
&lt;p&gt;The goal behind this study is to use the provided data set to identify whether a customer will subscribe to a term deposit or not.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-bank-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bank data&lt;/h2&gt;
&lt;p&gt;The data used in this study was obtained from &lt;strong&gt;UCI &lt;/strong&gt; website and we specifically used the Bank-full.csv dataset, to access the source of the data use the link below&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://archive.ics.uci.edu/ml/datasets/Bank+Marketing&#34; target=&#34;_blank&#34;&gt;Link to the dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Getting a glimpse into the data we can see that it is composed of 45211 observations and 17 variables.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 45,211
## Columns: 17
## $ age       &amp;lt;int&amp;gt; 58, 44, 33, 47, 33, 35, 28, 42, 58, 43, 41, 29, 53, 58, 57, ~
## $ job       &amp;lt;fct&amp;gt; management, technician, entrepreneur, blue-collar, unknown, ~
## $ marital   &amp;lt;fct&amp;gt; married, single, married, married, single, married, single, ~
## $ education &amp;lt;fct&amp;gt; tertiary, secondary, secondary, unknown, unknown, tertiary, ~
## $ default   &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, yes, no, no, no, no, no, no, no,~
## $ balance   &amp;lt;int&amp;gt; 2143, 29, 2, 1506, 1, 231, 447, 2, 121, 593, 270, 390, 6, 71~
## $ housing   &amp;lt;fct&amp;gt; yes, yes, yes, yes, no, yes, yes, yes, yes, yes, yes, yes, y~
## $ loan      &amp;lt;fct&amp;gt; no, no, yes, no, no, no, yes, no, no, no, no, no, no, no, no~
## $ contact   &amp;lt;fct&amp;gt; unknown, unknown, unknown, unknown, unknown, unknown, unknow~
## $ day       &amp;lt;chr&amp;gt; &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, &amp;quot;5&amp;quot;, ~
## $ month     &amp;lt;fct&amp;gt; may, may, may, may, may, may, may, may, may, may, may, may, ~
## $ duration  &amp;lt;int&amp;gt; 261, 151, 76, 92, 198, 139, 217, 380, 50, 55, 222, 137, 517,~
## $ campaign  &amp;lt;int&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~
## $ pdays     &amp;lt;int&amp;gt; -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, ~
## $ previous  &amp;lt;int&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~
## $ poutcome  &amp;lt;fct&amp;gt; unknown, unknown, unknown, unknown, unknown, unknown, unknow~
## $ y         &amp;lt;fct&amp;gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, ~&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-variables-description&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Variables description:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;bank client data:&lt;/p&gt;
&lt;p&gt;1 - age (numeric)
2 - job : type of job (categorical: “admin.”,“unknown”,“unemployed”,“management”,“housemaid”,“entrepreneur”,“student”,
“blue-collar”,“self-employed”,“retired”,“technician”,“services”)
3 - marital : marital status (categorical: “married”,“divorced”,“single”; note: “divorced” means divorced or widowed)
4 - education (categorical: “unknown”,“secondary”,“primary”,“tertiary”)
5 - default: has credit in default? (binary: “yes”,“no”)
6 - balance: average yearly balance, in euros (numeric)
7 - housing: has housing loan? (binary: “yes”,“no”)
8 - loan: has personal loan? (binary: “yes”,“no”)
*related with the last contact of the current campaign:
9 - contact: contact communication type (categorical: “unknown”,“telephone”,“cellular”)
10 - day: last contact day of the month (numeric)
11 - month: last contact month of year (categorical: “jan”, “feb”, “mar”, …, “nov”, “dec”)
12 - duration: last contact duration, in seconds (numeric)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;*other attributes:
13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)
15 - previous: number of contacts performed before this campaign and for this client (numeric)
16 - poutcome: outcome of the previous marketing campaign (categorical: “unknown”,“other”,“failure”,“success”)&lt;/p&gt;
&lt;p&gt;Output variable (desired target):
17 - y - has the client subscribed a term deposit? (binary: “yes”,“no”)&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;partioning-the-data-set-to-train-and-test-parts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Partioning the data set to train and test parts&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## 75% of the sample size
smp_size &amp;lt;- floor(0.75 * nrow(whole_data))

## set the seed to make your partition reproducible
set.seed(123)
train_ind &amp;lt;- sample(seq_len(nrow(whole_data)), size = smp_size)

data &amp;lt;- whole_data[train_ind, ]
test &amp;lt;- whole_data[-train_ind, ]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-univriate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Univriate Analysis&lt;/h2&gt;
&lt;p&gt;we will start our analysis with the metric data which are the age, balance, duration, campaign, pdays, and previous then we will analysis the non metric variables which are all the remaining variables.&lt;/p&gt;
&lt;p&gt;For the metric variables the shown plots, provided summary and the simple calculations, we can get valuable information about the each metric variable distribution, like the range, min, max, mean , media, mode, 1st and 3rd quartile, IQR, variance and standard deviation.&lt;/p&gt;
&lt;p&gt;For the non metric data the histogram would be used.&lt;/p&gt;
&lt;div id=&#34;the-age&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Age&lt;/h3&gt;
&lt;p&gt;The data distribution of the age is right skewed and the peak occurs at age 32.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-5-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   18.00   33.00   39.00   40.91   48.00   95.00&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 112.7199&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10.61696&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   age    n
## 1  32 1554&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75% 
##  15&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-balance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Balance&lt;/h3&gt;
&lt;p&gt;The data distribution for the balance variable ranges from -8019 to 102127, so it is better to use log x scale in this case.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   -6847      71     448    1353    1427  102127&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 8945813&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2990.955&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   balance    n
## 1       0 2641&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  75% 
## 1356&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-duration&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The duration&lt;/h3&gt;
&lt;p&gt;The data distribution of the duration variable has a wide range as it ranges from 0 to 4918, so it is better to use log x scale in this case also.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     0.0   103.0   180.0   258.1   318.0  3785.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 65821.23&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 256.5565&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   duration   n
## 1      104 142&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75% 
## 215&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-campaign&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Campaign&lt;/h3&gt;
&lt;p&gt;The data distribution of the campaign variable has ranges from 1 contact to 63 contacts where the mode is 1 contact that results in the distribution is right skewed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-8-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   1.000   1.000   2.000   2.777   3.000  58.000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 9.867114&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.141196&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   campaign     n
## 1        1 13125&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75% 
##   2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-pdays&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Pdays&lt;/h3&gt;
&lt;p&gt;The data distribution of the pdays variable ranges from -1 that means the client never contacted before to 871 days.
the distribution is left skewed with mode of -1 day.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-9-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   -1.00   -1.00   -1.00   40.57   -1.00  871.00&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10155.46&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 100.7743&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   pdays     n
## 1    -1 27657&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75% 
##   0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-previous&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Previous&lt;/h3&gt;
&lt;p&gt;The data distribution of the previous variable ranges from 0 to 275 contacts,the distribution is right skewed with the most dominant number of contact of 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-10-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
##   0.0000   0.0000   0.0000   0.5882   0.0000 275.0000&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 5.91227&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2.431516&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   previous     n
## 1        0 27657&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 75% 
##   0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-job&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Job&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the job variable, we can see that the most dominant job titles are Blue collar and Management then Technician.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-marital&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Marital&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the Marital variable, we can see that the most dominant marital status is married.
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-education&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Education&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the education variable, we can see that the most dominant education is the secondary education.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-default&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The default&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the default variable, we can see that the most of the clients has no credit in default.
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-housing&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Housing&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the Housing variable, we can see that clients with housing loan are greater than clients with no housing loan.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-loan&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Loan&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the loan variable, we can see that most of the clients don’t have loan.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-contact&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Contact&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the contact variable, we can see that most communication type was cellular.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-month&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Month&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the month variable, we can see that the most dominant month is May.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-poutcome&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Poutcome&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the poutcome variable, we can see that the most dominant outcome is “unknown”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-y&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Y&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the Y variable, we can see that the most of the client haven’t subscribed to term deposit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-20-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-bivariate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bivariate Analysis&lt;/h2&gt;
&lt;p&gt;In the Bivaraite analysis we will see the relationship between the variable y and all other variables&lt;/p&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-age-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the Age variable&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-21-1.png&#34; width=&#34;672&#34; /&gt;
### The relationship between variable Y and the balance variable
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-duration-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the duration variable&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;
### The relationship between variable Y and the Campaign variable
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-24-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-pdays-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the pdays variable&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-25-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-previous-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the previous variable&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-26-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-job-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the Job variable&lt;/h3&gt;
&lt;p&gt;We can see that the Management has the most subscription among other jobs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-27-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-marital-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the marital variable&lt;/h3&gt;
&lt;p&gt;Married people has more subscription that single and divorced.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-education-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the education variable&lt;/h3&gt;
&lt;p&gt;Secondary education has the more subscription than the remaining education types.
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-29-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-default-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the default variable&lt;/h3&gt;
&lt;p&gt;The clients that have default status are not subscribed to term deposit
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-30-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-housing-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the housing variable&lt;/h3&gt;
&lt;p&gt;Clients have not housing loan are subscribed to term deposit more than clients having housing loan.
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-31-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-loan-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the loan variable&lt;/h3&gt;
&lt;p&gt;Clients haven’t loan are subscribed to term deposit more than clients having loan.
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-contact-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the contact variable&lt;/h3&gt;
&lt;p&gt;We can see that cellular connection type results in more subscribtion to a term deposit
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-33-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-day-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the day variable&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-34-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-month-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the month variable&lt;/h3&gt;
&lt;p&gt;The Contact that happend in May results in more subscription to term deposit than other months.
&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-35-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-variable-y-and-the-poutcome-variable&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between variable Y and the poutcome variable&lt;/h3&gt;
&lt;p&gt;The unknown outcome has the most subscription to term deposit.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-36-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simple-logestic-regression-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The simple logestic regression models&lt;/h2&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-age-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable age only&lt;/h3&gt;
&lt;p&gt;From the model we can see that age variable is a significant variable for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ age, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.5813  -0.5081  -0.4939  -0.4855   2.1290  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -2.265534   0.066949 -33.840  &amp;lt; 2e-16 ***
## age          0.006031   0.001564   3.857 0.000115 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24516  on 33906  degrees of freedom
## AIC: 24520
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-job-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable job only&lt;/h3&gt;
&lt;p&gt;From the model we can see that out of 11 job types blue-collar, entrepreneur, housemaid, retired, services and student are the most significant job types in the prediction of the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ job, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8267  -0.5484  -0.4823  -0.3898   2.2871  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)      -1.96033    0.04865 -40.294  &amp;lt; 2e-16 ***
## jobblue-collar   -0.57904    0.06635  -8.727  &amp;lt; 2e-16 ***
## jobentrepreneur  -0.42491    0.11826  -3.593 0.000327 ***
## jobhousemaid     -0.37660    0.12482  -3.017 0.002551 ** 
## jobmanagement     0.14183    0.05950   2.383 0.017152 *  
## jobretired        0.73374    0.07620   9.629  &amp;lt; 2e-16 ***
## jobself-employed -0.04850    0.10148  -0.478 0.632730    
## jobservices      -0.36818    0.07946  -4.633  3.6e-06 ***
## jobstudent        1.06239    0.09734  10.914  &amp;lt; 2e-16 ***
## jobtechnician    -0.13271    0.06442  -2.060 0.039389 *  
## jobunemployed     0.28798    0.10040   2.868 0.004126 ** 
## jobunknown       -0.15645    0.22601  -0.692 0.488794    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 23973  on 33896  degrees of freedom
## AIC: 23997
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-marital-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable marital only&lt;/h3&gt;
&lt;p&gt;From the model we can see that all the marital status are considered significant variables for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ marital, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.5701  -0.5109  -0.4619  -0.4619   2.1404  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)    -1.97041    0.04882 -40.359  &amp;lt; 2e-16 ***
## maritalmarried -0.21361    0.05405  -3.952 7.76e-05 ***
## maritalsingle   0.23561    0.05659   4.163 3.14e-05 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24385  on 33905  degrees of freedom
## AIC: 24391
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-education-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable education only&lt;/h3&gt;
&lt;p&gt;From the model we can see that secondary, tertiary and unknown education types are significant variables for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ education, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.5695  -0.5695  -0.4742  -0.4267   2.2099  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)        -2.35073    0.04954 -47.450  &amp;lt; 2e-16 ***
## educationsecondary  0.22198    0.05530   4.014 5.98e-05 ***
## educationtertiary   0.61370    0.05693  10.781  &amp;lt; 2e-16 ***
## educationunknown    0.50519    0.09299   5.433 5.54e-08 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24362  on 33904  degrees of freedom
## AIC: 24370
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-default-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable default only&lt;/h3&gt;
&lt;p&gt;From the model we can see that both default cases are significant for the prediction of the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ default, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.5023  -0.5023  -0.5023  -0.5023   2.3732  
## 
## Coefficients:
##             Estimate Std. Error  z value Pr(&amp;gt;|z|)    
## (Intercept) -2.00667    0.01696 -118.304  &amp;lt; 2e-16 ***
## defaultyes  -0.74766    0.16816   -4.446 8.74e-06 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24506  on 33906  degrees of freedom
## AIC: 24510
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-balance-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable balance only&lt;/h3&gt;
&lt;p&gt;From the model we can see that both balance cases are considered significant for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ balance, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2789  -0.4959  -0.4871  -0.4840   2.1568  
## 
## Coefficients:
##               Estimate Std. Error  z value Pr(&amp;gt;|z|)    
## (Intercept) -2.085e+00  1.847e-02 -112.895   &amp;lt;2e-16 ***
## balance      4.509e-05  4.518e-06    9.979   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24437  on 33906  degrees of freedom
## AIC: 24441
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-housing-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable housing only&lt;/h3&gt;
&lt;p&gt;From the model we can see that wether owning a house loan or not, both cases are considered significant for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ housing, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.6025  -0.6025  -0.4042  -0.4042   2.2563  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.61438    0.02188  -73.79   &amp;lt;2e-16 ***
## housingyes  -0.84930    0.03484  -24.38   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 23913  on 33906  degrees of freedom
## AIC: 23917
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-loan-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable loan only&lt;/h3&gt;
&lt;p&gt;From the model we can see that both cases of the presence and absence of loan are considered significant for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ loan, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.5207  -0.5207  -0.5207  -0.3768   2.3155  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.92995    0.01782 -108.33   &amp;lt;2e-16 ***
## loanyes     -0.67977    0.05653  -12.03   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24362  on 33906  degrees of freedom
## AIC: 24366
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-contact-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable contact only&lt;/h3&gt;
&lt;p&gt;From the model we can see that the cellular and unknown contact types are considered significant for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ contact, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.5705  -0.5705  -0.5705  -0.2863   2.5357  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)      -1.73311    0.01885 -91.946   &amp;lt;2e-16 ***
## contacttelephone -0.15788    0.06705  -2.355   0.0185 *  
## contactunknown   -1.44078    0.05494 -26.224   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 23585  on 33905  degrees of freedom
## AIC: 23591
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-day-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable day only&lt;/h3&gt;
&lt;p&gt;From the model we can see that day variable is a significant variable for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ day, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8618  -0.5489  -0.4645  -0.3888   2.3639  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)  -0.7992     0.1381  -5.786 7.19e-09 ***
## day10        -0.5139     0.1844  -2.787  0.00532 ** 
## day11        -1.1994     0.1664  -7.210 5.61e-13 ***
## day12        -0.9022     0.1594  -5.661 1.51e-08 ***
## day13        -0.9307     0.1606  -5.794 6.87e-09 ***
## day14        -1.2945     0.1633  -7.928 2.23e-15 ***
## day15        -0.9899     0.1591  -6.222 4.92e-10 ***
## day16        -1.0057     0.1635  -6.150 7.73e-10 ***
## day17        -1.4617     0.1647  -8.873  &amp;lt; 2e-16 ***
## day18        -1.3807     0.1588  -8.693  &amp;lt; 2e-16 ***
## day19        -1.9000     0.1787 -10.632  &amp;lt; 2e-16 ***
## day2         -0.9752     0.1665  -5.855 4.77e-09 ***
## day20        -1.8062     0.1635 -11.044  &amp;lt; 2e-16 ***
## day21        -1.3730     0.1618  -8.487  &amp;lt; 2e-16 ***
## day22        -0.8387     0.1727  -4.856 1.20e-06 ***
## day23        -1.0173     0.1752  -5.806 6.40e-09 ***
## day24        -1.0206     0.2093  -4.876 1.08e-06 ***
## day25        -1.0311     0.1809  -5.699 1.20e-08 ***
## day26        -1.2114     0.1772  -6.836 8.13e-12 ***
## day27        -1.0265     0.1705  -6.019 1.75e-09 ***
## day28        -1.6408     0.1697  -9.672  &amp;lt; 2e-16 ***
## day29        -1.7452     0.1734 -10.062  &amp;lt; 2e-16 ***
## day3         -0.8357     0.1677  -4.983 6.26e-07 ***
## day30        -0.7284     0.1581  -4.608 4.06e-06 ***
## day31        -1.9316     0.2362  -8.177 2.91e-16 ***
## day4         -0.8272     0.1609  -5.142 2.71e-07 ***
## day5         -1.2481     0.1614  -7.734 1.04e-14 ***
## day6         -1.4638     0.1644  -8.905  &amp;lt; 2e-16 ***
## day7         -1.6730     0.1715  -9.756  &amp;lt; 2e-16 ***
## day8         -1.2551     0.1622  -7.738 1.01e-14 ***
## day9         -1.1965     0.1649  -7.256 3.98e-13 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24083  on 33877  degrees of freedom
## AIC: 24145
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-month-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable month only&lt;/h3&gt;
&lt;p&gt;From the model we can see that all the months are significant for predicting the possibility of subscription to a term deposit except for February.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ month, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1699  -0.4672  -0.4436  -0.3748   2.3198  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.35179    0.05248 -25.757  &amp;lt; 2e-16 ***
## monthaug    -0.77239    0.07066 -10.930  &amp;lt; 2e-16 ***
## monthdec     1.24125    0.16544   7.503 6.24e-14 ***
## monthfeb    -0.23880    0.07988  -2.990  0.00279 ** 
## monthjan    -0.84753    0.11504  -7.367 1.74e-13 ***
## monthjul    -0.91732    0.07079 -12.959  &amp;lt; 2e-16 ***
## monthjun    -0.80843    0.07373 -10.965  &amp;lt; 2e-16 ***
## monthmar     1.33409    0.12064  11.058  &amp;lt; 2e-16 ***
## monthmay    -1.26873    0.06554 -19.357  &amp;lt; 2e-16 ***
## monthnov    -0.84432    0.08037 -10.506  &amp;lt; 2e-16 ***
## monthoct     1.07262    0.10098  10.622  &amp;lt; 2e-16 ***
## monthsep     1.25381    0.11000  11.398  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 22913  on 33896  degrees of freedom
## AIC: 22937
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-duration-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable duration only&lt;/h3&gt;
&lt;p&gt;From the model we can see that duartion variable is a significant variable for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ duration, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.5727  -0.4383  -0.3609  -0.3169   2.5213  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -3.219e+00  3.041e-02 -105.87   &amp;lt;2e-16 ***
## duration     3.613e-03  6.395e-05   56.49   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 20523  on 33906  degrees of freedom
## AIC: 20527
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-campaign-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable campaign only&lt;/h3&gt;
&lt;p&gt;From the model we can see that the number of contacts performed during this campaign and for the client variable is a significant variable for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ campaign, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.5430  -0.5430  -0.5127  -0.4565   3.3628  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -1.716973   0.026529  -64.72   &amp;lt;2e-16 ***
## campaign    -0.122932   0.009274  -13.26   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24286  on 33906  degrees of freedom
## AIC: 24290
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-pdays-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable pdays only&lt;/h3&gt;
&lt;p&gt;From the model we can see that number of days that passed by after the client was last contacted from a previous campaign is significant variable for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ pdays, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.2040  -0.4695  -0.4695  -0.4695   2.1260  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -2.1471962  0.0189762 -113.15   &amp;lt;2e-16 ***
## pdays        0.0025369  0.0001365   18.59   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24221  on 33906  degrees of freedom
## AIC: 24225
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-previous-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable previous only&lt;/h3&gt;
&lt;p&gt;From the model we can see that the number of contacts performed before this campaign and for this client significant variable for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ previous, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -7.5518  -0.4809  -0.4809  -0.4809   2.1045  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -2.098905   0.017963 -116.85   &amp;lt;2e-16 ***
## previous     0.111322   0.006967   15.98   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 24275  on 33906  degrees of freedom
## AIC: 24279
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-logistic-regression-model-of-y-using-the-predictor-variable-poutcome-only&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The logistic regression model of y using the predictor variable poutcome only&lt;/h3&gt;
&lt;p&gt;From the model we can see that outcome of the previous marketing campaign is significant variable for predicting the possibility of subscription to a term deposit.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ poutcome, family = &amp;quot;binomial&amp;quot;, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.4439  -0.4394  -0.4394  -0.4394   2.1844  
## 
## Coefficients:
##                 Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)     -1.94806    0.04962 -39.258  &amp;lt; 2e-16 ***
## poutcomeother    0.34468    0.08741   3.943 8.04e-05 ***
## poutcomesuccess  2.55578    0.07934  32.212  &amp;lt; 2e-16 ***
## poutcomeunknown -0.34131    0.05381  -6.343 2.25e-10 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 22527  on 33904  degrees of freedom
## AIC: 22535
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-multiple-logistic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The multiple Logistic regression Model:&lt;/h2&gt;
&lt;p&gt;From the model we can see that the significance of the some of the variables have been changed to less significant or not even significant at all where the significant variables have three stars beside them (P&amp;lt;0.05) and the less significant variables have two or one star only while the not significant variables have no stars at all.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ ., family = binomial, data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.0852  -0.3691  -0.2436  -0.1456   3.2933  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)        -2.304e+00  2.720e-01  -8.469  &amp;lt; 2e-16 ***
## age                -1.211e-03  2.563e-03  -0.472 0.636697    
## jobblue-collar     -2.650e-01  8.448e-02  -3.137 0.001706 ** 
## jobentrepreneur    -2.381e-01  1.450e-01  -1.642 0.100498    
## jobhousemaid       -5.144e-01  1.576e-01  -3.263 0.001102 ** 
## jobmanagement      -1.135e-01  8.532e-02  -1.331 0.183270    
## jobretired          2.829e-01  1.145e-01   2.472 0.013442 *  
## jobself-employed   -2.433e-01  1.289e-01  -1.887 0.059228 .  
## jobservices        -2.076e-01  9.769e-02  -2.125 0.033551 *  
## jobstudent          3.205e-01  1.281e-01   2.502 0.012336 *  
## jobtechnician      -1.736e-01  8.012e-02  -2.166 0.030285 *  
## jobunemployed      -1.602e-01  1.286e-01  -1.245 0.212992    
## jobunknown         -4.341e-01  2.910e-01  -1.492 0.135709    
## maritalmarried     -2.019e-01  6.818e-02  -2.961 0.003065 ** 
## maritalsingle       5.891e-02  7.802e-02   0.755 0.450211    
## educationsecondary  2.114e-01  7.560e-02   2.796 0.005168 ** 
## educationtertiary   3.631e-01  8.787e-02   4.132 3.60e-05 ***
## educationunknown    2.336e-01  1.219e-01   1.917 0.055238 .  
## defaultyes         -1.291e-01  1.905e-01  -0.678 0.498028    
## balance             1.795e-05  6.039e-06   2.972 0.002957 ** 
## housingyes         -6.490e-01  5.115e-02 -12.687  &amp;lt; 2e-16 ***
## loanyes            -4.321e-01  6.948e-02  -6.220 4.97e-10 ***
## contacttelephone   -2.149e-01  8.922e-02  -2.408 0.016021 *  
## contactunknown     -1.711e+00  8.665e-02 -19.743  &amp;lt; 2e-16 ***
## day10               2.605e-01  2.375e-01   1.097 0.272642    
## day11              -2.541e-01  2.116e-01  -1.200 0.229957    
## day12               1.451e-01  2.063e-01   0.703 0.481974    
## day13               2.497e-01  2.085e-01   1.197 0.231157    
## day14              -8.085e-02  2.098e-01  -0.385 0.699887    
## day15               2.320e-02  2.059e-01   0.113 0.910287    
## day16              -6.925e-02  2.097e-01  -0.330 0.741199    
## day17              -7.200e-01  2.107e-01  -3.417 0.000634 ***
## day18              -3.113e-01  2.051e-01  -1.518 0.129039    
## day19              -8.877e-01  2.303e-01  -3.855 0.000116 ***
## day2               -3.155e-01  2.127e-01  -1.483 0.138033    
## day20              -6.166e-01  2.101e-01  -2.935 0.003340 ** 
## day21              -2.021e-01  2.124e-01  -0.952 0.341316    
## day22              -9.651e-02  2.223e-01  -0.434 0.664198    
## day23               3.233e-01  2.288e-01   1.413 0.157706    
## day24              -1.901e-01  2.624e-01  -0.724 0.468894    
## day25              -5.974e-02  2.292e-01  -0.261 0.794402    
## day26               1.647e-01  2.279e-01   0.723 0.469790    
## day27               5.321e-01  2.205e-01   2.413 0.015805 *  
## day28              -1.186e-01  2.230e-01  -0.532 0.594785    
## day29              -3.486e-01  2.244e-01  -1.553 0.120368    
## day3               -2.349e-01  2.146e-01  -1.095 0.273676    
## day30               2.731e-01  2.073e-01   1.317 0.187693    
## day31              -4.008e-01  3.084e-01  -1.300 0.193692    
## day4               -1.461e-01  2.065e-01  -0.708 0.479104    
## day5               -3.837e-01  2.069e-01  -1.855 0.063578 .  
## day6               -3.904e-01  2.110e-01  -1.850 0.064293 .  
## day7               -6.767e-01  2.180e-01  -3.103 0.001913 ** 
## day8               -1.466e-01  2.074e-01  -0.707 0.479685    
## day9               -8.388e-02  2.137e-01  -0.392 0.694708    
## monthaug           -8.271e-01  9.897e-02  -8.357  &amp;lt; 2e-16 ***
## monthdec            6.842e-01  2.095e-01   3.265 0.001093 ** 
## monthfeb           -2.137e-01  1.120e-01  -1.908 0.056345 .  
## monthjan           -1.284e+00  1.537e-01  -8.354  &amp;lt; 2e-16 ***
## monthjul           -9.017e-01  9.562e-02  -9.430  &amp;lt; 2e-16 ***
## monthjun            4.794e-01  1.127e-01   4.255 2.09e-05 ***
## monthmar            1.375e+00  1.467e-01   9.373  &amp;lt; 2e-16 ***
## monthmay           -5.297e-01  9.272e-02  -5.713 1.11e-08 ***
## monthnov           -6.742e-01  1.085e-01  -6.213 5.20e-10 ***
## monthoct            7.727e-01  1.296e-01   5.961 2.51e-09 ***
## monthsep            8.835e-01  1.423e-01   6.210 5.31e-10 ***
## duration            4.315e-03  7.605e-05  56.743  &amp;lt; 2e-16 ***
## campaign           -7.668e-02  1.149e-02  -6.673 2.51e-11 ***
## pdays               2.190e-04  3.497e-04   0.626 0.531150    
## previous            9.059e-03  6.566e-03   1.380 0.167650    
## poutcomeother       2.173e-01  1.048e-01   2.072 0.038242 *  
## poutcomesuccess     2.269e+00  9.590e-02  23.666  &amp;lt; 2e-16 ***
## poutcomeunknown     5.218e-02  1.079e-01   0.484 0.628580    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 15898  on 33836  degrees of freedom
## AIC: 16042
## 
## Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluating-our-multiple-logistic-regression-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluating our multiple Logistic regression Model&lt;/h2&gt;
&lt;p&gt;In this part we will use the test data to pass it to the multiple Logistic regression Model in order to classify whether clients associated with the given information in the test data set will subscribe to a term deposit. Also we will evaluate the performance of the model through using the confusion matrix&lt;/p&gt;
&lt;p&gt;After passing the test data set that represent 25% of the original data set to the developed multiple Logistic regression Model and evaluating the classification output with reference to the original output we got accuracy of 90.2g%.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## 0.0000337 0.0181261 0.0413998 0.1176430 0.1112072 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-54-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 x 3
##    .metric              .estimator .estimate
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
##  1 accuracy             binary        0.902 
##  2 kap                  binary        0.404 
##  3 sens                 binary        0.350 
##  4 spec                 binary        0.974 
##  5 ppv                  binary        0.640 
##  6 npv                  binary        0.920 
##  7 mcc                  binary        0.426 
##  8 j_index              binary        0.324 
##  9 bal_accuracy         binary        0.662 
## 10 detection_prevalence binary        0.0633
## 11 precision            binary        0.640 
## 12 recall               binary        0.350 
## 13 f_meas               binary        0.453&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;updating-the-model-with-the-significant-parameters-only&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Updating the model with the significant parameters only&lt;/h2&gt;
&lt;p&gt;In this part we will update our model to include only the significant predictors in the process of classification.&lt;/p&gt;
&lt;p&gt;Based on the obtained p-values from the previous model we can see that out of 17 predictors, 9 are considered the significant predictors. Accordingly our model would be updated to include the 9 significant predictors only.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;updated_mdl&amp;lt;- update (full_mdl , ~ job+ education +housing+ loan+contact +month+duration+campaign+poutcome)
summary(updated_mdl)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = y ~ job + education + housing + loan + contact + 
##     month + duration + campaign + poutcome, family = binomial, 
##     data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -4.9645  -0.3750  -0.2534  -0.1489   3.2598  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept)        -2.517e+00  1.290e-01 -19.508  &amp;lt; 2e-16 ***
## jobblue-collar     -3.312e-01  8.352e-02  -3.966 7.32e-05 ***
## jobentrepreneur    -3.390e-01  1.433e-01  -2.366 0.017980 *  
## jobhousemaid       -5.679e-01  1.562e-01  -3.635 0.000278 ***
## jobmanagement      -1.595e-01  8.434e-02  -1.892 0.058548 .  
## jobretired          2.015e-01  1.007e-01   2.002 0.045247 *  
## jobself-employed   -2.792e-01  1.275e-01  -2.190 0.028496 *  
## jobservices        -2.385e-01  9.671e-02  -2.467 0.013640 *  
## jobstudent          4.857e-01  1.218e-01   3.988 6.67e-05 ***
## jobtechnician      -1.873e-01  7.943e-02  -2.358 0.018376 *  
## jobunemployed      -1.650e-01  1.274e-01  -1.295 0.195302    
## jobunknown         -5.284e-01  2.871e-01  -1.841 0.065681 .  
## educationsecondary  2.271e-01  7.440e-02   3.053 0.002267 ** 
## educationtertiary   4.319e-01  8.568e-02   5.041 4.63e-07 ***
## educationunknown    2.634e-01  1.203e-01   2.190 0.028551 *  
## housingyes         -7.095e-01  4.994e-02 -14.207  &amp;lt; 2e-16 ***
## loanyes            -4.660e-01  6.869e-02  -6.783 1.18e-11 ***
## contacttelephone   -2.023e-01  8.762e-02  -2.309 0.020922 *  
## contactunknown     -1.711e+00  8.430e-02 -20.291  &amp;lt; 2e-16 ***
## monthaug           -8.428e-01  8.971e-02  -9.395  &amp;lt; 2e-16 ***
## monthdec            6.012e-01  2.038e-01   2.950 0.003178 ** 
## monthfeb           -2.390e-01  9.700e-02  -2.464 0.013729 *  
## monthjan           -1.179e+00  1.389e-01  -8.487  &amp;lt; 2e-16 ***
## monthjul           -8.314e-01  8.806e-02  -9.441  &amp;lt; 2e-16 ***
## monthjun            4.221e-01  1.039e-01   4.061 4.88e-05 ***
## monthmar            1.447e+00  1.408e-01  10.278  &amp;lt; 2e-16 ***
## monthmay           -3.879e-01  8.153e-02  -4.758 1.96e-06 ***
## monthnov           -8.996e-01  9.671e-02  -9.302  &amp;lt; 2e-16 ***
## monthoct            8.534e-01  1.251e-01   6.822 8.97e-12 ***
## monthsep            9.127e-01  1.366e-01   6.682 2.35e-11 ***
## duration            4.278e-03  7.526e-05  56.841  &amp;lt; 2e-16 ***
## campaign           -7.553e-02  1.130e-02  -6.682 2.35e-11 ***
## poutcomeother       2.521e-01  1.034e-01   2.438 0.014763 *  
## poutcomesuccess     2.346e+00  9.203e-02  25.491  &amp;lt; 2e-16 ***
## poutcomeunknown    -1.179e-02  6.651e-02  -0.177 0.859258    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24531  on 33907  degrees of freedom
## Residual deviance: 16143  on 33873  degrees of freedom
## AIC: 16213
## 
## Number of Fisher Scoring iterations: 6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluating-the-updated-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Evaluating the updated Model&lt;/h2&gt;
&lt;p&gt;After passing the test data set to the updated model and evaluating the classification output with respect to the observed values we got accuracy of 90.3% which is higher than the accuracy that we got by the model which includes all the predictors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## 0.0000337 0.0181261 0.0413998 0.1176430 0.1112072 1.0000000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/a02-classification-model/index.en_files/figure-html/unnamed-chunk-56-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 x 3
##    .metric              .estimator .estimate
##    &amp;lt;chr&amp;gt;                &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;
##  1 accuracy             binary        0.903 
##  2 kap                  binary        0.405 
##  3 sens                 binary        0.349 
##  4 spec                 binary        0.975 
##  5 ppv                  binary        0.645 
##  6 npv                  binary        0.920 
##  7 mcc                  binary        0.428 
##  8 j_index              binary        0.324 
##  9 bal_accuracy         binary        0.662 
## 10 detection_prevalence binary        0.0626
## 11 precision            binary        0.645 
## 12 recall               binary        0.349 
## 13 f_meas               binary        0.453&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;By comparing the accuracy associated with the first model that used all the predictors to the second model that used only the significant predictors, The accuracy associated with the second model is considered higher than the first one and the second model is considered less computationally expensive than the first model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Assignment A01  </title>
      <link>/post/assignment-a01/</link>
      <pubDate>Sat, 10 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/post/assignment-a01/</guid>
      <description>
&lt;script src=&#34;/post/assignment-a01/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-diamond-ring-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The diamond ring data&lt;/h2&gt;
&lt;p&gt;In this blog we used the diamond ring data. Getting a glimpse into the data we can see that it is composed of 440 rows where each row represents a diamond ring and for each ring the diamond characteristics are provided that are Color, Cut, Carat Weight, Clarity , Polish, Symmetry, and certification in addition to the price.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 440
## Columns: 9
## $ carat         &amp;lt;dbl&amp;gt; 0.92, 0.92, 0.82, 0.81, 0.90, 0.87, 0.80, 0.84, 0.80, 0.~
## $ colour        &amp;lt;chr&amp;gt; &amp;quot;I&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;J&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;F&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;D&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;~
## $ clarity       &amp;lt;chr&amp;gt; &amp;quot;SI2&amp;quot;, &amp;quot;SI2&amp;quot;, &amp;quot;SI2&amp;quot;, &amp;quot;SI1&amp;quot;, &amp;quot;VS2&amp;quot;, &amp;quot;SI2&amp;quot;, &amp;quot;SI2&amp;quot;, &amp;quot;SI1&amp;quot;, ~
## $ cut           &amp;lt;chr&amp;gt; &amp;quot;G&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;I&amp;quot;, &amp;quot;~
## $ certification &amp;lt;chr&amp;gt; &amp;quot;AGS&amp;quot;, &amp;quot;AGS&amp;quot;, &amp;quot;GIA&amp;quot;, &amp;quot;GIA&amp;quot;, &amp;quot;GIA&amp;quot;, &amp;quot;AGS&amp;quot;, &amp;quot;GIA&amp;quot;, &amp;quot;GIA&amp;quot;, ~
## $ polish        &amp;lt;chr&amp;gt; &amp;quot;V&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;~
## $ symmetry      &amp;lt;chr&amp;gt; &amp;quot;V&amp;quot;, &amp;quot;G&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;X&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;~
## $ price         &amp;lt;dbl&amp;gt; 3000, 3000, 3004, 3004, 3006, 3007, 3008, 3010, 3012, 30~
## $ wholesaler    &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,~&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-univriate-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Univriate Analysis&lt;/h2&gt;
&lt;p&gt;we will start our analysis with the metric data which are the price and the carat mass of the diamond ring then we will analyse the non metric data which are the colour, clarity, cut, certification, polish, symmetry, and wholesaler.&lt;/p&gt;
&lt;div id=&#34;the-price&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Price&lt;/h3&gt;
&lt;p&gt;From the following plots and provided summary, we can see that the diamond ring price ranges from minimum value of $160 to maximum value of $3145, the mean of the price is $1717, where in most of the diamond ring purchases (mode) the price was $520 and we can also see that the first quartile is $520, the third quartile is $3012 and the IQR is $2429 with median price of $2169.&lt;/p&gt;
&lt;p&gt;The price dispersion can be evaluated through a variance of $1382245 and standard deviation of $1175.689.&lt;/p&gt;
&lt;p&gt;The data distribution has no specific shape but we can notice that there is no diamond ring was sold in the price range from $700 to $1700 similarly at price of $2900.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     160     520    2169    1717    3012    3145&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1382245&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1175.689&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-carat-mass&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Carat mass&lt;/h3&gt;
&lt;p&gt;From the following plots and provided summary, we can see that the diamond ring carat mass ranges from minimum value of 0.09 gram to maximum value of 1.58 gram, the mean of the carat mass is 0.6693 gram, where in most of the diamond ring purchases (mode) the carat mass was 0.3 gram and we can also see that the first quartile is 0.3 gram, the third quartile is 1.01 gram and the IQR is 0.71 gram with median carat mass of 0.81 gram.&lt;/p&gt;
&lt;p&gt;The carat mass dispersion can be evaluated through a variance of 0.144248 grams and standard deviation of 0.3797999 grams.&lt;/p&gt;
&lt;p&gt;The data distribution has no specific shape but we can notice that there is no carat mass was sold in the carat mass range from 0.4 gram to 0.7 gram similarly at carat mass of 1.5 grams.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0900  0.3000  0.8100  0.6693  1.0100  1.5800&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.144248&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.3797999&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-colour&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Colour&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the color, we can see that the most purchased color for the diamond rings is the color I and after that are color J and color H and the least purchased are color L and color D.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-clarity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Clarity&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the clarity, we can see that the most purchased clarity type for the diamond rings is the clarity Sl1 and after that is clarity Sl2 and the least purchased are WS1 and Ws2.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-cut&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Cut&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the diamond ring cut, we can see that the most purchased cut type for the diamond rings is X and after that is cut V then cut I while the least purchased cuts are cut G and cut F.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-certification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Certification&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the diamond ring certification, we can see that the most purchased certification type for the diamond rings is GIA and after that is EGL then IGI while the least purchased certifications are DOW and AGS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-polish&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Polish&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the diamond ring polish, we can see that the most purchased polish type for the diamond rings is V and after that is G then X while the least purchased polish types are V,I and F.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-symmetry&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Symmetry&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the diamond ring symmetry, we can see that the most purchased symmetry type for the diamond rings is V and after that is G then X while the least purchased symmetry types are I and F.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;wholesaler&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Wholesaler&lt;/h3&gt;
&lt;p&gt;From the histogram plot of the diamond ring wholesaler, we can see that the most of the purchased diamond rings were purchased from wholesaler number 3 then wholesaler number 2, while the fewer purchases done at the wholesaler number 1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-bivariate-analysis-and-regressision-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Bivariate Analysis and regressision models&lt;/h2&gt;
&lt;div id=&#34;the-simple-regression-model-for-the-price-vs.-the-carat-mass&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The simple regression model for the price Vs. the carat mass&lt;/h3&gt;
&lt;p&gt;The correlation between the price and the carat mass of the diamond ring is 0.925436 that means that the two variables have a perfect positive relationship so that when the carat mass increases the price increase and vice versa.&lt;/p&gt;
&lt;p&gt;The covariance between the price and the carat mass of the diamond ring is 413.2318 which shows the extent to which the two variables change in tandem.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.925436&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 413.2318&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the following plot, a simple linear regression model has been fitted that can help in the prediction of the price of the diamond ring based on the carat mass.&lt;/p&gt;
&lt;p&gt;The simple linear regression model of the price Vs. the carat mass can be represented by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Y}_{i} = \beta_0 + \beta_1 \times CaratMass_{i}\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} =-200.5 + 2864.7 \times Carat Mass\]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.925436&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 413.2318&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01/index.en_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = price ~ carat, data = data)
## 
## Coefficients:
## (Intercept)        carat  
##      -200.5       2864.7&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simple-regression-model-for-the-price-vs.-the-colour&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The simple regression model for the price Vs. the colour&lt;/h3&gt;
&lt;p&gt;The correlation between the price and the color of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the colour of the diamond ring is very weak.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##              Df    Sum Sq Mean Sq F value   Pr(&amp;gt;F)    
## colour        8  51620143 6452518   5.009 5.87e-06 ***
## Residuals   431 555185378 1288133                     
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simple linear regression model of the price Vs. the color can be represented by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Y} = \beta_0 + \beta_1 \times colorE+ \beta_2 \times colorF+ \beta_3 \times colorG+ \beta_4\times colorH+ \beta_5 \times colorI+ \beta_6 \times colorJ+ \beta_7 \times colorK+ \beta_8 \times colorL\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} =2316.20 -764.29 \times colorE -982.36 \times colorF -147.73 \times colorG -873.92\times colorH -765.88 \times colorI -535.28 \times colorJ+ 42.06 \times colorK+  52.30\times colorL\]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = price ~ colour, data = data)
## 
## Coefficients:
## (Intercept)      colourE      colourF      colourG      colourH      colourI  
##     2316.20      -764.29      -982.36      -147.73      -873.92      -765.88  
##     colourJ      colourK      colourL  
##     -535.28        42.06        52.30&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simple-regression-model-for-the-price-vs.-the-clarity&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The simple regression model for the price Vs. the clarity&lt;/h3&gt;
&lt;p&gt;The correlation between the price and the clarity of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the clarity of the diamond ring is very weak.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##              Df    Sum Sq  Mean Sq F value Pr(&amp;gt;F)    
## clarity       8 194938677 24367335    25.5 &amp;lt;2e-16 ***
## Residuals   431 411866844   955608                   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simple linear regression model of the price Vs. the clarity can be represented by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Y} = \beta_0 + \beta_1 \times clarityI2 + \beta_2 \times claritySI1+ \beta_3 \times claritySI2+ \beta_4\times claritySI3 + \beta_5 \times clarityVS1+ \beta_6 \times clarityVS2+ \beta_7 \times clarityVVS1 + \beta_8 \times clarityVVS2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} = 2543.15 -201.22 \times clarityI2 -1495.53  \times claritySI1-568.95 \times claritySI2+ 76.24\times claritySI3 -1405.38 \times clarityVS1-1654.95 \times clarityVS2-1996.15 \times clarityVVS1 -1978.95 \times clarityVVS2\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(price ~ clarity, data = data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = price ~ clarity, data = data)
## 
## Coefficients:
## (Intercept)    clarityI2   claritySI1   claritySI2   claritySI3   clarityVS1  
##     2543.15      -201.22     -1495.53      -568.95        76.24     -1405.38  
##  clarityVS2  clarityVVS1  clarityVVS2  
##    -1654.95     -1996.15     -1978.95&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simple-regression-model-for-the-price-vs.-cut&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The simple regression model for the price Vs. cut&lt;/h3&gt;
&lt;p&gt;The correlation between the price and the cut of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the cut of the diamond ring is very weak.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##              Df    Sum Sq  Mean Sq F value   Pr(&amp;gt;F)    
## cut           4  66164570 16541142   13.31 3.07e-10 ***
## Residuals   435 540640951  1242853                     
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simple linear regression model of the price Vs. the cut can be represented by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Y} = \beta_0 + \beta_1 \times Cut G+ \beta_2 \times Cut I+ \beta_3 \times Cut V+ \beta_4\times Cut X\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} =2455.2-409.2  \times Cut G -723.5 \times Cut I-1277.1 \times Cut V -797.2\times Cut X \]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = price ~ cut, data = data)
## 
## Coefficients:
## (Intercept)         cutG         cutI         cutV         cutX  
##      2455.2       -409.2       -723.5      -1277.1       -797.2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simple-regression-model-for-the-price-vs.-the-certification&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The simple regression model for the price Vs. the certification&lt;/h3&gt;
&lt;p&gt;The correlation between the price and the certification of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the certification of the diamond ring is very weak.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                Df    Sum Sq  Mean Sq F value Pr(&amp;gt;F)    
## certification   4 238869617 59717404    70.6 &amp;lt;2e-16 ***
## Residuals     435 367935903   845830                   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simple linear regression model of the price Vs. the certification can be represented by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Y} = \beta_0 + \beta_1 \times certificationDOW  + \beta_2 \times certificationEGL  + \beta_3 \times certificationGIA  + \beta_4\times certificationIGI  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} = 3033.4 -1002.4 \times certificationDOW  -355.6 \times certificationEGL  -1573.6 \times certificationGIA  -2767.9 \times certificationIGI  \]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = price ~ certification, data = data)
## 
## Coefficients:
##      (Intercept)  certificationDOW  certificationEGL  certificationGIA  
##           3033.4           -1002.4            -355.6           -1573.6  
## certificationIGI  
##          -2767.9&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simple-regression-model-for-the-price-vs.-the-polish&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The simple regression model for the price Vs. the polish&lt;/h3&gt;
&lt;p&gt;The correlation between the price and the polish of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the certification of the diamond ring is very weak.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##              Df    Sum Sq Mean Sq F value   Pr(&amp;gt;F)    
## polish        5  28574143 5714829   4.289 0.000808 ***
## Residuals   434 578231378 1332330                     
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simple linear regression model of the price Vs. the polish can be represented by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Y} = \beta_0 + \beta_1 \times  polishG   + \beta_2 \times  polishI   + \beta_3 \times  polishV   + \beta_4\times  polishV + \beta_5\times  polishX  \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} =  2318.6 -404.0 \times  polishG   + 728.8 \times  polishI   + 762.4  \times  polishV   -715.5\times  polishV -939.8 \times  polishX  \]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lm(price ~ polish, data = data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = price ~ polish, data = data)
## 
## Coefficients:
## (Intercept)      polishG      polishI      polishv      polishV      polishX  
##      2318.6       -404.0        728.8        762.4       -715.5       -939.8&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simple-regression-model-for-the-price-vs.-the-symmetry&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The simple regression model for the price Vs. the symmetry&lt;/h3&gt;
&lt;p&gt;The correlation between the price and the symmetry of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the symmetry of the diamond ring is very weak.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##              Df    Sum Sq Mean Sq F value   Pr(&amp;gt;F)    
## symmetry      4  37655698 9413925   7.195 1.29e-05 ***
## Residuals   435 569149823 1308390                     
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simple linear regression model of the price Vs. the symmetry can be represented by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Y} = \beta_0 + \beta_1 \times symmetryG  + \beta_2 \times  symmetryI   + \beta_3 \times  symmetryV   + \beta_4\times  symmetryX\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} = 2432.3 -537.9 \times symmetryG+615.1 \times  symmetryI -966.8 \times  symmetryV  -673.0\times  symmetryX\]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = price ~ symmetry, data = data)
## 
## Coefficients:
## (Intercept)    symmetryG    symmetryI    symmetryV    symmetryX  
##      2432.3       -537.9        615.1       -966.8       -673.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-simple-regression-model-for-the-price-vs.-the-wholesaler&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The simple regression model for the price Vs. the wholesaler&lt;/h3&gt;
&lt;p&gt;The correlation between the price and the wholesaler of the diamond ring is shown below, based on the shown P-value we can see that the relationship between the price and the certification of the diamond ring is very weak.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;##                     Df    Sum Sq   Mean Sq F value Pr(&amp;gt;F)    
## factor(wholesaler)   2 578213494 289106747    4419 &amp;lt;2e-16 ***
## Residuals          437  28592027     65428                   
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The simple linear regression model of the price Vs. the wholesaler can be represented by:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Y} = \beta_0 + \beta_1 \times wholesaler1  + \beta_2 \times  wholesaler2 \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} =  3043.2 -381.2 \times wholesaler1  -2575.1 \times  wholesaler2 \]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = price ~ factor(wholesaler), data = data)
## 
## Coefficients:
##         (Intercept)  factor(wholesaler)2  factor(wholesaler)3  
##              3043.2               -381.2              -2575.1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-multiple-linear-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The multiple linear regression:&lt;/h3&gt;
&lt;p&gt;The multiple linear regression model can be achieved by using the following betas, so that we can predict the price of the diamond ring taking in consideration the all the variables either metric or non metric.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = price ~ carat + colour + clarity + cut + certification + 
##     polish + symmetry + factor(wholesaler), data = data)
## 
## Coefficients:
##         (Intercept)                carat              colourE  
##              788.88              1885.30              -166.61  
##             colourF              colourG              colourH  
##             -247.15              -229.60              -299.84  
##             colourI              colourJ              colourK  
##             -337.63              -434.39              -696.43  
##             colourL            clarityI2           claritySI1  
##             -895.44              -590.10               652.28  
##          claritySI2           claritySI3           clarityVS1  
##              560.51               290.46               743.08  
##          clarityVS2          clarityVVS1          clarityVVS2  
##              689.50              1015.21               760.50  
##                cutG                 cutI                 cutV  
##               48.82                84.51                78.45  
##                cutX     certificationDOW     certificationEGL  
##               93.37              -271.24              -307.29  
##    certificationGIA     certificationIGI              polishG  
##               12.16              -118.27                66.57  
##             polishI              polishv              polishV  
##              247.70               136.45                78.40  
##             polishX            symmetryG            symmetryI  
##               84.85               133.37                   NA  
##           symmetryV            symmetryX  factor(wholesaler)2  
##              151.49               137.40               112.07  
## factor(wholesaler)3  
##            -1460.67&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the above analysis, we can see that the price of the diamond ring depends mainly on the carat mass and the relationship between these two variables are strong and positive so that as the carat mass increase as the price of the diamond ring increase and vice versa.&lt;/p&gt;
&lt;p&gt;To evaluate the professor’s ring that he is interested on buying it we can use the multiple regression model mentioned above that result in estimated price of :&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} = 788.88 + 1885.30\times 0.9(carat) -434.39  (colorJ)+560.51  (ClaritySL2)+78.45  (Cut V)+ 12.16(Certification GIA)+ 66.57(Polish G) + 151.49(symmetry V)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[\widehat{Price} = $ 2920.44 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The estimated price based on our model is less than the price of the ring with around 180 dollars.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Project </title>
      <link>/post/the-project/</link>
      <pubDate>Mon, 21 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/the-project/</guid>
      <description>
&lt;script src=&#34;/post/the-project/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;section-the-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The dataset&lt;/h2&gt;
&lt;p&gt;The used data set in this project composed of a record of 7 common different fish species in fish market sales. For each fish participated on this record, a certain measurements were taken, which are the fish species, the weight in Gram g, vertical length in cm, diagonal length in cm, cross length in cm, height in cm and diagonal width in cm.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fish.png&#34; width=&#34;98%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The data was obtained from &lt;strong&gt;Kaggle &lt;/strong&gt; website, to access the source of the data use the link below&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/aungpyaeap/fish-market&#34; target=&#34;_blank&#34;&gt;Link to the dataset&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-explanatory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Explanatory data analysis&lt;/h2&gt;
&lt;p&gt;In this part we are going to create a data profiling report, in this report we can get an overview of the shape and structure of our dataset by summarizing their main characters and use statistical graphics and other data visualization methods.&lt;/p&gt;
&lt;iframe height=&#34;800&#34; width=&#34;100%&#34; frameborder=&#34;no&#34; src=&#34;./report.html/&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;Based on the data profiling report, specifically in the Univariate Distribution Histogram part , We can see that there are some fishes in our data set with zero weights, we need to remove the rows where the weight equal to zero.&lt;/p&gt;
&lt;p&gt;To access the &lt;strong&gt;Data Profiling Report&lt;/strong&gt; in a new window please press the link below
&lt;a href=&#34;./report.html&#34; target=&#34;_blank&#34;&gt;Link to open the data profiling report in a new window&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this part we can interactively perform further exploration to our data set.&lt;/p&gt;
&lt;iframe height=&#34;1000&#34; width=&#34;100%&#34; frameborder=&#34;no&#34; src=&#34;https://nouran.shinyapps.io/2021-06-21-project/&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;To open the &lt;strong&gt;Shiny app&lt;/strong&gt; in a new window please press the link below
&lt;a href=&#34;https://nouran.shinyapps.io/2021-06-21-project/&#34; target=&#34;_blank&#34;&gt;Link to open the Shiny app in a new window&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-fitting-a-model-to-predict-the-fish-height-based-on-its-cross-length&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Fitting a model to predict the fish height based on it’s cross length&lt;/h1&gt;
&lt;div id=&#34;section-linear-regression-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Linear Regression Model&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To create a liner regression model to predict the height of a fish given its cross length, we need to determine the value of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; that is the population parameter for the intercept and the value of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; that is the population parameter for the slope as shown in the following model:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Height}_{i} = \beta_0 + \beta_1 \times Cross Length_{i}\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;After determining the values of both &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, the updated linear regression model with the values of &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; is shown as follow:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Height}_{i} = 0.87 + 0.26 \times Cross Length_{i}\]&lt;/span&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;From the above model we can see that :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slope: For each additional cm the fish is longer, the height is expected to be higher, on average, by 0.26 cm.&lt;/li&gt;
&lt;li&gt;Intercept: fish that is 8.8 cm tall (minimum cross length value among the dataset) is expected to be 3.2 cm high, on average.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;section-the-data-is-shown-in-the-following-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The data is shown in the following plot:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/the-project/index_files/figure-html/height-cross_length-plot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-the-data-and-the-least-square-line-are-shown-in-the-following-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The data and the least square line are shown in the following plot:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/the-project/index_files/figure-html/heightcross_length-plot-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-the-data-the-least-square-line-and-the-residuales-are-shown-in-the-following-plot&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The data, the least square line and the residuales are shown in the following plot:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/the-project/index_files/figure-html/vis-res-1-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A05  Exploratory Data Analysis</title>
      <link>/post/a05-exploratory-data-analysis/</link>
      <pubDate>Wed, 16 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/a05-exploratory-data-analysis/</guid>
      <description>
&lt;script src=&#34;/post/a05-exploratory-data-analysis/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;section-the-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The dataset&lt;/h2&gt;
&lt;p&gt;The data used in this blog was obtained from The &lt;strong&gt;World Bank Data&lt;/strong&gt;, to access the source of the data use the link below&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://data.worldbank.org/topic/health?view=chart&#34; target=&#34;_blank&#34;&gt;Link to the data set&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-initial-analysis-questions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Initial Analysis Questions&lt;/h2&gt;
&lt;p&gt;1- which country in North America with the highest population in 2019?&lt;/p&gt;
&lt;p&gt;2- which country in North America has the highest population of elderly in 2019?&lt;/p&gt;
&lt;p&gt;3- which country in North America has the highest birth rate in 2019?&lt;/p&gt;
&lt;p&gt;4- which country in North America has the highest mortality rate in 2019?&lt;/p&gt;
&lt;p&gt;5- How does the population in Canada change over time?&lt;/p&gt;
&lt;p&gt;6- The difference between age distribution in Canada in 2000 and that in 2019?&lt;/p&gt;
&lt;p&gt;7- Is the modern technology that is present in all life aspects including healthcare affect the human lifespan in Canada?&lt;/p&gt;
&lt;p&gt;8- Is the modern lifestyle in Canada affect the adolescent mind regarding giving birth in this phase of their life?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-explanatory-data-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Explanatory data analysis&lt;/h2&gt;
&lt;div id=&#34;section-data-profiling-report-in-this-report-we-can-get-an-overview-of-the-shape-and-structure-of-our-dataset-by-summarizing-their-main-characters-and-use-statistical-graphics-and-other-data-visualization-methods.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Data profiling report, in this report we can get an overview of the shape and structure of our dataset by summarizing their main characters and use statistical graphics and other data visualization methods.&lt;/h3&gt;
&lt;iframe height=&#34;800&#34; width=&#34;100%&#34; frameborder=&#34;no&#34; src=&#34;./report.html/&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;To access the &lt;strong&gt;Data Profiling Report&lt;/strong&gt; in a new window please press the link below
&lt;a href=&#34;./report.html&#34; target=&#34;_blank&#34;&gt;Link to open the data profiling report in a new window&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-in-this-part-we-can-analyze-the-dataset-by-visually-explore-each-indicator-in-each-country&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;in this part we can analyze the dataset by visually explore each indicator in each country&lt;/h3&gt;
&lt;iframe height=&#34;800&#34; width=&#34;100%&#34; frameborder=&#34;no&#34; src=&#34;https://nouran.shinyapps.io/2021-06-16-a05-exploratory-data-analysis/&#34;&gt;
&lt;/iframe&gt;
&lt;p&gt;To open the &lt;strong&gt;Shiny app&lt;/strong&gt; in a new window please press the link below
&lt;a href=&#34;https://nouran.shinyapps.io/2021-06-16-a05-exploratory-data-analysis/&#34; target=&#34;_blank&#34;&gt;Link to open the Shiny app in a new window&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;section-question-1-whats-the-country-with-the-highest-population-in-north-america-in-2019&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 1: what’s the country with the highest population in North America in 2019?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/a05-exploratory-data-analysis/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
From Figure(1), we can see that in 2019 the United States of America has the highest population of 328239523 among the countries of North America, then Mexico that has a population of 127575529 then Canada that has population of 37593384.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-question-2-which-country-in-north-america-has-the-highest-population-of-elderly-in-2019&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 2: which country in North America has the highest population of elderly in 2019?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/a05-exploratory-data-analysis/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;from Figure(2) we can see that the elderly population among the North America countries follows the same ranking of the total population we did in question 1 , so that in 2019 the United States of America has the highest elderly population of 53206334, then Mexico that has elderly population of 9461844 then Canada that has elderly population of 6634442.&lt;/p&gt;
&lt;p&gt;Also from Figure (2), we can see the portion of the elderly population among the whole population in each country.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-question-3-which-country-in-north-america-has-the-highest-birth-rate-in-2019&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 3: which country in North America has the highest birth rate in 2019?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/a05-exploratory-data-analysis/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To answer this question we will use the indicator of fertility rate that is defined as the total births per woman in the country as we can see in Figure(3), using the fertility rate as an indicator to the birth rate, we can see that Haiti has the highest fertility rate in 2019 among the North America countries of value 2.89. Also we can see that the fertility rate in Canada was 1.47 in 2019.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-question-4-which-country-in-north-america-has-the-highest-mortality-rate-in-2019&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 4: which country in North America has the highest mortality rate in 2019?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/a05-exploratory-data-analysis/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To answer this question we will use the indicator of Death Rate Crude Per 1000 People in the country, as we can see in Figure(4), using the Death Rate Crude Per 1000 People as an indicator to the mortality rate, we can see that Grenada has the highest mortality rate in 2019 among the North America countries of value 9.58. Also we can see that in 2019 the death rate in Canada was 7.60 and the lowest death rate was in Honduras of value 4.45.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-question-5-how-does-the-population-in-canada-change-over-time&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 5: How does the population in Canada change over time?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/a05-exploratory-data-analysis/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From Figure (5) shown above , we can see that the population in Canada increases every year so that starting from year 1960 the population was 17909009 and it keeps increasing linearly reaching to population of 30685730 in 2000 and population of 37593384 in 2019.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-question-6-the-difference-between-age-distribution-in-canada-population-in-2000-and-that-in-2019&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 6: the difference between age distribution in Canada population in 2000 and that in 2019?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/a05-exploratory-data-analysis/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;
From Figure (6) shown above , we can see that the age distribution in Canada in 1960 that the zero to 9 age group was contributed more in the population than in 2019 and also in 2019 the plus eighty age group has a larger contribution in the population compared to 1960 where the contribution is almost zero and from 1960 to 2019 the contribution of the two age groups forty to forty nine and fifty to fifty nine increases over time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-question-7-is-the-modern-technology-that-is-present-in-all-life-aspects-including-healthcare-affect-the-human-lifespan-in-canada&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 7: Is the modern technology that is present in all life aspects including healthcare affect the human lifespan in Canada?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/a05-exploratory-data-analysis/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;from Figure(7), we can see that the modern technology present in all life aspects including healthcare affect the human lifespan in Canada, so that human tends to live longer as the population of the age group of eighty and above increases with time and technology.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/a05-exploratory-data-analysis/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can have a closer look by plotting the population of group age 80 and above from 1960 to 2019, as we can see in Figure (8) that there is an increase of this age group in Canada’s population since 1960 and since 2000 this increase became a little higher.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-question-8-is-the-modern-lifestyle-in-canada-affect-the-adolescent-mind-regarding-giving-birth-in-this-phase-of-their-life&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Question 8: Is the modern lifestyle in Canada affect the adolescent mind regarding giving birth in this phase of their life?&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/a05-exploratory-data-analysis/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From Figure (9), we can see that the Adolescent fertility rate in Canada has tremendously decreases over time starting at 1960 the adolescent fertility rate was 55.63 reaching to adolescent fertility rate of 7.68 in 2019. it seems that the modern life style in Canada has the effect to decrease the adolescent women fertility rate.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;section-summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;The modern technology has helped to make human live longer life, although in the other side the modern life affected the adolescent fertility rate in Canada.in 2019 United states of America has the highest population in the North America and comparing it Canada the difference is huge. In 2019 among countries of North America the mortality rate was the least in Honduras while the birth rate was the highest in Haiti.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fitting and Interpreting models </title>
      <link>/post/fitting-and-interpreting-models/</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/fitting-and-interpreting-models/</guid>
      <description>
&lt;script src=&#34;/post/fitting-and-interpreting-models/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/post/fitting-and-interpreting-models/index_files/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/post/fitting-and-interpreting-models/index_files/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;div id=&#34;import-libraries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import Libraries&lt;/h2&gt;
&lt;/div&gt;
&lt;div id=&#34;import-data-paris-paintings&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import Data: Paris Paintings&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pp &amp;lt;- read_csv(&amp;quot;paris-paintings.csv&amp;quot;, na = c(&amp;quot;n/a&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;NA&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -- Column specification --------------------------------------------------------
## cols(
##   .default = col_double(),
##   name = col_character(),
##   sale = col_character(),
##   lot = col_character(),
##   dealer = col_character(),
##   origin_author = col_character(),
##   origin_cat = col_character(),
##   school_pntg = col_character(),
##   price = col_number(),
##   subject = col_character(),
##   authorstandard = col_character(),
##   authorstyle = col_character(),
##   author = col_character(),
##   winningbidder = col_character(),
##   winningbiddertype = col_character(),
##   endbuyer = col_character(),
##   type_intermed = col_character(),
##   Shape = col_character(),
##   material = col_character(),
##   mat = col_character(),
##   materialCat = col_character()
## )
## i Use `spec()` for the full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;goal-predict-height-from-width&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Goal: Predict height from width&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{height}_{i} = \beta_0 + \beta_1 \times width_{i}\]&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/fitting-and-interpreting-models/index_files/figure-html/height-width-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;img src=&#34;tidymodels.png&#34; width=&#34;98%&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;step-1-specify-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 1: Specify model&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linear_reg()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear Regression Model Specification (regression)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;step-2-set-model-fitting-engine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 2: Set model fitting &lt;em&gt;engine&lt;/em&gt;&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;) # lm: linear model&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Linear Regression Model Specification (regression)
## 
## Computational engine: lm&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;step-3-fit-model-estimate-parameters&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Step 3: Fit model &amp;amp; estimate parameters&lt;/h2&gt;
&lt;p&gt;… using &lt;strong&gt;formula syntax&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;) %&amp;gt;%
  fit(Height_in ~ Width_in, data = pp)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## parsnip model object
## 
## Fit time:  0ms 
## 
## Call:
## stats::lm(formula = Height_in ~ Width_in, data = data)
## 
## Coefficients:
## (Intercept)     Width_in  
##      3.6214       0.7808&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;a-closer-look-at-model-output&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A closer look at model output&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## parsnip model object
## 
## Fit time:  0ms 
## 
## Call:
## stats::lm(formula = Height_in ~ Width_in, data = data)
## 
## Coefficients:
## (Intercept)     Width_in  
##      3.6214       0.7808&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;.large[
&lt;span class=&#34;math display&#34;&gt;\[\widehat{height}_{i} = 3.6214 + 0.7808 \times width_{i}\]&lt;/span&gt;]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;a-tidy-look-at-model-output&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A tidy look at model output&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;) %&amp;gt;%
  fit(Height_in ~ Width_in, data = pp) %&amp;gt;%
  tidy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term        estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)    3.62    0.254        14.3 8.82e-45
## 2 Width_in       0.781   0.00950      82.1 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;.large[
&lt;span class=&#34;math display&#34;&gt;\[\widehat{height}_{i} = 3.62 + 0.781 \times width_{i}\]&lt;/span&gt;]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;slope-and-intercept&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Slope and intercept&lt;/h2&gt;
&lt;p&gt;.large[
&lt;span class=&#34;math display&#34;&gt;\[\widehat{height}_{i} = 3.62 + 0.781 \times width_{i}\]&lt;/span&gt;]&lt;/p&gt;
&lt;p&gt;–&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Slope:&lt;/strong&gt; For each additional inch the painting is wider, the height is expected to be higher, on average, by 0.781 inches.&lt;/li&gt;
&lt;/ul&gt;
&lt;table style=&#34;width:4%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;4%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;- &lt;strong&gt;Intercept:&lt;/strong&gt; Paintings that are 0 inches wide are expected to be 3.62 inches high, on average. (Does this make sense?)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;correlation-does-not-imply-causation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Correlation does not imply causation&lt;/h2&gt;
&lt;p&gt;Remember this when interpreting model coefficients&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cell_phones.png&#34; width=&#34;90%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;.footnote[
Source: XKCD, &lt;a href=&#34;https://xkcd.com/925/&#34;&gt;Cell phones&lt;/a&gt;]&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;class: middle&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;parameter-estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Parameter estimation&lt;/h1&gt;
&lt;hr /&gt;
&lt;div id=&#34;linear-model-with-a-single-predictor&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Linear model with a single predictor&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;We’re interested in &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; (population parameter for the intercept) and &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; (population parameter for the slope) in the following model:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y}_{i} = \beta_0 + \beta_1~x_{i}\]&lt;/span&gt;&lt;/p&gt;
&lt;table style=&#34;width:4%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;4%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;- Tough luck, you can’t have them…&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;- So we use sample statistics to estimate them:&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;&lt;span class=&#34;math display&#34;&gt;\[\hat{y}_{i} = b_0 + b_1~x_{i}\]&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;least-squares-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Least squares regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The regression line minimizes the sum of squared residuals.&lt;/li&gt;
&lt;/ul&gt;
&lt;table style=&#34;width:4%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;4%&#34; /&gt;
&lt;/colgroup&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;- If &lt;span class=&#34;math inline&#34;&gt;\(e_i = y_i - \hat{y}_i\)&lt;/span&gt;, then, the regression line minimizes
&lt;span class=&#34;math inline&#34;&gt;\(\sum_{i = 1}^n e_i^2\)&lt;/span&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-residuals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing residuals&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/fitting-and-interpreting-models/index_files/figure-html/vis-res-1-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-residuals-cont.&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing residuals (cont.)&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/fitting-and-interpreting-models/index_files/figure-html/vis-res-2-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;visualizing-residuals-cont.-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualizing residuals (cont.)&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/fitting-and-interpreting-models/index_files/figure-html/vis-res-3-1.png&#34; width=&#34;70%&#34; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;properties-of-least-squares-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Properties of least squares regression&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The regression line goes through the center of mass point, the coordinates corresponding to average &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and average &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\((\bar{x}, \bar{y})\)&lt;/span&gt;:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\bar{y} = b_0 + b_1 \bar{x} ~ \rightarrow ~ b_0 = \bar{y} - b_1 \bar{x}\]&lt;/span&gt;&lt;/p&gt;
&lt;table style=&#34;width:4%;&#34;&gt;
&lt;colgroup&gt;
&lt;col width=&#34;4%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;- The slope has the same sign as the correlation coefficient: &lt;span class=&#34;math inline&#34;&gt;\(b_1 = r \frac{s_y}{s_x}\)&lt;/span&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;- The sum of the residuals is zero: &lt;span class=&#34;math inline&#34;&gt;\(\sum_{i = 1}^n e_i = 0\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;The residuals and &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; values are uncorrelated&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;class: middle&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;models-with-categorical-explanatory-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Models with categorical explanatory variables&lt;/h1&gt;
&lt;hr /&gt;
&lt;div id=&#34;categorical-predictor-with-2-levels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categorical predictor with 2 levels&lt;/h2&gt;
&lt;p&gt;.pull-left-narrow[
.small[&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3,393 x 3
##    name      Height_in landsALL
##    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1 L1764-2          37        0
##  2 L1764-3          18        0
##  3 L1764-4          13        1
##  4 L1764-5a         14        1
##  5 L1764-5b         14        1
##  6 L1764-6           7        0
##  7 L1764-7a          6        0
##  8 L1764-7b          6        0
##  9 L1764-8          15        0
## 10 L1764-9a          9        0
## 11 L1764-9b          9        0
## 12 L1764-10a        16        1
## 13 L1764-10b        16        1
## 14 L1764-10c        16        1
## 15 L1764-11         20        0
## 16 L1764-12a        14        1
## 17 L1764-12b        14        1
## 18 L1764-13a        15        1
## 19 L1764-13b        15        1
## 20 L1764-14         37        0
## # ... with 3,373 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;]
]
.pull-right-wide[
- &lt;code&gt;landsALL = 0&lt;/code&gt;: No landscape features
- &lt;code&gt;landsALL = 1&lt;/code&gt;: Some landscape features]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;height-landscape-features&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Height &amp;amp; landscape features&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;) %&amp;gt;%
  fit(Height_in ~ factor(landsALL), data = pp) %&amp;gt;%
  tidy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 5
##   term              estimate std.error statistic  p.value
##   &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 (Intercept)          22.7      0.328      69.1 0       
## 2 factor(landsALL)1    -5.65     0.532     -10.6 7.97e-26&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;height-landscape-features-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Height &amp;amp; landscape features&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\widehat{Height_{in}} = 22.7 - 5.645~landsALL\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Slope:&lt;/strong&gt; Paintings with landscape features are expected, on average, to be 5.645 inches shorter than paintings that without landscape features
&lt;ul&gt;
&lt;li&gt;Compares baseline level (&lt;code&gt;landsALL = 0&lt;/code&gt;) to the other level (&lt;code&gt;landsALL = 1&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Intercept:&lt;/strong&gt; Paintings that don’t have landscape features are expected, on average, to be 22.7 inches tall&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-between-height-and-school&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship between height and school&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;linear_reg() %&amp;gt;%
  set_engine(&amp;quot;lm&amp;quot;) %&amp;gt;%
  fit(Height_in ~ school_pntg, data = pp) %&amp;gt;%
  tidy()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 5
##   term            estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)        14.0       10.0     1.40  0.162  
## 2 school_pntgD/FL     2.33      10.0     0.232 0.816  
## 3 school_pntgF       10.2       10.0     1.02  0.309  
## 4 school_pntgG        1.65      11.9     0.139 0.889  
## 5 school_pntgI       10.3       10.0     1.02  0.306  
## 6 school_pntgS       30.4       11.4     2.68  0.00744
## 7 school_pntgX        2.87      10.3     0.279 0.780&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;dummy-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dummy variables&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 5
##   term            estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)        14.0       10.0     1.40  0.162  
## 2 school_pntgD/FL     2.33      10.0     0.232 0.816  
## 3 school_pntgF       10.2       10.0     1.02  0.309  
## 4 school_pntgG        1.65      11.9     0.139 0.889  
## 5 school_pntgI       10.3       10.0     1.02  0.306  
## 6 school_pntgS       30.4       11.4     2.68  0.00744
## 7 school_pntgX        2.87      10.3     0.279 0.780&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;When the categorical explanatory variable has many levels, they’re encoded to &lt;strong&gt;dummy variables&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Each coefficient describes the expected difference between heights in that particular school compared to the baseline level&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;categorical-predictor-with-3-levels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Categorical predictor with 3+ levels&lt;/h2&gt;
.pull-left-wide[
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
school_pntg
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
D_FL
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
F
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
G
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
I
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
S
&lt;/th&gt;
&lt;th style=&#34;text-align:center;&#34;&gt;
X
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
A
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
D/FL
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
F
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
G
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
I
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
S
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
X
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(68, 1, 84, 1) !important;&#34;&gt;
0
&lt;/td&gt;
&lt;td style=&#34;text-align:center;width: 10em; color: white !important;background-color: rgba(122, 209, 81, 1) !important;&#34;&gt;
1
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;]
.pull-right-narrow[
.small[&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3,393 x 3
##    name      Height_in school_pntg
##    &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      
##  1 L1764-2          37 F          
##  2 L1764-3          18 I          
##  3 L1764-4          13 D/FL       
##  4 L1764-5a         14 F          
##  5 L1764-5b         14 F          
##  6 L1764-6           7 I          
##  7 L1764-7a          6 F          
##  8 L1764-7b          6 F          
##  9 L1764-8          15 I          
## 10 L1764-9a          9 D/FL       
## 11 L1764-9b          9 D/FL       
## 12 L1764-10a        16 X          
## 13 L1764-10b        16 X          
## 14 L1764-10c        16 X          
## 15 L1764-11         20 D/FL       
## 16 L1764-12a        14 D/FL       
## 17 L1764-12b        14 D/FL       
## 18 L1764-13a        15 D/FL       
## 19 L1764-13b        15 D/FL       
## 20 L1764-14         37 F          
## # ... with 3,373 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;]
]&lt;/p&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;relationship-between-height-and-school-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Relationship between height and school&lt;/h2&gt;
&lt;p&gt;.small[&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 7 x 5
##   term            estimate std.error statistic p.value
##   &amp;lt;chr&amp;gt;              &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1 (Intercept)        14.0       10.0     1.40  0.162  
## 2 school_pntgD/FL     2.33      10.0     0.232 0.816  
## 3 school_pntgF       10.2       10.0     1.02  0.309  
## 4 school_pntgG        1.65      11.9     0.139 0.889  
## 5 school_pntgI       10.3       10.0     1.02  0.306  
## 6 school_pntgS       30.4       11.4     2.68  0.00744
## 7 school_pntgX        2.87      10.3     0.279 0.780&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Austrian school (A)&lt;/strong&gt; paintings are expected, on average, to be &lt;strong&gt;14 inches&lt;/strong&gt; tall.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Dutch/Flemish school (D/FL)&lt;/strong&gt; paintings are expected, on average, to be &lt;strong&gt;2.33 inches taller&lt;/strong&gt; than &lt;em&gt;Austrian school&lt;/em&gt; paintings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;French school (F)&lt;/strong&gt; paintings are expected, on average, to be &lt;strong&gt;10.2 inches taller&lt;/strong&gt; than &lt;em&gt;Austrian school&lt;/em&gt; paintings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;German school (G)&lt;/strong&gt; paintings are expected, on average, to be &lt;strong&gt;1.65 inches taller&lt;/strong&gt; than &lt;em&gt;Austrian school&lt;/em&gt; paintings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Italian school (I)&lt;/strong&gt; paintings are expected, on average, to be &lt;strong&gt;10.3 inches taller&lt;/strong&gt; than &lt;em&gt;Austrian school&lt;/em&gt; paintings.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Spanish school (S)&lt;/strong&gt; paintings are expected, on average, to be &lt;strong&gt;30.4 inches taller&lt;/strong&gt; than &lt;em&gt;Austrian school&lt;/em&gt; paintings.&lt;/li&gt;
&lt;li&gt;Paintings whose school is &lt;strong&gt;unknown (X)&lt;/strong&gt; are expected, on average, to be &lt;strong&gt;2.87 inches taller&lt;/strong&gt; than &lt;em&gt;Austrian school&lt;/em&gt; paintings.
]&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A04 Shiny App</title>
      <link>/post/a04-shiny-app/</link>
      <pubDate>Fri, 11 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/a04-shiny-app/</guid>
      <description>
&lt;script src=&#34;/post/a04-shiny-app/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;section-provisional-count-of-deaths-involving-coronavirus-disease-2019-covid-19-by-united-states-county&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Provisional count of deaths involving coronavirus disease 2019 (COVID-19) by United States county&lt;/h2&gt;
&lt;p&gt;The data used in this post is obtained from Data.Gov and it shows the provisional count of deaths involving coronavirus disease 2019 (COVID-19) by United States county.&lt;/p&gt;
&lt;p&gt;The dataset is intended for public access and use.&lt;/p&gt;
&lt;iframe height=&#34;800&#34; width=&#34;100%&#34; frameborder=&#34;no&#34; src=&#34;https://nouran.shinyapps.io/Data-Anaytics/&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A03</title>
      <link>/post/a03/</link>
      <pubDate>Thu, 03 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/a03/</guid>
      <description>
&lt;script src=&#34;/post/a03/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;reading-the-lego_sales.csv-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading the lego_sales.csv file&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.3     v purrr   0.3.4
## v tibble  3.1.1     v dplyr   1.0.6
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lego_sales &amp;lt;- read_csv(&amp;quot;lego_sales.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -- Column specification --------------------------------------------------------
## cols(
##   first_name = col_character(),
##   last_name = col_character(),
##   age = col_double(),
##   phone_number = col_character(),
##   set_id = col_double(),
##   number = col_character(),
##   theme = col_character(),
##   subtheme = col_character(),
##   year = col_double(),
##   name = col_character(),
##   pieces = col_double(),
##   us_price = col_double(),
##   image_url = col_character(),
##   quantity = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(lego_sales)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 620
## Columns: 14
## $ first_name   &amp;lt;chr&amp;gt; &amp;quot;Kimberly&amp;quot;, &amp;quot;Neel&amp;quot;, &amp;quot;Neel&amp;quot;, &amp;quot;Chelsea&amp;quot;, &amp;quot;Chelsea&amp;quot;, &amp;quot;Chelse~
## $ last_name    &amp;lt;chr&amp;gt; &amp;quot;Beckstead&amp;quot;, &amp;quot;Garvin&amp;quot;, &amp;quot;Garvin&amp;quot;, &amp;quot;Bouchard&amp;quot;, &amp;quot;Bouchard&amp;quot;, ~
## $ age          &amp;lt;dbl&amp;gt; 24, 35, 35, 41, 41, 41, 19, 19, 37, 37, 19, 19, 20, 36, 3~
## $ phone_number &amp;lt;chr&amp;gt; &amp;quot;216-555-2549&amp;quot;, &amp;quot;819-555-3189&amp;quot;, &amp;quot;819-555-3189&amp;quot;, NA, NA, N~
## $ set_id       &amp;lt;dbl&amp;gt; 24701, 25626, 24665, 24695, 25626, 24721, 24797, 24701, 2~
## $ number       &amp;lt;chr&amp;gt; &amp;quot;76062&amp;quot;, &amp;quot;70595&amp;quot;, &amp;quot;21031&amp;quot;, &amp;quot;31048&amp;quot;, &amp;quot;70595&amp;quot;, &amp;quot;10831&amp;quot;, &amp;quot;75~
## $ theme        &amp;lt;chr&amp;gt; &amp;quot;DC Comics Super Heroes&amp;quot;, &amp;quot;Ninjago&amp;quot;, &amp;quot;Architecture&amp;quot;, &amp;quot;Cre~
## $ subtheme     &amp;lt;chr&amp;gt; &amp;quot;Mighty Micros&amp;quot;, &amp;quot;Rise of the Villains&amp;quot;, NA, NA, &amp;quot;Rise of~
## $ year         &amp;lt;dbl&amp;gt; 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 2018, 201~
## $ name         &amp;lt;chr&amp;gt; &amp;quot;Robin vs. Bane&amp;quot;, &amp;quot;Ultra Stealth Raider&amp;quot;, &amp;quot;Burj Khalifa&amp;quot;,~
## $ pieces       &amp;lt;dbl&amp;gt; 77, 1093, 333, 368, 1093, 19, 233, 77, 108, NA, 13, 15, 6~
## $ us_price     &amp;lt;dbl&amp;gt; 9.99, 119.99, 39.99, 29.99, 119.99, 9.99, 24.99, 9.99, 9.~
## $ image_url    &amp;lt;chr&amp;gt; &amp;quot;http://images.brickset.com/sets/images/76062-1.jpg&amp;quot;, &amp;quot;ht~
## $ quantity     &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, ~&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lego_sales&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 620 x 14
##    first_name last_name    age phone_number set_id number theme  subtheme   year
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 Kimberly   Beckstead     24 216-555-2549  24701 76062  DC Co~ Mighty M~  2018
##  2 Neel       Garvin        35 819-555-3189  25626 70595  Ninja~ Rise of ~  2018
##  3 Neel       Garvin        35 819-555-3189  24665 21031  Archi~ &amp;lt;NA&amp;gt;       2018
##  4 Chelsea    Bouchard      41 &amp;lt;NA&amp;gt;          24695 31048  Creat~ &amp;lt;NA&amp;gt;       2018
##  5 Chelsea    Bouchard      41 &amp;lt;NA&amp;gt;          25626 70595  Ninja~ Rise of ~  2018
##  6 Chelsea    Bouchard      41 &amp;lt;NA&amp;gt;          24721 10831  Duplo  &amp;lt;NA&amp;gt;       2018
##  7 Bryanna    Welsh         19 &amp;lt;NA&amp;gt;          24797 75138  Star ~ Episode V  2018
##  8 Bryanna    Welsh         19 &amp;lt;NA&amp;gt;          24701 76062  DC Co~ Mighty M~  2018
##  9 Caleb      Garcia-Wi~    37 907-555-9236  24730 41115  Frien~ &amp;lt;NA&amp;gt;       2018
## 10 Caleb      Garcia-Wi~    37 907-555-9236  25611 21127  Minec~ Minifig-~  2018
## # ... with 610 more rows, and 5 more variables: name &amp;lt;chr&amp;gt;, pieces &amp;lt;dbl&amp;gt;,
## #   us_price &amp;lt;dbl&amp;gt;, image_url &amp;lt;chr&amp;gt;, quantity &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;q1.what-are-the-three-most-common-first-names-of-customers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Q1.What are the three most common first names of customers?&lt;/h2&gt;
&lt;div id=&#34;the-answer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The answer:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;common_first_names&amp;lt;- lego_sales %&amp;gt;%
  count(first_name,sort = TRUE)  %&amp;gt;%
  head(3)
cat(&amp;quot;The three most common first names of customers are &amp;quot;,common_first_names$first_name,sep = &amp;quot;\n &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The three most common first names of customers are 
##  Jackson
##  Jacob
##  Joseph&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;q2.what-are-the-three-most-common-themes-of-lego-sets-purchased&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q2.What are the three most common themes of lego sets purchased?&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;the-answer-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The answer:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;common_themes &amp;lt;-lego_sales %&amp;gt;%
  count(theme ,sort = TRUE)%&amp;gt;%
  head(3)
cat(&amp;quot;The three most common themes of lego sets purchased are  &amp;quot;,common_themes$theme ,sep = &amp;quot;\n &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The three most common themes of lego sets purchased are  
##  Star Wars
##  Nexo Knights
##  Gear&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;q3.among-the-most-common-theme-of-lego-sets-purchased-what-is-the-most-common-subtheme&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q3.Among the most common theme of lego sets purchased, what is the most common subtheme?&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;the-answer-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The answer:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;common_subtheme&amp;lt;- lego_sales %&amp;gt;%
  filter (theme==&amp;quot;Star Wars&amp;quot;) %&amp;gt;%
  count(subtheme,sort = TRUE )%&amp;gt;%
  head(1)
cat(&amp;quot;The most common subtheme among the most common theme of lego sets purchased is &amp;quot;,common_subtheme$subtheme ,sep = &amp;quot;\n &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The most common subtheme among the most common theme of lego sets purchased is 
##  The Force Awakens&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;q4.create-a-new-variable-called-age_group-and-group-the-ages-into-the-following-categories-18-and-under-19---25-26---35-36---50-51-and-over.-be-sure-to-save-the-updated-data-set-so-you-can-use-the-new-variable-in-other-questions.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q4.Create a new variable called age_group and group the ages into the following categories: “18 and under”, “19 - 25”, “26 - 35”, “36 - 50”, “51 and over”. Be sure to save the updated data set so you can use the new variable in other questions.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;the-answer-3&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The answer:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  lego_sales_age_group &amp;lt;- lego_sales %&amp;gt;%
  mutate (age_group=case_when(
    between(age,min(age),18)~ &amp;quot;18 and under&amp;quot;,
    between(age,19,25)~ &amp;quot;19 - 25&amp;quot;,
    between(age,26,35)~ &amp;quot;26 - 35&amp;quot;,
    between(age,36,50)~ &amp;quot;36 - 50&amp;quot;,
    between(age,51,max(age))~ &amp;quot;51 and over&amp;quot;
        ))
## Displaying the new data frame with the new variable:

lego_sales_age_group&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 620 x 15
##    first_name last_name    age phone_number set_id number theme  subtheme   year
##    &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;         &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
##  1 Kimberly   Beckstead     24 216-555-2549  24701 76062  DC Co~ Mighty M~  2018
##  2 Neel       Garvin        35 819-555-3189  25626 70595  Ninja~ Rise of ~  2018
##  3 Neel       Garvin        35 819-555-3189  24665 21031  Archi~ &amp;lt;NA&amp;gt;       2018
##  4 Chelsea    Bouchard      41 &amp;lt;NA&amp;gt;          24695 31048  Creat~ &amp;lt;NA&amp;gt;       2018
##  5 Chelsea    Bouchard      41 &amp;lt;NA&amp;gt;          25626 70595  Ninja~ Rise of ~  2018
##  6 Chelsea    Bouchard      41 &amp;lt;NA&amp;gt;          24721 10831  Duplo  &amp;lt;NA&amp;gt;       2018
##  7 Bryanna    Welsh         19 &amp;lt;NA&amp;gt;          24797 75138  Star ~ Episode V  2018
##  8 Bryanna    Welsh         19 &amp;lt;NA&amp;gt;          24701 76062  DC Co~ Mighty M~  2018
##  9 Caleb      Garcia-Wi~    37 907-555-9236  24730 41115  Frien~ &amp;lt;NA&amp;gt;       2018
## 10 Caleb      Garcia-Wi~    37 907-555-9236  25611 21127  Minec~ Minifig-~  2018
## # ... with 610 more rows, and 6 more variables: name &amp;lt;chr&amp;gt;, pieces &amp;lt;dbl&amp;gt;,
## #   us_price &amp;lt;dbl&amp;gt;, image_url &amp;lt;chr&amp;gt;, quantity &amp;lt;dbl&amp;gt;, age_group &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Displaying selected variables from the new generated data frame so that the new variable age_group can be clearly seen:

lego_sales_age_group %&amp;gt;% select(first_name,age,age_group)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 620 x 3
##    first_name   age age_group
##    &amp;lt;chr&amp;gt;      &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    
##  1 Kimberly      24 19 - 25  
##  2 Neel          35 26 - 35  
##  3 Neel          35 26 - 35  
##  4 Chelsea       41 36 - 50  
##  5 Chelsea       41 36 - 50  
##  6 Chelsea       41 36 - 50  
##  7 Bryanna       19 19 - 25  
##  8 Bryanna       19 19 - 25  
##  9 Caleb         37 36 - 50  
## 10 Caleb         37 36 - 50  
## # ... with 610 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;q5.what-is-the-probability-a-randomly-selected-customer&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q5.What is the probability a randomly selected customer&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;is in the 19 - 25 age group?&lt;/li&gt;
&lt;li&gt;is in the 19 - 25 age group and purchased a Duplo theme set?&lt;/li&gt;
&lt;li&gt;is in the 19 - 25 age group given they purchased a Duplo theme set?&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-answer-4&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The answer:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# 1st part Calculating the probability a randomly selected customer is in the 19 - 25 age group
from19_to25_age_group &amp;lt;- lego_sales_age_group %&amp;gt;%
count(age_group) %&amp;gt;%
  filter( age_group==&amp;quot;19 - 25&amp;quot;)

prob_of_19_25 &amp;lt;- (from19_to25_age_group$n/nrow(lego_sales))
       
# 2nd part Calculating the probability a randomly selected customer is in the 19 - 25 age group and purchased a Duplo theme set
from19_25_duplo_theme  &amp;lt;- lego_sales_age_group %&amp;gt;%
  filter( age_group==&amp;quot;19 - 25&amp;quot; &amp;amp; theme==&amp;quot;Duplo&amp;quot;)

prob_from19_25_duplo_theme &amp;lt;- (nrow(from19_25_duplo_theme)/nrow(lego_sales))

# 3rd part Calculating the probability a randomly selected customer is in the 19 - 25 age group given they purchased a Duplo theme set
duplo_theme_customers  &amp;lt;- lego_sales_age_group %&amp;gt;%
  filter(theme==&amp;quot;Duplo&amp;quot;)
  
prob_of_19_given_duplo_theme &amp;lt;- (nrow(from19_25_duplo_theme)/nrow(duplo_theme_customers))


cat(&amp;quot;The probability of a randomly selected customer is in the 19 - 25 age groupis &amp;quot;,prob_of_19_25 ,sep = &amp;quot;\n &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The probability of a randomly selected customer is in the 19 - 25 age groupis 
##  0.2080645&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;The probability of a randomly selected customer is in the 19 - 25 age group and purchased a Duplo theme is &amp;quot;,prob_from19_25_duplo_theme ,sep = &amp;quot;\n &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The probability of a randomly selected customer is in the 19 - 25 age group and purchased a Duplo theme is 
##  0.01451613&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;The probability of a randomly selected customer is in the 19 - 25 age group given they purchased a Duplo theme is &amp;quot;,prob_of_19_given_duplo_theme ,sep = &amp;quot;\n &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The probability of a randomly selected customer is in the 19 - 25 age group given they purchased a Duplo theme is 
##  0.2571429&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;q6.which-age-group-has-purchased-the-largest-number-of-lego-sets-how-many-did-they-purchase&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q6.Which age group has purchased the largest number of lego sets? How many did they purchase?&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;the-answer-5&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The answer:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lego_sales_by_age_group &amp;lt;- lego_sales_age_group %&amp;gt;% group_by(age_group) %&amp;gt;% 
  summarise(total_quantity=sum(quantity)) %&amp;gt;% 
  arrange(desc(total_quantity))%&amp;gt;%
  head(1)
  cat(&amp;quot;The age group that has purchased the largest number of lego sets is&amp;quot;,lego_sales_by_age_group$age_group,sep=&amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The age group that has purchased the largest number of lego sets is
## 36 - 50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  cat(&amp;quot;They purchased a total number of lego sets equal to &amp;quot;,lego_sales_by_age_group$total_quantity,sep=&amp;quot;\n&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## They purchased a total number of lego sets equal to 
## 313&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;q7.which-age-group-has-spent-the-most-money-on-legos-how-much-did-they-spend&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q7.Which age group has spent the most money on legos? How much did they spend?&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;the-answer-6&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The answer:&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;most_spending_by_age_group &amp;lt;- lego_sales_age_group %&amp;gt;% mutate(total_price=us_price*quantity) %&amp;gt;%
  group_by(age_group) %&amp;gt;% 
  summarise(total_spending=sum(total_price)) %&amp;gt;% 
  arrange(desc(total_spending))%&amp;gt;%
  head(1)
cat(&amp;quot;The age group that has spent the most money on Lego is  &amp;quot;,most_spending_by_age_group$age_group ,sep = &amp;quot;\n &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The age group that has spent the most money on Lego is  
##  36 - 50&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cat(&amp;quot;They spend amount of US dollars equal to  &amp;quot;,most_spending_by_age_group$total_spending ,sep = &amp;quot;\n &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## They spend amount of US dollars equal to  
##  9532.87&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;q8.come-up-with-a-question-you-want-to-answer-using-this-data-and-write-it-down.-then-create-a-data-visualization-that-answers-the-question-and-briefly-explain-how-your-visualization-answers-the-question.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q8.Come up with a question you want to answer using this data, and write it down. Then, create a data visualization that answers the question, and briefly explain how your visualization answers the question.&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;the-answer-7&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The answer:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The question could be how much revenue Lego makes from each theme and the contribution of each age group in each theme revenue?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The answer of this question can be obtained by first multiplying the quantity of sets purchased by every customer by the price per each set, then group the data frame by the theme so that the revenue from each theme can be obtained, plotting the theme on the x axis and the revenue on the y axis of the column plot would allow us easily to determine the revenue of each theme and the age groups contributed in each theme revenue.&lt;/p&gt;
&lt;p&gt;From the plot we can see that Star Wars theme makes the most revenue of more than 4000 Us dollars where 26-35 and 36-50 age groups have more contributions on this revenue than the other age groups, next is the Ninjago theme of revenue more than 2000 US dollar where we can see that 18 and under age group doesn’t contribute in this revenue much and city theme of revenue more than 2000 US dollar where we can see that this theme doesn’t attract the 51 and over age group and the themes that makes the little revenues are Classic, Collectable Minifigures and seasonal themes where each theme attract only a certain age group that is 36-50, 51 and over and 26-35 respectively.&lt;/p&gt;
&lt;p&gt;The Duplo, Elvis, Friends, Gear and Minicraft themes make revenue of a little more than 1000 us dollars where the contribution of the 18 and under age group is very small, the revenue from the remaining themes is less than 1000 us dollars and the age groups contribution vary from one theme to another.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;lego_sales_by_theme &amp;lt;- lego_sales_age_group %&amp;gt;% mutate(total_price=us_price*quantity) %&amp;gt;%
  group_by(theme) 

ggplot(lego_sales_by_theme,aes(x=theme,y=total_price,fill=age_group))+
  geom_col()+
  theme(axis.text.x=element_text(color = &amp;quot;black&amp;quot;, size=9, angle=90, vjust=.5, hjust=0.7))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/a03/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;q9.add-one-element-to-the-plot-from-the-previous-exercise-to-change-the-look-of-the-plot-without-changing-the-underlying-data.-for-example-you-can-change-the-theme-background-color-add-annotations-etc.-state-the-change-youre-making-and-display-the-updated-visualization.-we-encourage-you-to-be-creative&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Q9.Add one element to the plot from the previous exercise to change the look of the plot without changing the underlying data. For example, you can change the theme, background color, add annotations, etc. State the change you’re making and display the updated visualization. We encourage you to be creative!&lt;/h3&gt;
&lt;/div&gt;
&lt;div id=&#34;the-answer-8&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The answer:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Title for the plot has been added&lt;/li&gt;
&lt;li&gt;Titles for the 2 axes of the plot have been added&lt;/li&gt;
&lt;li&gt;Black and white theme is used&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;The background color has been changed&lt;/li&gt;
&lt;li&gt;viridis used to update the color map&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(lego_sales_by_theme,aes(x=theme,y=total_price,fill=age_group))+
  geom_col() +
  scale_fill_viridis_d() +
  theme_bw()+
  theme(axis.text.x=element_text(color = &amp;quot;red&amp;quot;, size=9, angle=90, vjust=.5, hjust=0.7)) +
  theme(plot.background = element_rect(fill = &amp;quot;#BFD5E3&amp;quot;))+
  labs(
    x = &amp;quot;Theme&amp;quot;, y = &amp;quot;The revenue in US Dollars &amp;quot;, 
    title = &amp;quot;The revenue associated with each theme and the contribution \n                        of each age group in this revenue&amp;quot;, 
         fill = &amp;quot;Age Groups&amp;quot; 
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/a03/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data I/O</title>
      <link>/post/data-i-o/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/data-i-o/</guid>
      <description>
&lt;script src=&#34;/post/data-i-o/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;import-nobel.csv-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import Nobel.csv file&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.3     v purrr   0.3.4
## v tibble  3.1.1     v dplyr   1.0.6
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;nobel &amp;lt;- read_csv(&amp;quot;nobel.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -- Column specification --------------------------------------------------------
## cols(
##   .default = col_character(),
##   id = col_double(),
##   year = col_double(),
##   born_date = col_date(format = &amp;quot;&amp;quot;),
##   died_date = col_date(format = &amp;quot;&amp;quot;),
##   share = col_double()
## )
## i Use `spec()` for the full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(nobel)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 26
##      id firstname  surname  year category affiliation   city  country born_date 
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;    &amp;lt;chr&amp;gt;         &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;date&amp;gt;    
## 1     1 Wilhelm C~ Röntgen  1901 Physics  Munich Unive~ Muni~ Germany 1845-03-27
## 2     2 Hendrik A. Lorentz  1902 Physics  Leiden Unive~ Leid~ Nether~ 1853-07-18
## 3     3 Pieter     Zeeman   1902 Physics  Amsterdam Un~ Amst~ Nether~ 1865-05-25
## 4     4 Henri      Becque~  1903 Physics  École Polyte~ Paris France  1852-12-15
## 5     5 Pierre     Curie    1903 Physics  École munici~ Paris France  1859-05-15
## 6     6 Marie      Curie    1903 Physics  &amp;lt;NA&amp;gt;          &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;    1867-11-07
## # ... with 17 more variables: died_date &amp;lt;date&amp;gt;, gender &amp;lt;chr&amp;gt;, born_city &amp;lt;chr&amp;gt;,
## #   born_country &amp;lt;chr&amp;gt;, born_country_code &amp;lt;chr&amp;gt;, died_city &amp;lt;chr&amp;gt;,
## #   died_country &amp;lt;chr&amp;gt;, died_country_code &amp;lt;chr&amp;gt;, overall_motivation &amp;lt;chr&amp;gt;,
## #   share &amp;lt;dbl&amp;gt;, motivation &amp;lt;chr&amp;gt;, born_country_original &amp;lt;chr&amp;gt;,
## #   born_city_original &amp;lt;chr&amp;gt;, died_country_original &amp;lt;chr&amp;gt;,
## #   died_city_original &amp;lt;chr&amp;gt;, city_original &amp;lt;chr&amp;gt;, country_original &amp;lt;chr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;write-a-csv-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Write a csv file&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df &amp;lt;- tribble(
  ~x, ~y,
  1,  &amp;quot;a&amp;quot;,
  2,  &amp;quot;b&amp;quot;,
  3,  &amp;quot;c&amp;quot;
)
write_csv(df, file = &amp;quot;df.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;dealing-with-bad-variable-names&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dealing with bad variable names&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#edibnb_badnames &amp;lt;- read_csv(&amp;quot;edibnb-badnames.csv&amp;quot;)
#names(edibnb_badnames)

edibnb_col_names &amp;lt;- read_csv(&amp;quot;edibnb-badnames.csv&amp;quot;,
                             col_names = c(&amp;quot;id&amp;quot;, &amp;quot;price&amp;quot;, 
                                           &amp;quot;neighbourhood&amp;quot;, &amp;quot;accommodates&amp;quot;,
                                           &amp;quot;bathroom&amp;quot;, &amp;quot;bedroom&amp;quot;, 
                                           &amp;quot;bed&amp;quot;, &amp;quot;review_scores_rating&amp;quot;, 
                                           &amp;quot;n_reviews&amp;quot;, &amp;quot;url&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -- Column specification --------------------------------------------------------
## cols(
##   id = col_character(),
##   price = col_character(),
##   neighbourhood = col_character(),
##   accommodates = col_character(),
##   bathroom = col_character(),
##   bedroom = col_character(),
##   bed = col_character(),
##   review_scores_rating = col_character(),
##   n_reviews = col_character(),
##   url = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(edibnb_col_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;id&amp;quot;                   &amp;quot;price&amp;quot;                &amp;quot;neighbourhood&amp;quot;       
##  [4] &amp;quot;accommodates&amp;quot;         &amp;quot;bathroom&amp;quot;             &amp;quot;bedroom&amp;quot;             
##  [7] &amp;quot;bed&amp;quot;                  &amp;quot;review_scores_rating&amp;quot; &amp;quot;n_reviews&amp;quot;           
## [10] &amp;quot;url&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;importing-data-with-snake-case-variables&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Importing data with snake case variables&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;edibnb_clean_names &amp;lt;- read_csv(&amp;quot;edibnb-badnames.csv&amp;quot;) %&amp;gt;%
  janitor::clean_names()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -- Column specification --------------------------------------------------------
## cols(
##   ID = col_double(),
##   Price = col_double(),
##   neighbourhood = col_character(),
##   accommodates = col_double(),
##   `Number of bathrooms` = col_double(),
##   `Number of Bedrooms` = col_double(),
##   `n beds` = col_double(),
##   `Review Scores Rating` = col_double(),
##   `Number of reviews` = col_double(),
##   listing_url = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(edibnb_clean_names)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;id&amp;quot;                   &amp;quot;price&amp;quot;                &amp;quot;neighbourhood&amp;quot;       
##  [4] &amp;quot;accommodates&amp;quot;         &amp;quot;number_of_bathrooms&amp;quot;  &amp;quot;number_of_bedrooms&amp;quot;  
##  [7] &amp;quot;n_beds&amp;quot;               &amp;quot;review_scores_rating&amp;quot; &amp;quot;number_of_reviews&amp;quot;   
## [10] &amp;quot;listing_url&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;read-df-na.csv&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Read df-na.csv&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#read_csv(&amp;quot;df-na.csv&amp;quot;) 
read_csv(&amp;quot;df-na.csv&amp;quot;, 
         na = c(&amp;quot;&amp;quot;, &amp;quot;NA&amp;quot;, &amp;quot;.&amp;quot;, &amp;quot;9999&amp;quot;, &amp;quot;Not applicable&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -- Column specification --------------------------------------------------------
## cols(
##   x = col_double(),
##   y = col_character(),
##   z = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 3
##       x y     z     
##   &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; 
## 1     1 a     hi    
## 2    NA b     hello 
## 3     3 &amp;lt;NA&amp;gt;  &amp;lt;NA&amp;gt;  
## 4     4 d     ola   
## 5     5 e     hola  
## 6    NA f     whatup
## 7     7 g     wassup
## 8     8 h     sup   
## 9     9 i     &amp;lt;NA&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-an-xlsx-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading an xlsx file&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
fav_food &amp;lt;- read_excel(&amp;quot;favourite-food.xlsx&amp;quot;,
                               na = c(&amp;quot;N/A&amp;quot;, &amp;quot;99999&amp;quot;)) %&amp;gt;%
janitor::clean_names()
fav_food&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 6
##   student_id full_name        favourite_food     meal_plan           age   ses  
##        &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;               &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;
## 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only          4     High 
## 2          2 Barclay Lynn     French fries       Lunch only          5     Midd~
## 3          3 Jayendra Lyne    &amp;lt;NA&amp;gt;               Breakfast and lunch 7     Low  
## 4          4 Leon Rossini     Anchovies          Lunch only          &amp;lt;NA&amp;gt;  Midd~
## 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch five  High&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fav_food &amp;lt;- fav_food %&amp;gt;%
  mutate(
    age = if_else(age == &amp;quot;five&amp;quot;, &amp;quot;5&amp;quot;, age),
    age = as.numeric(age)
    )

fav_food&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 6
##   student_id full_name        favourite_food     meal_plan             age ses  
##        &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4 High 
## 2          2 Barclay Lynn     French fries       Lunch only              5 Midd~
## 3          3 Jayendra Lyne    &amp;lt;NA&amp;gt;               Breakfast and lunch     7 Low  
## 4          4 Leon Rossini     Anchovies          Lunch only             NA Midd~
## 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5 High&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;glimpse(fav_food)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Rows: 5
## Columns: 6
## $ student_id     &amp;lt;dbl&amp;gt; 1, 2, 3, 4, 5
## $ full_name      &amp;lt;chr&amp;gt; &amp;quot;Sunil Huffmann&amp;quot;, &amp;quot;Barclay Lynn&amp;quot;, &amp;quot;Jayendra Lyne&amp;quot;, &amp;quot;Leo~
## $ favourite_food &amp;lt;chr&amp;gt; &amp;quot;Strawberry yoghurt&amp;quot;, &amp;quot;French fries&amp;quot;, NA, &amp;quot;Anchovies&amp;quot;, ~
## $ meal_plan      &amp;lt;chr&amp;gt; &amp;quot;Lunch only&amp;quot;, &amp;quot;Lunch only&amp;quot;, &amp;quot;Breakfast and lunch&amp;quot;, &amp;quot;Lun~
## $ age            &amp;lt;dbl&amp;gt; 4, 5, 7, NA, 5
## $ ses            &amp;lt;chr&amp;gt; &amp;quot;High&amp;quot;, &amp;quot;Middle&amp;quot;, &amp;quot;Low&amp;quot;, &amp;quot;Middle&amp;quot;, &amp;quot;High&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;etl-of-ses-varaible&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;ETL of SES varaible&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fav_food %&amp;gt;%
  count(ses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   ses        n
##   &amp;lt;chr&amp;gt;  &amp;lt;int&amp;gt;
## 1 High       2
## 2 Low        1
## 3 Middle     2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fav_food &amp;lt;- fav_food %&amp;gt;%
  mutate(ses = fct_relevel(ses, &amp;quot;Low&amp;quot;, &amp;quot;Middle&amp;quot;, &amp;quot;High&amp;quot;))
fav_food %&amp;gt;%
  count(ses)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   ses        n
##   &amp;lt;fct&amp;gt;  &amp;lt;int&amp;gt;
## 1 Low        1
## 2 Middle     2
## 3 High       2&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fav_food&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 6
##   student_id full_name        favourite_food     meal_plan             age ses  
##        &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;
## 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4 High 
## 2          2 Barclay Lynn     French fries       Lunch only              5 Midd~
## 3          3 Jayendra Lyne    &amp;lt;NA&amp;gt;               Breakfast and lunch     7 Low  
## 4          4 Leon Rossini     Anchovies          Lunch only             NA Midd~
## 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5 High&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;putting-it-altogether&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Putting it altogether&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;fav_food &amp;lt;- read_excel(&amp;quot;favourite-food.xlsx&amp;quot;, na = c(&amp;quot;N/A&amp;quot;, &amp;quot;99999&amp;quot;)) %&amp;gt;%
  janitor::clean_names() %&amp;gt;%
  mutate(
    age = if_else(age == &amp;quot;five&amp;quot;, &amp;quot;5&amp;quot;, age), 
    age = as.numeric(age),
    ses = fct_relevel(ses, &amp;quot;Low&amp;quot;, &amp;quot;Middle&amp;quot;, &amp;quot;High&amp;quot;)
  )
fav_food&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 5 x 6
##   student_id full_name        favourite_food     meal_plan             age ses  
##        &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;               &amp;lt;dbl&amp;gt; &amp;lt;fct&amp;gt;
## 1          1 Sunil Huffmann   Strawberry yoghurt Lunch only              4 High 
## 2          2 Barclay Lynn     French fries       Lunch only              5 Midd~
## 3          3 Jayendra Lyne    &amp;lt;NA&amp;gt;               Breakfast and lunch     7 Low  
## 4          4 Leon Rossini     Anchovies          Lunch only             NA Midd~
## 5          5 Chidiegwu Dunkel Pizza              Breakfast and lunch     5 High&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Record</title>
      <link>/post/data-record/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>/post/data-record/</guid>
      <description>
&lt;script src=&#34;/post/data-record/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;import-and-transform-relig-income.csv-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import and Transform Relig-income.csv file&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.3     v purrr   0.3.4
## v tibble  3.1.1     v dplyr   1.0.6
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readxl)
rel_inc &amp;lt;- read_excel(&amp;quot;relig-income.xlsx&amp;quot;)
rel_inc_long &amp;lt;- rel_inc %&amp;gt;%
  rename( 
    religion = `Religious tradition`, 
    n = `Sample Size` 
  ) %&amp;gt;%
  pivot_longer( 
    cols = -c(religion, n),   # all but religion and n 
    names_to = &amp;quot;income&amp;quot;,  
    values_to = &amp;quot;proportion&amp;quot; 
 
) %&amp;gt;%
  mutate(frequency = round(proportion * n))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-using-barplot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Visualize using Barplot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rel_inc_long &amp;lt;- rel_inc_long %&amp;gt;%
  mutate(religion = case_when(
    religion == &amp;quot;Evangelical Protestant&amp;quot;           ~ &amp;quot;Ev. Protestant&amp;quot;,
    religion == &amp;quot;Historically Black Protestant&amp;quot;    ~ &amp;quot;Hist. Black Protestant&amp;quot;,
    religion == &amp;#39;Unaffiliated (religious &amp;quot;nones&amp;quot;)&amp;#39; ~ &amp;quot;Unaffiliated&amp;quot;,
    TRUE                                           ~ religion
  ))

rel_inc_long &amp;lt;- rel_inc_long %&amp;gt;%
  mutate(religion = fct_rev(religion))

ggplot(rel_inc_long, aes(y = religion, x = frequency)) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-record/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fill-barplot-with-income&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fill Barplot with Income&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(rel_inc_long, aes(y = religion, x = frequency, fill=income )) +
  geom_col()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-record/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(rel_inc_long, aes(y = religion, x = frequency, fill=income )) +
  geom_col(position = &amp;quot;fill&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-record/index_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(rel_inc_long, aes(y = religion, x = frequency, fill=income )) +
  geom_col(position = &amp;quot;fill&amp;quot;)+
  scale_fill_viridis_d()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-record/index_files/figure-html/unnamed-chunk-3-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;change-the-theme-of-the-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Change the Theme of the plot&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(rel_inc_long, aes(y = religion, x = frequency, fill = income)) +
  geom_col(position = &amp;quot;fill&amp;quot;) +
  scale_fill_viridis_d() +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-record/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(rel_inc_long, aes(y = religion, x = frequency, fill = income)) +
  geom_col(position = &amp;quot;fill&amp;quot;) +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-record/index_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(rel_inc_long, aes(y = religion, x = frequency, fill = income)) +
  geom_col(position = &amp;quot;fill&amp;quot;) +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(legend.position = &amp;quot;bottom&amp;quot;) +
  guides(fill = guide_legend(nrow = 2, byrow = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-record/index_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(rel_inc_long, aes(y = religion, x = frequency, fill = income)) +
  geom_col(position = &amp;quot;fill&amp;quot;) +
  scale_fill_viridis_d() +
  theme_minimal() +
  theme(legend.position = &amp;quot;bottom&amp;quot;) +
  guides(fill = guide_legend(nrow = 2, byrow = TRUE)) +
  labs(
    x = &amp;quot;Proportion&amp;quot;, y = &amp;quot;&amp;quot;, 
    title = &amp;quot;Income distribution by religious group&amp;quot;, 
    subtitle = &amp;quot;Source: Pew Research Center, Religious Landscape Study&amp;quot;, 
    fill = &amp;quot;Income&amp;quot; 
    )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-record/index_files/figure-html/unnamed-chunk-4-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Assignment A02: TIDYVERSE</title>
      <link>/post/assignment-a02-tidyverse/</link>
      <pubDate>Thu, 27 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/assignment-a02-tidyverse/</guid>
      <description>
&lt;script src=&#34;/post/assignment-a02-tidyverse/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;covid-19-world-vaccination-progress&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;COVID-19 World Vaccination Progress&lt;/h2&gt;
&lt;p&gt;Daily and Total Vaccination for COVID-19 in the World from Our World in Data, Data is collected daily from Our World in Data GitHub repository for covid-19, merged and uploaded.
the used data set in this blog is updated version (119) and it was released in may 26th,2021.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.3     v purrr   0.3.4
## v tibble  3.1.1     v dplyr   1.0.6
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)


# reading the CSV file after placing it in the same directory where getdw() was used to determine the current directory
country_vacc &amp;lt;- read_csv(&amp;quot;country_vaccinations.csv&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -- Column specification --------------------------------------------------------
## cols(
##   country = col_character(),
##   iso_code = col_character(),
##   date = col_date(format = &amp;quot;&amp;quot;),
##   total_vaccinations = col_double(),
##   people_vaccinated = col_double(),
##   people_fully_vaccinated = col_double(),
##   daily_vaccinations_raw = col_double(),
##   daily_vaccinations = col_double(),
##   total_vaccinations_per_hundred = col_double(),
##   people_vaccinated_per_hundred = col_double(),
##   people_fully_vaccinated_per_hundred = col_double(),
##   daily_vaccinations_per_million = col_double(),
##   vaccines = col_character(),
##   source_name = col_character(),
##   source_website = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-two-tables-from-the-country_vaccinations-dataset&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating two tables from the country_vaccinations dataset&lt;/h2&gt;
&lt;p&gt;table_1 includes the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;country: this is the country for which the vaccination information is provided&lt;/li&gt;
&lt;li&gt;iso_code: ISO code for the country&lt;/li&gt;
&lt;li&gt;date: Date of observation&lt;/li&gt;
&lt;li&gt;people_vaccinated: Total number of people who received at least one vaccine dose&lt;/li&gt;
&lt;li&gt;daily_vaccinations:for a certain data entry, the number of vaccination for that date/country&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;table_2 includes the following variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;country: this is the country for which the vaccination information is provided&lt;/li&gt;
&lt;li&gt;iso_code: ISO code for the country&lt;/li&gt;
&lt;li&gt;date: Date of observation&lt;/li&gt;
&lt;li&gt;people_fully_vaccinated: this is the number of people that received the entire set of immunization according to the immunization scheme (typically 2)&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating the first table (table_1)
table_1 &amp;lt;- country_vacc %&amp;gt;%
select (country,iso_code,date,people_vaccinated,daily_vaccinations)

table_1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20,390 x 5
##    country     iso_code date       people_vaccinated daily_vaccinations
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;    &amp;lt;date&amp;gt;                 &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt;
##  1 Afghanistan AFG      2021-02-22                 0                 NA
##  2 Afghanistan AFG      2021-02-23                NA               1367
##  3 Afghanistan AFG      2021-02-24                NA               1367
##  4 Afghanistan AFG      2021-02-25                NA               1367
##  5 Afghanistan AFG      2021-02-26                NA               1367
##  6 Afghanistan AFG      2021-02-27                NA               1367
##  7 Afghanistan AFG      2021-02-28              8200               1367
##  8 Afghanistan AFG      2021-03-01                NA               1580
##  9 Afghanistan AFG      2021-03-02                NA               1794
## 10 Afghanistan AFG      2021-03-03                NA               2008
## # ... with 20,380 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Creating the second table (table_2)
table_2 &amp;lt;- country_vacc %&amp;gt;%
select (country,iso_code,date, people_fully_vaccinated) 
table_2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 20,390 x 4
##    country     iso_code date       people_fully_vaccinated
##    &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;    &amp;lt;date&amp;gt;                       &amp;lt;dbl&amp;gt;
##  1 Afghanistan AFG      2021-02-22                      NA
##  2 Afghanistan AFG      2021-02-23                      NA
##  3 Afghanistan AFG      2021-02-24                      NA
##  4 Afghanistan AFG      2021-02-25                      NA
##  5 Afghanistan AFG      2021-02-26                      NA
##  6 Afghanistan AFG      2021-02-27                      NA
##  7 Afghanistan AFG      2021-02-28                      NA
##  8 Afghanistan AFG      2021-03-01                      NA
##  9 Afghanistan AFG      2021-03-02                      NA
## 10 Afghanistan AFG      2021-03-03                      NA
## # ... with 20,380 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;calculating-the-percentage-of-people-who-are-fully-vaccinated-in-canada&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Calculating the percentage of people who are fully vaccinated in Canada&lt;/h2&gt;
&lt;p&gt;to calculate the percentage of people who are fully vaccinated we need first to join table_1 and table_2, then applying filter to pick only the Canada data set thenWe can calculate the percentage of people who are fully vaccinated by dividing the number of fully vaccinated people by the total number of vaccinations per day and multiply the result by 100.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;percent_of_fully_vacc &amp;lt;- table_1 %&amp;gt;%
  left_join(table_2)%&amp;gt;%
  filter(country==&amp;quot;Canada&amp;quot;) %&amp;gt;%
  mutate(percent_of_fully_vaccinated_people=people_fully_vaccinated/daily_vaccinations*100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;country&amp;quot;, &amp;quot;iso_code&amp;quot;, &amp;quot;date&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(percent_of_fully_vacc,aes(date,percent_of_fully_vaccinated_people))+
  geom_col()+
  labs(title=&amp;quot;              Percetage of fully vaccinated people from the daily vaccination&amp;quot;,x=&amp;quot;Date&amp;quot;,y=&amp;quot;percentage of fully vaccinated&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 23 rows containing missing values (position_stack).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a02-tidyverse/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;percent_of_fully_vacc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 163 x 7
##    country iso_code date       people_vaccinated daily_vaccinations
##    &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;    &amp;lt;date&amp;gt;                 &amp;lt;dbl&amp;gt;              &amp;lt;dbl&amp;gt;
##  1 Canada  CAN      2020-12-14                NA                 NA
##  2 Canada  CAN      2020-12-15                NA                718
##  3 Canada  CAN      2020-12-16                NA               1509
##  4 Canada  CAN      2020-12-17                NA               2399
##  5 Canada  CAN      2020-12-18                NA               2792
##  6 Canada  CAN      2020-12-19                NA               2378
##  7 Canada  CAN      2020-12-20                NA               2122
##  8 Canada  CAN      2020-12-21                NA               2980
##  9 Canada  CAN      2020-12-22                NA               3697
## 10 Canada  CAN      2020-12-23                NA               4581
## # ... with 153 more rows, and 2 more variables: people_fully_vaccinated &amp;lt;dbl&amp;gt;,
## #   percent_of_fully_vaccinated_people &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-summary-table-for-canada-vaccination-records&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating a summary table for Canada vaccination records&lt;/h2&gt;
&lt;p&gt;The Can_vacc table will be created after fully joining the two tables which are table country_vacc and percent_of_fully_vacc, then filter Canada data and from the filtered data select and rearrange the date, daily_vaccinations, vaccines, percent_of_fully_vaccinated_people variables&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Can_vacc &amp;lt;- country_vacc  %&amp;gt;%
full_join(percent_of_fully_vacc)  %&amp;gt;%
filter(country==&amp;quot;Canada&amp;quot;) %&amp;gt;%
select(date,daily_vaccinations,vaccines,percent_of_fully_vaccinated_people)%&amp;gt;%
relocate(percent_of_fully_vaccinated_people, .after =daily_vaccinations )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = c(&amp;quot;country&amp;quot;, &amp;quot;iso_code&amp;quot;, &amp;quot;date&amp;quot;, &amp;quot;people_vaccinated&amp;quot;, &amp;quot;people_fully_vaccinated&amp;quot;, &amp;quot;daily_vaccinations&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; Can_vacc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 163 x 4
##    date       daily_vaccinatio~ percent_of_fully_vaccin~ vaccines               
##    &amp;lt;date&amp;gt;                 &amp;lt;dbl&amp;gt;                    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                  
##  1 2020-12-14                NA                       NA Moderna, Oxford/AstraZ~
##  2 2020-12-15               718                       NA Moderna, Oxford/AstraZ~
##  3 2020-12-16              1509                       NA Moderna, Oxford/AstraZ~
##  4 2020-12-17              2399                       NA Moderna, Oxford/AstraZ~
##  5 2020-12-18              2792                       NA Moderna, Oxford/AstraZ~
##  6 2020-12-19              2378                       NA Moderna, Oxford/AstraZ~
##  7 2020-12-20              2122                       NA Moderna, Oxford/AstraZ~
##  8 2020-12-21              2980                       NA Moderna, Oxford/AstraZ~
##  9 2020-12-22              3697                       NA Moderna, Oxford/AstraZ~
## 10 2020-12-23              4581                       NA Moderna, Oxford/AstraZ~
## # ... with 153 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;grouping-the-data-by-the-vaccine-scheme&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;grouping the data by the vaccine scheme&lt;/h2&gt;
&lt;p&gt;we will group our data based on the Vaccines scheme and calculate the mean of the daily_vaccinations and arrange the result in descending order to know the most used vaccine scheme&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Vaccine_scheme&amp;lt;- country_vacc %&amp;gt;% group_by(vaccines) %&amp;gt;% 
  drop_na() %&amp;gt;% 
  summarise(mean_daily_vacc=mean(daily_vaccinations))%&amp;gt;%
  arrange(desc(mean_daily_vacc))

Vaccine_scheme&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 35 x 2
##    vaccines                                                      mean_daily_vacc
##    &amp;lt;chr&amp;gt;                                                                   &amp;lt;dbl&amp;gt;
##  1 Johnson&amp;amp;Johnson, Moderna, Pfizer/BioNTech                            2129707.
##  2 Covaxin, Oxford/AstraZeneca                                          1825083.
##  3 EpiVacCorona, Sputnik V                                               260009.
##  4 CanSino, Oxford/AstraZeneca, Pfizer/BioNTech, Sinovac, Sputn~         200752.
##  5 Pfizer/BioNTech, Sinovac                                              144037.
##  6 CanSino, Oxford/AstraZeneca, Sinopharm/Beijing, Sinovac, Spu~         141494 
##  7 Oxford/AstraZeneca, Pfizer/BioNTech, Sinovac                          140997.
##  8 Moderna, Oxford/AstraZeneca, Pfizer/BioNTech                          134150.
##  9 Oxford/AstraZeneca, Sinovac                                           129134.
## 10 Johnson&amp;amp;Johnson, Moderna, Oxford/AstraZeneca, Pfizer/BioNTech          80371.
## # ... with 25 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;grouping-the-data-by-the-source-of-that-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;grouping the data by the source of that data&lt;/h2&gt;
&lt;p&gt;we will group our data based on the source of the data and showing the number of data obtained from each source and then ranking the sources to determine the most source used to collect the data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Source&amp;lt;-country_vacc %&amp;gt;% 
  group_by(source_name)  %&amp;gt;% 
  summarize(n=n())  %&amp;gt;% 
  arrange(desc(n))
Source&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 98 x 2
##    source_name                                            n
##    &amp;lt;chr&amp;gt;                                              &amp;lt;int&amp;gt;
##  1 Ministry of Health                                  5984
##  2 World Health Organization                           2248
##  3 Government of the United Kingdom                     725
##  4 SPC Public Health Division                           510
##  5 Federal Office of Public Health                      273
##  6 Ministry of Public Health                            255
##  7 Norwegian Institute of Public Health                 175
##  8 Official data from provinces via covid19tracker.ca   163
##  9 National Health Commission                           162
## 10 Official data from local governments via gogov.ru    162
## # ... with 88 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-monthly-vaccination-records&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating a monthly vaccination records&lt;/h2&gt;
&lt;p&gt;monthly_vacc table includes the vaccination records at the end of Jan., Feb., Mach , April and May for each of Australia, Bahamas, Canada and United Kingdom.&lt;/p&gt;
&lt;p&gt;In monthly_vacc_wider table we dropped any NA data and rearranged the variables to have a wider table&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monthly_vacc &amp;lt;- country_vacc   %&amp;gt;%
filter(date==&amp;quot;2021-01-31&amp;quot;| date==&amp;quot;2021-02-28&amp;quot;|date==&amp;quot;2021-03-31&amp;quot;|date==&amp;quot;2021-04-30&amp;quot;|date==&amp;quot;2021-05-20&amp;quot;) %&amp;gt;%
  select(country,date,total_vaccinations)%&amp;gt;%
  filter(country==&amp;quot;Canada&amp;quot;|country==&amp;quot;Australia&amp;quot; |country==&amp;quot;United Kingdom&amp;quot;|country==&amp;quot;Bahamas&amp;quot;)

monthly_vacc_wider &amp;lt;- monthly_vacc %&amp;gt;% drop_na()  %&amp;gt;% 
  pivot_wider(names_from = date,values_from = total_vaccinations)

monthly_vacc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 17 x 3
##    country        date       total_vaccinations
##    &amp;lt;chr&amp;gt;          &amp;lt;date&amp;gt;                  &amp;lt;dbl&amp;gt;
##  1 Australia      2021-02-28              31894
##  2 Australia      2021-03-31             670349
##  3 Australia      2021-04-30            2179544
##  4 Australia      2021-05-20            3371728
##  5 Bahamas        2021-03-31                 NA
##  6 Bahamas        2021-04-30                 NA
##  7 Bahamas        2021-05-20                 NA
##  8 Canada         2021-01-31             957229
##  9 Canada         2021-02-28            1887059
## 10 Canada         2021-03-31            5690380
## 11 Canada         2021-04-30           13420198
## 12 Canada         2021-05-20           19841562
## 13 United Kingdom 2021-01-31            9790576
## 14 United Kingdom 2021-02-28           21091267
## 15 United Kingdom 2021-03-31           35660902
## 16 United Kingdom 2021-04-30           49319518
## 17 United Kingdom 2021-05-20           59178397&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;monthly_vacc_wider&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 6
##   country       `2021-02-28` `2021-03-31` `2021-04-30` `2021-05-20` `2021-01-31`
##   &amp;lt;chr&amp;gt;                &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 Australia            31894       670349      2179544      3371728           NA
## 2 Canada             1887059      5690380     13420198     19841562       957229
## 3 United Kingd~     21091267     35660902     49319518     59178397      9790576&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-april-vaccination-records&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Creating April vaccination records&lt;/h2&gt;
&lt;p&gt;April_vacc table includes the vaccination records of April for each of Australia, Bahamas, Canada, United Kingdom, Hong Kong, Luxembourg, Russia, Scotland, Switzerland, United States and Turkey&lt;/p&gt;
&lt;p&gt;In April_vacc_longer table the variables were rearranged to have a longer table and any NA values have been replaced by 0.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;April_vacc &amp;lt;- country_vacc   %&amp;gt;%
filter(date==&amp;quot;2021-04-30&amp;quot;) %&amp;gt;%
  select(country,date,total_vaccinations)%&amp;gt;%
  filter(country==&amp;quot;Canada&amp;quot;|country==&amp;quot;Australia&amp;quot; |country==&amp;quot;United Kingdom&amp;quot;|country==&amp;quot;Bahamas&amp;quot;|country==&amp;quot;Hong Kong&amp;quot;|country==&amp;quot;Luxembourg&amp;quot;|country==&amp;quot;Russia&amp;quot;|country==&amp;quot;Scotland&amp;quot;|country==&amp;quot;Switzerland&amp;quot;|country==&amp;quot;United States&amp;quot;|country==&amp;quot;Turkey&amp;quot;) %&amp;gt;%
  pivot_wider(names_from = country,values_from = total_vaccinations)


April_vacc_longer &amp;lt;- April_vacc %&amp;gt;%
  pivot_longer(cols=&amp;quot;Australia&amp;quot;:&amp;quot;United States&amp;quot;,names_to = &amp;quot;Country&amp;quot;,values_to = &amp;quot;Total_vaccinations&amp;quot;) %&amp;gt;%
  replace_na(list(Total_vaccinations=0))

April_vacc&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 12
##   date       Australia Bahamas   Canada `Hong Kong` Luxembourg   Russia Scotland
##   &amp;lt;date&amp;gt;         &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1 2021-04-30   2179544      NA 13420198     1413788         NA 19834392  4075205
## # ... with 4 more variables: Switzerland &amp;lt;dbl&amp;gt;, Turkey &amp;lt;dbl&amp;gt;,
## #   United Kingdom &amp;lt;dbl&amp;gt;, United States &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;April_vacc_longer&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 11 x 3
##    date       Country        Total_vaccinations
##    &amp;lt;date&amp;gt;     &amp;lt;chr&amp;gt;                       &amp;lt;dbl&amp;gt;
##  1 2021-04-30 Australia                 2179544
##  2 2021-04-30 Bahamas                         0
##  3 2021-04-30 Canada                   13420198
##  4 2021-04-30 Hong Kong                 1413788
##  5 2021-04-30 Luxembourg                      0
##  6 2021-04-30 Russia                   19834392
##  7 2021-04-30 Scotland                  4075205
##  8 2021-04-30 Switzerland               2769526
##  9 2021-04-30 Turkey                   22816891
## 10 2021-04-30 United Kingdom           49319518
## 11 2021-04-30 United States           240159677&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Wrangling using Tidyverse</title>
      <link>/post/data-wrangling-using-tidyverse/</link>
      <pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/data-wrangling-using-tidyverse/</guid>
      <description>
&lt;script src=&#34;/post/data-wrangling-using-tidyverse/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;import-the-hotels.cvs-file&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import the Hotels.cvs file&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.3     v purrr   0.3.4
## v tibble  3.1.1     v dplyr   1.0.6
## v tidyr   1.1.3     v stringr 1.4.0
## v readr   1.4.0     v forcats 0.5.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hotels &amp;lt;- read_csv(&amp;quot;hotels.csv&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## -- Column specification --------------------------------------------------------
## cols(
##   .default = col_double(),
##   hotel = col_character(),
##   arrival_date_month = col_character(),
##   meal = col_character(),
##   country = col_character(),
##   market_segment = col_character(),
##   distribution_channel = col_character(),
##   reserved_room_type = col_character(),
##   assigned_room_type = col_character(),
##   deposit_type = col_character(),
##   agent = col_character(),
##   company = col_character(),
##   customer_type = col_character(),
##   reservation_status = col_character(),
##   reservation_status_date = col_date(format = &amp;quot;&amp;quot;)
## )
## i Use `spec()` for the full column specifications.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(hotels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 32
##   hotel is_canceled lead_time arrival_date_ye~ arrival_date_mo~ arrival_date_we~
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                       &amp;lt;dbl&amp;gt;
## 1 Reso~           0       342             2015 July                           27
## 2 Reso~           0       737             2015 July                           27
## 3 Reso~           0         7             2015 July                           27
## 4 Reso~           0        13             2015 July                           27
## 5 Reso~           0        14             2015 July                           27
## 6 Reso~           0        14             2015 July                           27
## # ... with 26 more variables: arrival_date_day_of_month &amp;lt;dbl&amp;gt;,
## #   stays_in_weekend_nights &amp;lt;dbl&amp;gt;, stays_in_week_nights &amp;lt;dbl&amp;gt;, adults &amp;lt;dbl&amp;gt;,
## #   children &amp;lt;dbl&amp;gt;, babies &amp;lt;dbl&amp;gt;, meal &amp;lt;chr&amp;gt;, country &amp;lt;chr&amp;gt;,
## #   market_segment &amp;lt;chr&amp;gt;, distribution_channel &amp;lt;chr&amp;gt;, is_repeated_guest &amp;lt;dbl&amp;gt;,
## #   previous_cancellations &amp;lt;dbl&amp;gt;, previous_bookings_not_canceled &amp;lt;dbl&amp;gt;,
## #   reserved_room_type &amp;lt;chr&amp;gt;, assigned_room_type &amp;lt;chr&amp;gt;, booking_changes &amp;lt;dbl&amp;gt;,
## #   deposit_type &amp;lt;chr&amp;gt;, agent &amp;lt;chr&amp;gt;, company &amp;lt;chr&amp;gt;, days_in_waiting_list &amp;lt;dbl&amp;gt;,
## #   customer_type &amp;lt;chr&amp;gt;, adr &amp;lt;dbl&amp;gt;, required_car_parking_spaces &amp;lt;dbl&amp;gt;,
## #   total_of_special_requests &amp;lt;dbl&amp;gt;, reservation_status &amp;lt;chr&amp;gt;,
## #   reservation_status_date &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;names(hotels)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;hotel&amp;quot;                          &amp;quot;is_canceled&amp;quot;                   
##  [3] &amp;quot;lead_time&amp;quot;                      &amp;quot;arrival_date_year&amp;quot;             
##  [5] &amp;quot;arrival_date_month&amp;quot;             &amp;quot;arrival_date_week_number&amp;quot;      
##  [7] &amp;quot;arrival_date_day_of_month&amp;quot;      &amp;quot;stays_in_weekend_nights&amp;quot;       
##  [9] &amp;quot;stays_in_week_nights&amp;quot;           &amp;quot;adults&amp;quot;                        
## [11] &amp;quot;children&amp;quot;                       &amp;quot;babies&amp;quot;                        
## [13] &amp;quot;meal&amp;quot;                           &amp;quot;country&amp;quot;                       
## [15] &amp;quot;market_segment&amp;quot;                 &amp;quot;distribution_channel&amp;quot;          
## [17] &amp;quot;is_repeated_guest&amp;quot;              &amp;quot;previous_cancellations&amp;quot;        
## [19] &amp;quot;previous_bookings_not_canceled&amp;quot; &amp;quot;reserved_room_type&amp;quot;            
## [21] &amp;quot;assigned_room_type&amp;quot;             &amp;quot;booking_changes&amp;quot;               
## [23] &amp;quot;deposit_type&amp;quot;                   &amp;quot;agent&amp;quot;                         
## [25] &amp;quot;company&amp;quot;                        &amp;quot;days_in_waiting_list&amp;quot;          
## [27] &amp;quot;customer_type&amp;quot;                  &amp;quot;adr&amp;quot;                           
## [29] &amp;quot;required_car_parking_spaces&amp;quot;    &amp;quot;total_of_special_requests&amp;quot;     
## [31] &amp;quot;reservation_status&amp;quot;             &amp;quot;reservation_status_date&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;select-a-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Select a variable&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# select(hotels, lead_time)
# select(hotels, hotel,lead_time)
hotels %&amp;gt;%
  select(hotel, lead_time) %&amp;gt;%
  arrange(desc(lead_time))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 119,390 x 2
##    hotel        lead_time
##    &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;
##  1 Resort Hotel       737
##  2 Resort Hotel       709
##  3 City Hotel         629
##  4 City Hotel         629
##  5 City Hotel         629
##  6 City Hotel         629
##  7 City Hotel         629
##  8 City Hotel         629
##  9 City Hotel         629
## 10 City Hotel         629
## # ... with 119,380 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;select-a-range-of-variable&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Select a range of variable&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hotels %&amp;gt;%
  select(hotel, lead_time)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 119,390 x 2
##    hotel        lead_time
##    &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;
##  1 Resort Hotel       342
##  2 Resort Hotel       737
##  3 Resort Hotel         7
##  4 Resort Hotel        13
##  5 Resort Hotel        14
##  6 Resort Hotel        14
##  7 Resort Hotel         0
##  8 Resort Hotel         9
##  9 Resort Hotel        85
## 10 Resort Hotel        75
## # ... with 119,380 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hotels %&amp;gt;%
  select(starts_with(&amp;quot;arrival&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 119,390 x 4
##    arrival_date_year arrival_date_mon~ arrival_date_week_n~ arrival_date_day_of~
##                &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                            &amp;lt;dbl&amp;gt;                &amp;lt;dbl&amp;gt;
##  1              2015 July                                27                    1
##  2              2015 July                                27                    1
##  3              2015 July                                27                    1
##  4              2015 July                                27                    1
##  5              2015 July                                27                    1
##  6              2015 July                                27                    1
##  7              2015 July                                27                    1
##  8              2015 July                                27                    1
##  9              2015 July                                27                    1
## 10              2015 July                                27                    1
## # ... with 119,380 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;select-a-range-of-rows-cases&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;select a range of rows /cases&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hotels %&amp;gt;%
  # we will select 1st, 5th, 8th and 10th rows
  slice(c(1,5,8, 10))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 32
##   hotel is_canceled lead_time arrival_date_ye~ arrival_date_mo~ arrival_date_we~
##   &amp;lt;chr&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;            &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;                       &amp;lt;dbl&amp;gt;
## 1 Reso~           0       342             2015 July                           27
## 2 Reso~           0        14             2015 July                           27
## 3 Reso~           0         9             2015 July                           27
## 4 Reso~           1        75             2015 July                           27
## # ... with 26 more variables: arrival_date_day_of_month &amp;lt;dbl&amp;gt;,
## #   stays_in_weekend_nights &amp;lt;dbl&amp;gt;, stays_in_week_nights &amp;lt;dbl&amp;gt;, adults &amp;lt;dbl&amp;gt;,
## #   children &amp;lt;dbl&amp;gt;, babies &amp;lt;dbl&amp;gt;, meal &amp;lt;chr&amp;gt;, country &amp;lt;chr&amp;gt;,
## #   market_segment &amp;lt;chr&amp;gt;, distribution_channel &amp;lt;chr&amp;gt;, is_repeated_guest &amp;lt;dbl&amp;gt;,
## #   previous_cancellations &amp;lt;dbl&amp;gt;, previous_bookings_not_canceled &amp;lt;dbl&amp;gt;,
## #   reserved_room_type &amp;lt;chr&amp;gt;, assigned_room_type &amp;lt;chr&amp;gt;, booking_changes &amp;lt;dbl&amp;gt;,
## #   deposit_type &amp;lt;chr&amp;gt;, agent &amp;lt;chr&amp;gt;, company &amp;lt;chr&amp;gt;, days_in_waiting_list &amp;lt;dbl&amp;gt;,
## #   customer_type &amp;lt;chr&amp;gt;, adr &amp;lt;dbl&amp;gt;, required_car_parking_spaces &amp;lt;dbl&amp;gt;,
## #   total_of_special_requests &amp;lt;dbl&amp;gt;, reservation_status &amp;lt;chr&amp;gt;,
## #   reservation_status_date &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hotels %&amp;gt;%
  filter(hotel == &amp;quot;City Hotel&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 79,330 x 32
##    hotel      is_canceled lead_time arrival_date_year arrival_date_month
##    &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;             &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;             
##  1 City Hotel           0         6              2015 July              
##  2 City Hotel           1        88              2015 July              
##  3 City Hotel           1        65              2015 July              
##  4 City Hotel           1        92              2015 July              
##  5 City Hotel           1       100              2015 July              
##  6 City Hotel           1        79              2015 July              
##  7 City Hotel           0         3              2015 July              
##  8 City Hotel           1        63              2015 July              
##  9 City Hotel           1        62              2015 July              
## 10 City Hotel           1        62              2015 July              
## # ... with 79,320 more rows, and 27 more variables:
## #   arrival_date_week_number &amp;lt;dbl&amp;gt;, arrival_date_day_of_month &amp;lt;dbl&amp;gt;,
## #   stays_in_weekend_nights &amp;lt;dbl&amp;gt;, stays_in_week_nights &amp;lt;dbl&amp;gt;, adults &amp;lt;dbl&amp;gt;,
## #   children &amp;lt;dbl&amp;gt;, babies &amp;lt;dbl&amp;gt;, meal &amp;lt;chr&amp;gt;, country &amp;lt;chr&amp;gt;,
## #   market_segment &amp;lt;chr&amp;gt;, distribution_channel &amp;lt;chr&amp;gt;, is_repeated_guest &amp;lt;dbl&amp;gt;,
## #   previous_cancellations &amp;lt;dbl&amp;gt;, previous_bookings_not_canceled &amp;lt;dbl&amp;gt;,
## #   reserved_room_type &amp;lt;chr&amp;gt;, assigned_room_type &amp;lt;chr&amp;gt;, booking_changes &amp;lt;dbl&amp;gt;,
## #   deposit_type &amp;lt;chr&amp;gt;, agent &amp;lt;chr&amp;gt;, company &amp;lt;chr&amp;gt;, days_in_waiting_list &amp;lt;dbl&amp;gt;,
## #   customer_type &amp;lt;chr&amp;gt;, adr &amp;lt;dbl&amp;gt;, required_car_parking_spaces &amp;lt;dbl&amp;gt;,
## #   total_of_special_requests &amp;lt;dbl&amp;gt;, reservation_status &amp;lt;chr&amp;gt;,
## #   reservation_status_date &amp;lt;date&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hotels %&amp;gt;%
  filter( 
    adults == 0,
    children &amp;gt;= 1
    ) %&amp;gt;% 
  select(adults, babies, children)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 223 x 3
##    adults babies children
##     &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
##  1      0      0        3
##  2      0      0        2
##  3      0      0        2
##  4      0      0        2
##  5      0      0        2
##  6      0      0        3
##  7      0      1        2
##  8      0      0        2
##  9      0      0        2
## 10      0      0        2
## # ... with 213 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hotels %&amp;gt;%
  filter( 
    adults == 0,     
    children &amp;gt;= 1 &amp;amp; babies &amp;gt;= 1     # &amp;amp; means and
    ) %&amp;gt;%
  select(adults, babies, children)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 3
##   adults babies children
##    &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1      0      1        2
## 2      0      1        2
## 3      0      1        2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;summarizing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;summarizing&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hotels %&amp;gt;%
  count(hotel,market_segment)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 14 x 3
##    hotel        market_segment     n
##    &amp;lt;chr&amp;gt;        &amp;lt;chr&amp;gt;          &amp;lt;int&amp;gt;
##  1 City Hotel   Aviation         237
##  2 City Hotel   Complementary    542
##  3 City Hotel   Corporate       2986
##  4 City Hotel   Direct          6093
##  5 City Hotel   Groups         13975
##  6 City Hotel   Offline TA/TO  16747
##  7 City Hotel   Online TA      38748
##  8 City Hotel   Undefined          2
##  9 Resort Hotel Complementary    201
## 10 Resort Hotel Corporate       2309
## 11 Resort Hotel Direct          6513
## 12 Resort Hotel Groups          5836
## 13 Resort Hotel Offline TA/TO   7472
## 14 Resort Hotel Online TA      17729&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;mutation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;mutation&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;little_ones&amp;lt;- hotels
little_ones %&amp;lt;&amp;gt;%
  mutate(little_ones = children + babies) %&amp;gt;%
  select(children, babies, little_ones) %&amp;gt;%
  arrange(desc(little_ones))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary&amp;lt;- hotels %&amp;gt;%
  group_by(hotel) %&amp;gt;%
  summarise(mean_adr = mean(adr))
summary&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 2 x 2
##   hotel        mean_adr
##   &amp;lt;chr&amp;gt;           &amp;lt;dbl&amp;gt;
## 1 City Hotel      105. 
## 2 Resort Hotel     95.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fun with R</title>
      <link>/post/fun-with-r/</link>
      <pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/fun-with-r/</guid>
      <description>
&lt;script src=&#34;/post/fun-with-r/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;the-variables-and-numbers&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Variables and Numbers&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&amp;lt;- 1
y=2
3 -&amp;gt; z
x+y&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y*z&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;y/x&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 2&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Vectors&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a&amp;lt;- 0:10
print(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  0  1  2  3  4  5  6  7  8  9 10&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;b&amp;lt;- 10:-4
print(b)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] 10  9  8  7  6  5  4  3  2  1  0 -1 -2 -3 -4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;integer&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  int [1:11] 0 1 2 3 4 5 6 7 8 9 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a[ c(1, 5, 10) ]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0 4 9&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(a)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0 1 2 3 4 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(a,4)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0 1 2 3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tail(b,5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  0 -1 -2 -3 -4&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;compine-operators-for-creatinmg-vectors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compine Operators for Creatinmg Vectors&lt;/h2&gt;
&lt;p&gt;When we create a vector with multiple data types, R coerces the vector to most compatible data type&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;C&amp;lt;-c(1:5,10.5,&amp;#39;red&amp;#39;,&amp;#39;green&amp;#39;,&amp;quot;yellow&amp;quot;)
print(C)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;1&amp;quot;      &amp;quot;2&amp;quot;      &amp;quot;3&amp;quot;      &amp;quot;4&amp;quot;      &amp;quot;5&amp;quot;      &amp;quot;10.5&amp;quot;   &amp;quot;red&amp;quot;    &amp;quot;green&amp;quot; 
## [9] &amp;quot;yellow&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(C)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;character&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(C)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  chr [1:9] &amp;quot;1&amp;quot; &amp;quot;2&amp;quot; &amp;quot;3&amp;quot; &amp;quot;4&amp;quot; &amp;quot;5&amp;quot; &amp;quot;10.5&amp;quot; &amp;quot;red&amp;quot; &amp;quot;green&amp;quot; &amp;quot;yellow&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d&amp;lt;- c(1:5,10.5)

print(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  1.0  2.0  3.0  4.0  5.0 10.5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;class(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;numeric&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;str(d)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  num [1:6] 1 2 3 4 5 10.5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;sequence-operator-application&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sequence Operator Application&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;x&amp;lt;-seq(0,8*pi,length.out = 200);
y &amp;lt;- sin(x);
plot(x,y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/fun-with-r/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;dropping-missing-values&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Dropping Missing Values&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a&amp;lt;-c(3,-2,4,NA,-1,8,-4,9,NA, 11,3,8,NA)
a[!is.na(a)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1]  3 -2  4 -1  8 -4  9 11  3  8&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;-c(2,3,-1,3,5,2,-3,1)
cat(&amp;quot;The sum of positive integers in a =&amp;quot;,sum(a[a&amp;gt;0]))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The sum of positive integers in a = 16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;matrix&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Matrix&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M1&amp;lt;-matrix(1:12, ncol=4,byrow=TRUE)
M1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
## [2,]    5    6    7    8
## [3,]    9   10   11   12&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M1[,3]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  3  7 11&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M2&amp;lt;-matrix(1:12, nrow=4)
M2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3]
## [1,]    1    5    9
## [2,]    2    6   10
## [3,]    3    7   11
## [4,]    4    8   12&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;challange-matrix-element&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Challange: Matrix element&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;M1 &amp;lt;-matrix(1:20,ncol=4) 
M1[c(3,5),c(2,4)]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2]
## [1,]    8   18
## [2,]   10   20&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-frames&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Frames&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DF &amp;lt;- data.frame(
gender = c(&amp;quot;Male&amp;quot;, &amp;quot;Male&amp;quot;,&amp;quot;Female&amp;quot;),
height = c(152, 171.5, 165),
weight = c(81,93, 78),
age =c(42,38,26),
row.names=c(&amp;#39;Ally&amp;#39;,&amp;#39;Belinda&amp;#39;,&amp;#39;Alfred&amp;#39;)
)
DF$age&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 42 38 26&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DF[DF$gender==&amp;quot;Male&amp;quot;,]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##         gender height weight age
## Ally      Male  152.0     81  42
## Belinda   Male  171.5     93  38&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Assignment A01: GGPLOT2 </title>
      <link>/post/assignment-a01-ggplot2/</link>
      <pubDate>Sat, 22 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/assignment-a01-ggplot2/</guid>
      <description>
&lt;script src=&#34;/post/assignment-a01-ggplot2/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;first-the-effect-of-vitamin-c-on-tooth-growth-in-guinea-pigs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First, The Effect of Vitamin C on Tooth Growth in Guinea Pigs&lt;/h2&gt;
&lt;p&gt;In this blog we are going to show the effect of providing vitamin C on the growth of the odontoblasts of 60 guinea pigs.&lt;/p&gt;
&lt;div id=&#34;keywords&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Keywords:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Odontoblasts: cells responsible for tooth growth&lt;/li&gt;
&lt;li&gt;Ascorbic acid: a form of vitamin C and coded as VC.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;each-pig-received-one-of-three-dose-levels-of-vitamin-c-which-are&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Each pig received one of three dose levels of vitamin C which are:&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: lower-alpha&#34;&gt;
&lt;li&gt;0.5 mg/day&lt;/li&gt;
&lt;li&gt;1 mg/day&lt;/li&gt;
&lt;li&gt;2 mg/day&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;the-vitamin-c-dose-is-provided-by-one-of-the-two-following-delivery-methods&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The Vitamin C dose is provided by one of the two following delivery methods:&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Through orange juice.&lt;/li&gt;
&lt;li&gt;Through ascorbic acid.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The collected data is composed of 60 observations one for each pig where the tooth length, taken dose and the delivery method are recorded.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-number-of-participant-in-each-dose-level-providing-the-dose-delivery-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The number of participant in each dose level providing the dose delivery method&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;
from the above Fig (1), we can see that for every dose level there are 20 participants and from which 10 participants received the dose through orange juice (OJ) and the other 10 participants received the dose through ascorbic acid (VC).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-relationship-between-the-tooth-growth-and-the-dose-level&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The relationship between the tooth growth and the dose level&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;
From Fig(2) we can see that the tooth growth that is determined by measuring the tooth length vs the dose level given to each pig providing the delivery method of the dose.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grouping-the-data-based-on-both-the-dose-and-the-delivery-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Grouping the data based on both the dose and the delivery method&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;
From Fig(3), we can have a more detailed information about each group, where the grouping is done based on the dose level and the dose delivery method. in fig (3), we can see that the group with dose level equal 2 mg/day and deliver the dose through ascorbic acid (VC/2) achieved the largest tooth growth but when the dose was 0.5 mg/day and the delivery method was VC, it achieved the lowest tooth growth.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;distribution-of-tooth-growth-at-each-dose-level-for-each-delivery-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Distribution of Tooth growth at each dose level for each delivery method&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;
From Fig (4), we can see that at dose level 0.5 and 1 mg/day the tooth growth is better when the delivery method is the orange juice and at dose level of 2mg/day the tooth growth is better when the dose is delivered by ascorbic acid.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tooth-length-histogram&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Tooth length histogram&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(ToothGrowth,aes(x=len))+
geom_histogram(binwidth = 1)+
labs(title=&amp;quot;                                Counts for Tooth length measurements &amp;quot;,x=&amp;quot;Tooth Length&amp;quot;,
 caption = &amp;quot;Fig(5)                                                                                                       &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;
From Fig(5), we can see that the peak of the histogram occurs at length around 26 with 5 teeth occurred at the bin.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-of-the-tooth-growth-based-on-the-dose-delivery-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density of the tooth growth based on the dose delivery method&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;
Fig (6), shows the distribution of tooth length for each dose delivery method.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplot-for-the-tooth-length-distribution-based-on-the-delivery-method&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Boxplot for the tooth length distribution based on the delivery method&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Fig(7) shows the distribution of the tooth growth data based on each dose delivery method. This boxplot shows the minimum, maximum, median, first quartile and third quartile in the data set.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;density-of-the-tooth-growth-based-on-the-dose-level&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Density of the tooth growth based on the dose level&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Fig (8), shows the distribution of tooth length for each dose level.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;boxplot-for-the-tooth-length-distribution-based-on-the-dose-level&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Boxplot for the tooth length distribution based on the dose level&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Fig (9), shows the distribution of the tooth growth data based on each dose level. This boxplot shows the minimum, maximum, median, first quartile and third quartile in the data set.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;second-average-heights-and-weights-for-american-women&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Second, Average Heights and Weights for American Women&lt;/h2&gt;
&lt;p&gt;This data set provides the average heights and weights for American women of age ranges from 30 to 39&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(women,aes(height,weight))+geom_point()+labs(title=&amp;quot;                                                  Height vs weight&amp;quot;,x=&amp;quot;Height in In &amp;quot;,y=&amp;quot;Weight in Ibs&amp;quot;, caption = &amp;quot;Fig(10)                                                                                                       &amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/assignment-a01-ggplot2/index_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Fig (10), shows us that the relationship between the height and the weight for the American women is linearly.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data and Visualization</title>
      <link>/post/data-and-visualization/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/data-and-visualization/</guid>
      <description>
&lt;script src=&#34;/post/data-and-visualization/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;data-visualization-exercise&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data visualization Exercise&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
starwars&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 87 x 14
##    name    height  mass hair_color  skin_color eye_color birth_year sex   gender
##    &amp;lt;chr&amp;gt;    &amp;lt;int&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;       &amp;lt;chr&amp;gt;      &amp;lt;chr&amp;gt;          &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt; 
##  1 Luke S~    172    77 blond       fair       blue            19   male  mascu~
##  2 C-3PO      167    75 &amp;lt;NA&amp;gt;        gold       yellow         112   none  mascu~
##  3 R2-D2       96    32 &amp;lt;NA&amp;gt;        white, bl~ red             33   none  mascu~
##  4 Darth ~    202   136 none        white      yellow          41.9 male  mascu~
##  5 Leia O~    150    49 brown       light      brown           19   fema~ femin~
##  6 Owen L~    178   120 brown, grey light      blue            52   male  mascu~
##  7 Beru W~    165    75 brown       light      blue            47   fema~ femin~
##  8 R5-D4       97    32 &amp;lt;NA&amp;gt;        white, red red             NA   none  mascu~
##  9 Biggs ~    183    84 black       light      brown           24   male  mascu~
## 10 Obi-Wa~    182    77 auburn, wh~ fair       blue-gray       57   male  mascu~
## # ... with 77 more rows, and 5 more variables: homeworld &amp;lt;chr&amp;gt;, species &amp;lt;chr&amp;gt;,
## #   films &amp;lt;list&amp;gt;, vehicles &amp;lt;list&amp;gt;, starships &amp;lt;list&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;mass-vs-weight&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Mass Vs Weight&lt;/h3&gt;
&lt;p&gt;We will study the mass vs weight relationship through scatter plot&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ggplot(data = starwars, mapping = aes(x = height, y = mass)) +
  geom_point() +
  labs(title = &amp;quot;                                Mass vs. height of Starwars characters&amp;quot;,
       x = &amp;quot;Height (cm)&amp;quot;, y = &amp;quot;Weight (kg)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-and-visualization/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;anscombes-quartet&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Anscombe’s Quartet&lt;/h3&gt;
&lt;p&gt;We summarize the quartet information by each set of data&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Tmisc)
quartet %&amp;gt;%
  group_by(set) %&amp;gt;%
  summarise(
    mean_x = mean(x), 
    mean_y = mean(y),
    sd_x = sd(x),
    sd_y = sd(y),
    r = cor(x, y)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 6
##   set   mean_x mean_y  sd_x  sd_y     r
##   &amp;lt;fct&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 I          9   7.50  3.32  2.03 0.816
## 2 II         9   7.50  3.32  2.03 0.816
## 3 III        9   7.5   3.32  2.03 0.816
## 4 IV         9   7.50  3.32  2.03 0.817&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We visualize all four sets
&lt;img src=&#34;/post/data-and-visualization/index_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Visualisation using ggplot2 </title>
      <link>/post/data-visualisation-using-ggplot2/</link>
      <pubDate>Tue, 18 May 2021 00:00:00 +0000</pubDate>
      <guid>/post/data-visualisation-using-ggplot2/</guid>
      <description>
&lt;script src=&#34;/post/data-visualisation-using-ggplot2/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;palmer-penguins-data-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Palmer Penguins Data Visualization&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;/post/data-visualisation-using-ggplot2/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
