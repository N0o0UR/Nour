---
title: Tree Based method A04
author: Nouran
date: '2021-07-30'
slug: tree-based-method-a04
categories:
  - tidymodels
  - ggplot2
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-07-30T12:03:44-03:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



```{r,warning=FALSE , message=FALSE,echo=FALSE,results=FALSE}
library("DataExplorer")
library("janitor")
library("tidyr")
library("tidyverse")
library("dplyr")
library("yardstick")
library("ggplot2")
library("caret")
library("tidymodels")
library("readxl")
library("boot")

```


### The Goal

The goal behind this study is to use the decision tree to build a tree-based model for prediction, so that we will use the provided data set to identify whether a customer will subscribe to a term deposit or not. 


### The data
The data used in this study was obtained from UCI website and we specifically used the Bank-full.csv dataset.

```{r,warning=FALSE , message=FALSE,echo=FALSE,results=FALSE}

bank_data<-read.csv2 ("bank-full.csv",stringsAsFactors=T)%>%
mutate(day=as.character(day))


```


### Splitting the data to training and testing splits

In this step we are going to split the data so that 3/4 of the data would be assigned for training and the remaining 1/4 of the data would be assigned for testing.

```{r,warning=FALSE , message=FALSE,echo=FALSE}

set.seed(121)


data_split <- initial_split(bank_data,prop = 0.75,strata = y)

train_data<- data_split%>%
  training()

test_data <- data_split%>%
  testing()

```



### Create the decision tree  model

Creating the decision tree model that will be used to identify whether a customer will subscribe to a term deposit or not.


```{r,warning=FALSE , message=FALSE}

decision_tree_mdl <- decision_tree() %>%
  # Specify the engine
  set_engine('rpart') %>%
  # Specify the mode
  set_mode('classification')

decision_tree_mdl
```

### Creating the recipe

Creating the suitable recipe to apply feature engineering to our data


```{r,warning=FALSE , message=FALSE,echo=FALSE}
the_recipe <- recipe(y ~ .,data = train_data) %>%
step_corr(all_numeric(), threshold = 0.9) %>%
step_normalize(all_numeric()) %>%
step_dummy(all_nominal(), -all_outcomes())%>%
  # remove zero variance predictors
  step_zv(all_predictors())
the_recipe
```


### Create workflow

Create a workflow that combine the model and the recipe.

```{r,warning=FALSE , message=FALSE,echo=FALSE}
wkfl <- workflow() %>%
  # Add model
  add_model(decision_tree_mdl) %>%
  # Add recipe
  add_recipe(the_recipe)

wkfl
```


### Fit the model

Train and fitting the model using the training split of the data.

```{r,warning=FALSE , message=FALSE,echo=FALSE}

mdl_fit <- wkfl %>%
  fit(data=train_data)
mdl_fit

```


### Make predictions for training data

Using the trained model for prediction on the training data.

```{r,warning=FALSE , message=FALSE,echo=FALSE}

train_pred<-predict(mdl_fit, train_data,type = 'class') %>%
  bind_cols(train_data %>% select(everything()))

train_pred


```
### Evaluate performance on tarining data

Evaluate the performance of the model on the training data.The accuracy of the model on the training data is 90.2%.

```{r,warning=FALSE , message=FALSE,echo=FALSE}

conf_mat(train_pred, truth = y, estimate = .pred_class)

custom_metrics <-
metric_set(accuracy, sens, spec)

custom_metrics(train_pred,
truth = y,
estimate = .pred_class)

```
### Make predictions for testing data

Using the trained model for prediction on the testing data.


```{r,warning=FALSE , message=FALSE,echo=FALSE}
test_pred <- predict(mdl_fit, test_data) %>%
  bind_cols(test_data %>% select(everything()))
test_pred

```
### Evaluate performance on testing data

Evaluate the performance of the model on the testing data.The accuracy of the model on the testing data is 90%. 

```{r,warning=FALSE , message=FALSE,echo=FALSE}

conf_mat(test_pred, truth = y, estimate = .pred_class)


test_metric<- custom_metrics(test_pred,
truth = y,
estimate = .pred_class)

test_metric

```
## Apply Cross Validation method for model evaluation

We will use the cross-validation resampling method to evaluate the performance of our decision tree model.

### Split data into folds

The train data is split-ted into 10 folds

```{r,warning=FALSE , message=FALSE,echo=FALSE}


set.seed(13)
folds <- vfold_cv(train_data,v = 10,strata = y)
folds

```


### Fit resamples

Fit the folds into the workflow.

```{r,warning=FALSE , message=FALSE,echo=FALSE}


cv_fit<- wkfl%>%
  fit_resamples(folds)

```

### Collect CV metrics

Show the metrics resulted from Cross-Validation process. The accurcay achieved after using the 10 folds cross validation for resampling is 90.1% .


  
```{r,warning=FALSE , message=FALSE,echo=FALSE}

collect_metrics(cv_fit)
```


## Hyperparameter tuning 

In this part we will tune the hyperparameters of the decision tree and check the tuning effect on the model accuracy.

### Set the hyperparameters for tuning in the decision tree

We will modify the model so that the hyperparameters will be undergo tuuining and accordingly the workflow would be updated with the new model.

```{r}

tune_model <- decision_tree(cost_complexity = tune(),
tree_depth = tune(),
min_n = tune()) %>%
set_engine('rpart') %>%
set_mode('classification')

tune_wkfl <- wkfl %>%
update_model(tune_model)


```


### Create a tuning grid

Create the tuning grid with 3 levels for each parameter.
```{r}
set.seed(216)
grid<-grid_regular(parameters(tune_model),levels = 3)


```
### Hyperparameter tuning with cross validation

From the shown graph we can see the accuracy Vs. the different combination of the 3 hyperparameters levels.  

```{r}
tune_results <- tune_wkfl %>%
tune_grid(resamples = folds,
grid = grid,
metrics = metric_set(accuracy))

autoplot(tune_results)

tune_results %>%
collect_metrics()
```
### Select the best parameters

From the above graph we can see that the best hyperparameters combination is minimum node size of 40, tree depth of 8 and the cost complexity of 0.0000000001, where this combination result in an accuracy of 92.5%

```{r}
final_params <- select_best(tune_results)
final_params



```
### Finalize the model 

Finalize the model with the best value for each hyperparameter.
```{r}
best_spec <- finalize_model(tune_model,
final_params)
best_spec
```

## Passing the testing data to the tuned model 

Make prediction with the tuned model on the testing data.
```{r}
tuned_decision_tree_mdl <- decision_tree(cost_complexity = 1e-10,tree_depth = 8,min_n = 2) %>%
  # Specify the engine
  set_engine('rpart') %>%
  # Specify the mode
  set_mode('classification')

tuned_wkflw <- workflow() %>%
  # Add model
  add_model(tuned_decision_tree_mdl) %>%
  # Add recipe
  add_recipe(the_recipe)

fit_tuned_mdl <- tuned_wkflw %>%
  fit(data=test_data)


tuned_prediction<-predict(fit_tuned_mdl, test_data,type = 'class') %>%
  bind_cols(test_data %>% select(everything()))

tuned_prediction



```

### Evaluate the performance of the tuned model

Evaluate the performance of the tuned model on the testing data.
```{r}
conf_mat(tuned_prediction, truth = y, estimate = .pred_class)


test_metric_tuned<- custom_metrics(tuned_prediction,
truth = y,
estimate = .pred_class)

test_metric_tuned
```


### Conclusion

The accuracy associated with the untuned model on the training data is 90.2%, and that associated with the untuned model on the testing data is 90% and that associated with the same model using cross validation resampling method is 90.1%. The accuracy in the testing data is quite good and it is slightly less than the accuracy in the training data which indicates that our model is not exposed to overfitting on the training data, also the accuracy resulted from the testing data is comparable with the accuracy we got from cross-validation resampling method that indicates that our model is working properly. After tuning the model with the best combination of the 3 hyperparameters that are min_n=40 , tree_depth=8 and the cost_complexity=0.0000000001 we got an accuracy of 92.5% that is considered the highest accuracy. This shows us the importance and the effect of tuning the decision tree model hyperparameters on the accuracy of the model.
