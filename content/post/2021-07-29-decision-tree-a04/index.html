---
title: Decision tree A04
author: Nouran
date: '2021-07-29'
slug: decision-tree-a04
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-07-30T00:03:35-03:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="the-goal" class="section level3">
<h3>The Goal</h3>
<p>The goal behind this study is to use the decision tree to build a tree-based model for prediction, so that we will use the provided data set to identify whether a customer will subscribe to a term deposit or not.</p>
</div>
<div id="the-data" class="section level3">
<h3>The data</h3>
<p>The data used in this study was obtained from UCI website and we specifically used the Bank-full.csv dataset.</p>
</div>
<div id="splitting-the-data-to-training-and-testing-splits" class="section level3">
<h3>Splitting the data to training and testing splits</h3>
<p>In this step we are going to split the data so that 3/4 of the data would be assigned for training and the remaining 1/4 of the data would be assigned for testing.</p>
</div>
<div id="create-the-decision-tree-model" class="section level3">
<h3>Create the decision tree model</h3>
<p>Creating the decision tree model that will be used to identify whether a customer will subscribe to a term deposit or not.</p>
</div>
<div id="creating-the-recipe" class="section level3">
<h3>Creating the recipe</h3>
<p>Creating the suitable recipe to apply feature engineering to our data</p>
</div>
<div id="create-workflow" class="section level3">
<h3>Create workflow</h3>
<p>Create a workflow that combine the model and the recipe.</p>
</div>
<div id="fit-the-model" class="section level3">
<h3>Fit the model</h3>
<p>Train and fitting the model using the training split of the data.</p>
</div>
<div id="make-predictions-for-training-data" class="section level3">
<h3>Make predictions for training data</h3>
<p>Using the trained model for prediction on the training data.</p>
<pre><code>## # A tibble: 33,907 x 18
##    .pred_class   age job         marital education default balance housing loan 
##    &lt;fct&gt;       &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;
##  1 no             58 management  married tertiary  no         2143 yes     no   
##  2 no             44 technician  single  secondary no           29 yes     no   
##  3 no             47 blue-collar married unknown   no         1506 yes     no   
##  4 no             35 management  married tertiary  no          231 yes     no   
##  5 no             28 management  single  tertiary  no          447 yes     yes  
##  6 no             42 entreprene~ divorc~ tertiary  yes           2 yes     no   
##  7 no             58 retired     married primary   no          121 yes     no   
##  8 no             43 technician  single  secondary no          593 yes     no   
##  9 no             29 admin.      single  secondary no          390 yes     no   
## 10 no             53 technician  married secondary no            6 yes     no   
## # ... with 33,897 more rows, and 9 more variables: contact &lt;fct&gt;, day &lt;chr&gt;,
## #   month &lt;fct&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;, previous &lt;int&gt;,
## #   poutcome &lt;fct&gt;, y &lt;fct&gt;</code></pre>
</div>
<div id="evaluate-performance-on-tarining-data" class="section level3">
<h3>Evaluate performance on tarining data</h3>
<p>Evaluate the performance of the model on the training data.</p>
<pre><code>##           Truth
## Prediction    no   yes
##        no  29146  2538
##        yes   795  1428</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.902
## 2 sens     binary         0.973
## 3 spec     binary         0.360</code></pre>
</div>
<div id="make-predictions-for-testing-data" class="section level3">
<h3>Make predictions for testing data</h3>
<p>Using the trained model for prediction on the testing data.</p>
<pre><code>## # A tibble: 11,304 x 18
##    .pred_class   age job         marital education default balance housing loan 
##    &lt;fct&gt;       &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;
##  1 no             33 entreprene~ married secondary no            2 yes     yes  
##  2 no             33 unknown     single  unknown   no            1 no      no   
##  3 no             41 admin.      divorc~ secondary no          270 yes     no   
##  4 no             57 services    married secondary no          162 yes     no   
##  5 no             45 admin.      single  unknown   no           13 yes     no   
##  6 no             56 management  married tertiary  no          779 yes     no   
##  7 no             40 retired     married primary   no            0 yes     yes  
##  8 no             46 management  single  secondary no         -246 yes     no   
##  9 no             36 technician  single  secondary no          265 yes     yes  
## 10 no             57 technician  married secondary no          839 no      yes  
## # ... with 11,294 more rows, and 9 more variables: contact &lt;fct&gt;, day &lt;chr&gt;,
## #   month &lt;fct&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;, previous &lt;int&gt;,
## #   poutcome &lt;fct&gt;, y &lt;fct&gt;</code></pre>
</div>
<div id="evaluate-performance-on-testing-data" class="section level3">
<h3>Evaluate performance on testing data</h3>
<p>Evaluate the performance of the model on the testing data.</p>
<pre><code>##           Truth
## Prediction   no  yes
##        no  9698  847
##        yes  283  476</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.900
## 2 sens     binary         0.972
## 3 spec     binary         0.360</code></pre>
</div>
<div id="apply-cross-validation-method-for-model-evaluation" class="section level2">
<h2>Apply Cross Validation method for model evaluation</h2>
<p>We will use the cross-validation resampling method to evaluate the performance of our decision tree model.</p>
<div id="split-data-into-folds" class="section level3">
<h3>Split data into folds</h3>
<p>The train data is split-ted into 5 folds</p>
</div>
<div id="fit-resamples" class="section level3">
<h3>Fit resamples</h3>
<p>Fit the folds into the workflow.</p>
</div>
<div id="collect-cv-metrics" class="section level3">
<h3>Collect CV metrics</h3>
<p>Show the metrics resulted from Cross-Validation process</p>
<pre><code>## # A tibble: 2 x 6
##   .metric  .estimator  mean     n std_err .config             
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary     0.901    10 0.00143 Preprocessor1_Model1
## 2 roc_auc  binary     0.747    10 0.00361 Preprocessor1_Model1</code></pre>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>The accuracy associated with the model using the training data is 90.1%, that associated with the model using the testing data is 90.3% and that associated with the model using cross validation resampling method is 90.1%. The accuracy in the testing data is quite good and it is slightly better than the accuracy in the training data which indicates that our model is not exposed to overfitting on the training data, also the accuracy resulted from the testing data is comparable with the accuracy we got from cross-validation resampling method that indicates that our model is working properly.</p>
</div>
<div id="hyperparameter-tuning" class="section level3">
<h3>Hyperparameter tuning</h3>
<p>In this part we will tune the hyperparameters of the decision tree and check the tuning effect on the model accuracy.</p>
<pre class="r"><code># creating the decision tree with and tune it&#39;s hyperparameters

tune_model &lt;- decision_tree(cost_complexity = tune(),
tree_depth = tune(),
min_n = tune()) %&gt;%
set_engine(&#39;rpart&#39;) %&gt;%
set_mode(&#39;classification&#39;)

tune_wkfl &lt;- wkfl %&gt;%
update_model(tune_model)

set.seed(216)
grid&lt;-grid_regular(parameters(tune_model),size = 3)</code></pre>
<pre><code>## Warning: `size` is not an argument to `grid_regular()`. Did you mean `levels`?</code></pre>
<pre class="r"><code># Hyperparameter tuning with cross validation
tune_results &lt;- tune_wkfl %&gt;%
tune_grid(resamples = folds,
grid = grid,
metrics = metric_set(accuracy))

autoplot(tune_results)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>tune_results %&gt;%
collect_metrics()</code></pre>
<pre><code>## # A tibble: 27 x 9
##    cost_complexity tree_depth min_n .metric  .estimator  mean     n   std_err
##              &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;
##  1    0.0000000001          1     2 accuracy binary     0.883    10 0.0000417
##  2    0.00000316            1     2 accuracy binary     0.883    10 0.0000417
##  3    0.1                   1     2 accuracy binary     0.883    10 0.0000417
##  4    0.0000000001          8     2 accuracy binary     0.899    10 0.000797 
##  5    0.00000316            8     2 accuracy binary     0.899    10 0.000797 
##  6    0.1                   8     2 accuracy binary     0.883    10 0.0000417
##  7    0.0000000001         15     2 accuracy binary     0.891    10 0.00151  
##  8    0.00000316           15     2 accuracy binary     0.891    10 0.00151  
##  9    0.1                  15     2 accuracy binary     0.883    10 0.0000417
## 10    0.0000000001          1    21 accuracy binary     0.883    10 0.0000417
## # ... with 17 more rows, and 1 more variable: .config &lt;chr&gt;</code></pre>
<pre class="r"><code># best_model &lt;- tuning %&gt;%
# select_best(metric = &#39;accuracy&#39;)
# best_model

final_params &lt;- select_best(tune_results)
final_params</code></pre>
<pre><code>## # A tibble: 1 x 4
##   cost_complexity tree_depth min_n .config              
##             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                
## 1    0.0000000001          8    40 Preprocessor1_Model22</code></pre>
<pre class="r"><code>best_spec &lt;- finalize_model(tune_model,
final_params)
best_spec</code></pre>
<pre><code>## Decision Tree Model Specification (classification)
## 
## Main Arguments:
##   cost_complexity = 1e-10
##   tree_depth = 8
##   min_n = 40
## 
## Computational engine: rpart</code></pre>
</div>
</div>
<div id="passing-the-testing-data-to-the-tuned-model" class="section level2">
<h2>passing the testing data to the tuned model</h2>
<pre class="r"><code>tuned_decision_tree_mdl &lt;- decision_tree(cost_complexity = 1e-10,tree_depth = 8,min_n = 2) %&gt;%
  # Specify the engine
  set_engine(&#39;rpart&#39;) %&gt;%
  # Specify the mode
  set_mode(&#39;classification&#39;)

tuned_wkflw &lt;- workflow() %&gt;%
  # Add model
  add_model(tuned_decision_tree_mdl) %&gt;%
  # Add recipe
  add_recipe(the_recipe)

fit_tuned_mdl &lt;- tuned_wkflw %&gt;%
  fit(data=test_data)


tuned_prediction&lt;-predict(fit_tuned_mdl, test_data,type = &#39;class&#39;) %&gt;%
  bind_cols(test_data %&gt;% select(everything()))

tuned_prediction</code></pre>
<pre><code>## # A tibble: 11,304 x 18
##    .pred_class   age job         marital education default balance housing loan 
##    &lt;fct&gt;       &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;   &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt; &lt;fct&gt;   &lt;fct&gt;
##  1 no             33 entreprene~ married secondary no            2 yes     yes  
##  2 no             33 unknown     single  unknown   no            1 no      no   
##  3 no             41 admin.      divorc~ secondary no          270 yes     no   
##  4 no             57 services    married secondary no          162 yes     no   
##  5 no             45 admin.      single  unknown   no           13 yes     no   
##  6 no             56 management  married tertiary  no          779 yes     no   
##  7 no             40 retired     married primary   no            0 yes     yes  
##  8 no             46 management  single  secondary no         -246 yes     no   
##  9 no             36 technician  single  secondary no          265 yes     yes  
## 10 no             57 technician  married secondary no          839 no      yes  
## # ... with 11,294 more rows, and 9 more variables: contact &lt;fct&gt;, day &lt;chr&gt;,
## #   month &lt;fct&gt;, duration &lt;int&gt;, campaign &lt;int&gt;, pdays &lt;int&gt;, previous &lt;int&gt;,
## #   poutcome &lt;fct&gt;, y &lt;fct&gt;</code></pre>
<pre class="r"><code>conf_mat(tuned_prediction, truth = y, estimate = .pred_class)</code></pre>
<pre><code>##           Truth
## Prediction   no  yes
##        no  9821  688
##        yes  160  635</code></pre>
<pre class="r"><code>test_metric_tuned&lt;- custom_metrics(tuned_prediction,
truth = y,
estimate = .pred_class)

test_metric_tuned</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.925
## 2 sens     binary         0.984
## 3 spec     binary         0.480</code></pre>
<div id="conclusion-1" class="section level3">
<h3>Conclusion</h3>
<p>The accuracy associated with the model using the training data is 90.1%, that associated with the model using the testing data is 90.3% and that associated with the model using cross validation resampling method is 90.1%. The accuracy in the testing data is quite good and it is slightly better than the accuracy in the training data which indicates that our model is not exposed to overfitting on the training data, also the accuracy resulted from the testing data is comparable with the accuracy we got from cross-validation resampling method that indicates that our model is working properly.</p>
</div>
</div>
